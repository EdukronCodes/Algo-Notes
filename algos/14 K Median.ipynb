{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb340dd-95b8-498c-b3d4-a29946999042",
   "metadata": {},
   "source": [
    "Okay, here are detailed and well-organized notes on K-Medians Clustering, following the same structure and depth requirements as your enhanced prompt for K-Modes.\n",
    "\n",
    "## K-Medians Clustering: Detailed Explanatory Notes\n",
    "\n",
    "### Introduction to K-Medians Clustering\n",
    "\n",
    "K-Medians clustering is a partitioning algorithm designed for **numerical data**, serving as a robust alternative to the more common K-Means algorithm. Its direct lineage from K-Means is evident in its iterative partitioning approach. The core motivation for K-Medians arises from a key limitation of K-Means: its sensitivity to outliers. K-Means uses the mean to define cluster centers (centroids), and the mean is heavily influenced by extreme values. Furthermore, K-Means typically minimizes the sum of squared Euclidean distances, which further amplifies the effect of outliers. K-Medians addresses this by using **medoids** as cluster centers instead of means. A medoid is an actual data point within the cluster that is most centrally located.\n",
    "\n",
    "The **objective of K-Medians** is to partition numerical data into *K* distinct clusters by minimizing the sum of chosen distances (often **Manhattan distance/L1 norm**) between data points and their cluster medoids. By using medoids and often the Manhattan distance, K-Medians reduces the influence of outliers, leading to potentially more representative clusters in datasets with skewed distributions or erroneous data points. While potentially more computationally intensive in its update step compared to K-Means, its robustness makes it a valuable tool for datasets where outliers are a concern. It shares the simplicity of iterative refinement with K-Means but offers a more resilient clustering outcome.\n",
    "\n",
    "### The K-Medians Algorithm Steps (Detailed Explanation)\n",
    "\n",
    "The K-Medians algorithm, like K-Means and K-Modes, is an iterative procedure involving initialization, assignment, and medoid update steps.\n",
    "\n",
    "**1. Initialization:**\n",
    "The initialization step in K-Medians involves selecting *K* initial medoids, which must be actual data points from the dataset.\n",
    "*   **Random Selection:** The most common method is to randomly select *K* unique data points from the dataset to serve as the initial medoids for the *K* clusters. As with K-Means and K-Modes, this approach is simple but can lead to different local optima or slower convergence. Running the algorithm multiple times (`n_init`) with different random initializations is standard practice.\n",
    "*   **More Sophisticated Initialization Methods:**\n",
    "    *   **Build Step of PAM (Partitioning Around Medoids):** While PAM is a more comprehensive algorithm, its initial \"BUILD\" phase can be adapted. It often involves selecting points that are centrally located or contribute most to reducing an initial cost. For example, one could select the point that minimizes the sum of distances to all other points as the first medoid, then iteratively select subsequent medoids that best reduce the overall sum of distances given the already chosen medoids.\n",
    "    *   **k-means++ style initialization (adapted):** The k-means++ strategy (choosing initial centers that are far apart) can be adapted. The first medoid is chosen randomly, and subsequent medoids are chosen from the remaining data points with a probability proportional to their squared distance (or chosen distance metric) to the nearest already-selected medoid. This encourages diverse initial medoids.\n",
    "The sensitivity of K-Medians to initial medoid selection is a known characteristic, making good initialization or multiple runs crucial for better quality results.\n",
    "\n",
    "**2. Assignment Step:**\n",
    "Once the initial *K* medoids (M<sub>1</sub>, M<sub>2</sub>, ..., M<sub>K</sub>), which are actual data points, are established, this step assigns each data point *X<sub>i</sub>* in the dataset to the cluster whose medoid is \"closest.\"\n",
    "The distance measure typically used is the **Manhattan distance (L1 norm)**, though other L<sub>p</sub> norms like Euclidean distance (L2 norm) can be used (though L1 is more characteristic for K-Medians' robustness). For two data points *X* = (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>m</sub>) and a medoid *M* = (m<sub>1</sub>, m<sub>2</sub>, ..., m<sub>m</sub>) in an *m*-dimensional numerical space, the Manhattan distance *d<sub>1</sub>(X, M)* is:\n",
    "`d<sub>1</sub>(X, M) = Σ<sub>j=1</sub><sup>m</sup> |x<sub>j</sub> - m<sub>j</sub>|`\n",
    "**Example:**\n",
    "Suppose data point *X* = (2, 5) and medoid *M* = (4, 3).\n",
    "Manhattan distance *d<sub>1</sub>(X, M) = |2 - 4| + |5 - 3| = |-2| + |2| = 2 + 2 = 4.\n",
    "Each data point *X<sub>i</sub>* is assigned to cluster *C<sub>l</sub>* if its distance to medoid *M<sub>l</sub>* is the minimum among all *K* medoids. This step groups data points based on their proximity to the current medoids, forming the initial or intermediate clusters.\n",
    "\n",
    "**3. Medoid Update Step:**\n",
    "After all data points are assigned to clusters, the medoid for each cluster is re-evaluated. This is a key difference from K-Means (which calculates a mean). For each cluster *C<sub>l</sub>*, the new medoid *M<sub>l</sub>* is selected as the **actual data point *within that cluster C<sub>l</sub>*** that minimizes the sum of distances to all other data points *also within cluster C<sub>l</sub>*.\n",
    "Mathematically, for each cluster *C<sub>l</sub>*, the new medoid *M<sub>l</sub>* is:\n",
    "`M<sub>l</sub> = argmin<sub>X<sub>p</sub> ∈ C<sub>l</sub></sub> Σ<sub>X<sub>q</sub> ∈ C<sub>l</sub></sub> d(X<sub>p</sub>, X<sub>q</sub>)`\n",
    "where `d` is the chosen distance metric (e.g., Manhattan distance).\n",
    "This typically requires calculating the sum of distances from each point in the cluster to every other point in the same cluster and selecting the point for which this sum is minimal. This can be computationally intensive for large clusters, as it might involve an O(n<sub>c</sub><sup>2</sup> * m) operation per cluster in a naive pairwise comparison (where n<sub>c</sub> is the number of points in the cluster and m is dimensionality). More optimized approaches exist, especially for specific distance metrics or 1D median finding, but the general medoid finding is demanding. This step ensures the medoid is a true representative existing within the data, shifting it to the \"most central\" data point of its current cluster members.\n",
    "\n",
    "**4. Convergence Criteria:**\n",
    "The iterative process of assignment and medoid update continues until one or more stopping conditions are met:\n",
    "*   **Medoids no longer change:** If the set of medoids in the current iteration is identical to the set from the previous iteration.\n",
    "*   **Assignments of data points to clusters no longer change:** No data points switch clusters between iterations.\n",
    "*   **A maximum number of iterations is reached:** A user-defined limit prevents indefinite execution.\n",
    "*   **The cost function (total sum of distances) doesn't decrease significantly:** If the improvement in the objective function value falls below a small threshold.\n",
    "These criteria ensure the algorithm terminates, typically at a local minimum of the objective function.\n",
    "\n",
    "### Mathematical Foundations and Distance Measure\n",
    "\n",
    "K-Medians is grounded in minimizing a specific cost function using an appropriate distance metric for numerical data, often chosen for robustness.\n",
    "\n",
    "**Distance Measure:**\n",
    "The most characteristic distance measure for K-Medians is the **Manhattan distance (L1 norm)**. For two *m*-dimensional numerical vectors *X* = (x<sub>1</sub>, ..., x<sub>m</sub>) and *Y* = (y<sub>1</sub>, ..., y<sub>m</sub>), it is:\n",
    "`d<sub>1</sub>(X, Y) = Σ<sub>j=1</sub><sup>m</sup> |x<sub>j</sub> - y<sub>j</sub>|`\n",
    "The **intuitive meaning** is the sum of the absolute differences between the coordinates of the two points. It's often called the \"city block\" or \"taxicab\" distance because it represents the distance a taxi would travel on a grid-like street layout.\n",
    "The primary advantage of Manhattan distance in this context is its **robustness to outliers**. Unlike squared Euclidean distance (used in K-Means), which heavily penalizes large deviations, the L1 norm gives a linear penalty to these deviations. Thus, extreme points have less influence on the sum of distances, making the medoid (and the overall clustering) less susceptible to being skewed by outliers. While Euclidean distance (L2 norm) `d<sub>2</sub>(X, Y) = sqrt(Σ(x<sub>j</sub> - y<sub>j</sub>)<sup>2</sup>)` can also be used, K-Medians is most distinctly itself and most robust when paired with L1.\n",
    "\n",
    "**Objective Function:**\n",
    "K-Medians seeks to find a partition of the data and a set of medoids that minimize the sum of distances from each data point to the medoid of its assigned cluster. Let *X* = {*X<sub>1</sub>, ..., X<sub>N</sub>*} be the dataset, *W* be the partition matrix (*w<sub>il</sub>* = 1 if *X<sub>i</sub>* ∈ cluster *C<sub>l</sub>*, 0 otherwise), and *M* = {*M<sub>1</sub>, ..., M<sub>K</sub>*} be the set of *K* medoids. The objective function *P(W, M)* is:\n",
    "`P(W, M) = Σ<sub>l=1</sub><sup>K</sup> Σ<sub>i=1</sub><sup>N</sup> w<sub>il</sub> * d(X<sub>i</sub>, M<sub>l</sub>)`\n",
    "Or, more compactly:\n",
    "`P(W, M) = Σ<sub>l=1</sub><sup>K</sup> Σ<sub>X<sub>i</sub> ∈ C<sub>l</sub></sub> d(X<sub>i</sub>, M<sub>l</sub>)`\n",
    "where `d` is the chosen distance metric (e.g., Manhattan distance).\n",
    "The assignment step fixes *M* and minimizes *P* with respect to *W*. The medoid update step fixes *W* and minimizes *P* with respect to *M* by finding the optimal data point within each cluster to serve as its medoid. This iterative process guarantees that the cost function will monotonically decrease (or stay the same) with each full iteration, but it converges to a local minimum, not necessarily the global one.\n",
    "\n",
    "### Assumptions and Limitations\n",
    "\n",
    "K-Medians, while robust, has its own set of assumptions and limitations.\n",
    "\n",
    "1.  **Numerical Data Only:** K-Medians is designed for numerical (interval or ratio scale) attributes. It cannot directly handle categorical data because distance metrics like Manhattan or Euclidean are not defined for nominal values.\n",
    "2.  **Sensitivity to Initial Medoids:** Similar to K-Means and K-Modes, the final clustering solution can depend on the initial choice of medoids. Different starting points can lead to different local optima. Multiple runs (`n_init`) or smarter initialization techniques are recommended.\n",
    "3.  **Need to Pre-specify K:** The number of clusters, *K*, must be provided by the user. Choosing an appropriate *K* is often challenging and requires domain knowledge or heuristic methods.\n",
    "4.  **Impact of Irrelevant Attributes and Feature Scaling:**\n",
    "    *   **Feature Scaling:** Distance-based algorithms like K-Medians are highly sensitive to the scale of numerical features. Attributes with larger ranges or variances will dominate the distance calculation. Therefore, **feature scaling (e.g., standardization or min-max normalization) is crucial** before applying K-Medians to ensure all attributes contribute more equitably.\n",
    "    *   **Irrelevant Attributes:** Attributes that do not contribute to the natural grouping structure can obscure meaningful clusters by adding noise to the distance calculations.\n",
    "5.  **Computational Cost of Medoid Update:** The medoid update step, which involves finding the data point within each cluster that minimizes the sum of intra-cluster distances, can be computationally expensive. For a cluster with *n<sub>c</sub>* points, a naive approach might take O(n<sub>c</sub><sup>2</sup>) distance calculations. This can make K-Medians slower than K-Means, especially for large datasets or large *K*, as K-Means' centroid update is O(n<sub>c</sub>). More sophisticated algorithms like PAM (which K-Medians is a variant of) have optimizations, but the core idea remains.\n",
    "6.  **\"Hard\" Assignment:** K-Medians performs a hard assignment, assigning each data point to exactly one cluster. This may not be suitable for data with fuzzy boundaries or overlapping clusters.\n",
    "7.  **Choice of Distance Metric:** While Manhattan (L1) is common for robustness, the choice of distance metric can impact results. The metric should align with the data's characteristics and the problem's goals.\n",
    "\n",
    "### Practical Guidance on Choosing K and Evaluation\n",
    "\n",
    "Selecting *K* for K-Medians involves similar strategies to K-Means, adapted for its specific cost function and properties.\n",
    "\n",
    "1.  **Cost Function Plot (Elbow Method):** Plot the total sum of distances (the value of the objective function *P(W,M)* using, e.g., Manhattan distance) against different values of *K*. Look for an \"elbow\" point where adding more clusters yields diminishing returns in reducing the total intra-cluster distance. This often provides a useful heuristic.\n",
    "2.  **Silhouette Score:** This metric evaluates how similar a point is to its own cluster compared to other clusters. It can be calculated using the chosen distance metric (e.g., Manhattan). The average silhouette score across all points for different *K* values can be plotted, with higher scores indicating better-defined clusters.\n",
    "3.  **Domain Knowledge and Interpretability of Medoids:** The choice of *K* should lead to interpretable clusters. Since medoids are actual data points, they can be directly examined. If the medoids represent meaningful, distinct prototypes and the clusters they define make sense in the application context, it supports the chosen *K*. For instance, in customer segmentation, each medoid should represent a recognizable customer profile.\n",
    "4.  **Gap Statistic:** This method compares the intra-cluster dispersion of the data with that of random data. It can be more robust than the elbow method but is computationally more intensive. It can be adapted for K-Medians by using the appropriate distance metric.\n",
    "5.  **Visual Inspection of Cluster Profiles and Scatter Plots:**\n",
    "    *   For low-dimensional data (2D or 3D), scatter plots colored by cluster assignment, with medoids highlighted, can visually assess cluster separation.\n",
    "    *   For higher-dimensional data, examine cluster profiles: create box plots or histograms for each numerical attribute, faceted by cluster. This helps understand the characteristics of each cluster. Distinct profiles support a good choice of *K*.\n",
    "\n",
    "### Python Implementation with scikit-learn-extra or pyclustering\n",
    "\n",
    "K-Medians is not part of the main `scikit-learn` library, but implementations are available in `sklearn_extra.cluster.KMedoids` or the `pyclustering` library. Here's a demonstration using `sklearn_extra`.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_extra.cluster import KMedoids # Or from pyclustering.cluster.kmedoids import kmedoids\n",
    "from sklearn.preprocessing import StandardScaler # Important for K-Medians\n",
    "from sklearn.datasets import make_blobs # For synthetic numerical data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Code Walkthrough ---\n",
    "\n",
    "# 1. Import libraries (done above)\n",
    "\n",
    "# 2. Data loading and PREPROCESSING (SCALING)\n",
    "# Let's create synthetic numerical data with potential for outliers\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
    "# Introduce some outliers\n",
    "rng = np.random.RandomState(10)\n",
    "X_outliers = rng.uniform(low=-10, high=10, size=(20, X.shape[1]))\n",
    "X_with_outliers = np.vstack([X, X_outliers])\n",
    "\n",
    "df = pd.DataFrame(X_with_outliers, columns=['Feature1', 'Feature2'])\n",
    "\n",
    "print(\"Dataset Head (with potential outliers):\")\n",
    "print(df.head())\n",
    "\n",
    "# Feature Scaling: CRUCIAL for distance-based algorithms like K-Medians\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=df.columns)\n",
    "\n",
    "print(\"\\nScaled Dataset Head:\")\n",
    "print(df_scaled.head())\n",
    "\n",
    "\n",
    "# 3. Instantiating KMedoids\n",
    "# Parameters for sklearn_extra.KMedoids:\n",
    "#   n_clusters: The number of clusters to form (K).\n",
    "#   metric: The distance metric to use (e.g., 'manhattan', 'euclidean'). 'manhattan' is typical for K-Medians.\n",
    "#   init: Method for initialization ('random', 'heuristic', 'k-medoids++').\n",
    "#         'random': Selects K random data points as initial medoids.\n",
    "#         'heuristic': (Often refers to a PAM-like build step or similar fast init)\n",
    "#         'k-medoids++': Similar to k-means++, aims for well-separated initial medoids.\n",
    "#   max_iter: Maximum number of iterations.\n",
    "#   random_state: For reproducibility.\n",
    "\n",
    "k = 4 # Let's try to find 4 clusters\n",
    "kmedians = KMedoids(n_clusters=k, metric='manhattan', init='k-medoids++', max_iter=300, random_state=42)\n",
    "# For a more \"classic\" K-Medians, one might use init='random' and run multiple times (n_init is not directly in sklearn_extra.KMedoids,\n",
    "# you'd loop manually or rely on 'k-medoids++' or 'heuristic' for good starts).\n",
    "\n",
    "# 4. Fitting the model and accessing results\n",
    "print(f\"\\nFitting K-Medians with K={k} using Manhattan distance...\")\n",
    "kmedians.fit(X_scaled) # Use scaled data\n",
    "\n",
    "# Accessing results:\n",
    "# .labels_: Cluster labels for each data point.\n",
    "# .cluster_centers_: Coordinates of the cluster medoids (these are rows from X_scaled).\n",
    "# .medoid_indices_: The indices in the original data of the medoids.\n",
    "# .inertia_: Sum of distances of samples to their closest cluster center (using the specified metric).\n",
    "\n",
    "print(\"\\nCluster Labels:\")\n",
    "print(kmedians.labels_)\n",
    "print(\"\\nCluster Medoids (Coordinates from scaled data):\")\n",
    "print(kmedians.cluster_centers_)\n",
    "print(\"\\nIndices of Medoids in the original (scaled) dataset:\")\n",
    "print(kmedians.medoid_indices_)\n",
    "# To see the medoids in original unscaled data:\n",
    "# original_medoids = df.iloc[kmedians.medoid_indices_]\n",
    "# print(\"\\nCluster Medoids (Original Unscaled Data):\")\n",
    "# print(original_medoids)\n",
    "print(\"\\nInertia (Sum of Manhattan distances to medoids):\")\n",
    "print(kmedians.inertia_)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "df_scaled['cluster_kmedians'] = kmedians.labels_\n",
    "df['cluster_kmedians'] = kmedians.labels_ # Also add to original for visualization if needed\n",
    "\n",
    "# --- Visualizations ---\n",
    "\n",
    "# 5. Scatter Plot of Clusters and Medoids (for 2D data)\n",
    "print(\"\\nGenerating Cluster Visualization...\")\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=df_scaled, x='Feature1', y='Feature2', hue='cluster_kmedians', palette='viridis', legend='full')\n",
    "# Plot medoids\n",
    "medoids_scaled = kmedians.cluster_centers_\n",
    "plt.scatter(medoids_scaled[:, 0], medoids_scaled[:, 1], s=200, marker='X', c='red', edgecolor='black', label='Medoids')\n",
    "plt.title(f'K-Medians Clustering (K={k}, Manhattan Distance)')\n",
    "plt.xlabel('Feature 1 (Scaled)')\n",
    "plt.ylabel('Feature 2 (Scaled)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 6. Plotting the cost (inertia) vs. K (Elbow Method)\n",
    "print(\"\\nCalculating inertia for different K values (Elbow Method)...\")\n",
    "inertia_values = []\n",
    "k_range = range(1, 10) # Test K from 1 to 9\n",
    "\n",
    "for k_val in k_range:\n",
    "    kmed_model = KMedoids(n_clusters=k_val, metric='manhattan', init='k-medoids++', max_iter=300, random_state=42)\n",
    "    kmed_model.fit(X_scaled)\n",
    "    inertia_values.append(kmed_model.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, inertia_values, marker='o')\n",
    "plt.title('Elbow Method for K-Medians (Inertia vs. K)')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia (Sum of Manhattan Distances)')\n",
    "plt.xticks(list(k_range))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# The scatter plot shows data points colored by their assigned cluster and the red 'X' markers indicate the medoids.\n",
    "# Since medoids are actual data points, they lie within their respective clusters.\n",
    "# The inertia plot helps choose K; look for an \"elbow\" where the rate of decrease in inertia slows down.\n",
    "# Medoids themselves (their feature values) describe the \"typical\" point of each cluster.\n",
    "```\n",
    "\n",
    "### Comparison with K-Means and K-Prototypes\n",
    "\n",
    "Comparing K-Medians with K-Means and K-Prototypes clarifies its specific niche.\n",
    "\n",
    "**K-Means:**\n",
    "*   **Cluster Center:** K-Means uses the **mean** of points in a cluster as its centroid. K-Medians uses a **medoid** (an actual data point within the cluster that minimizes intra-cluster sum of distances).\n",
    "*   **Distance Metric:** K-Means typically minimizes the sum of **squared Euclidean distances** (L2 norm squared). K-Medians often minimizes the sum of **Manhattan distances** (L1 norm), though other metrics can be used.\n",
    "*   **Robustness to Outliers:** K-Means is sensitive to outliers because the mean and squared Euclidean distance are heavily affected by extreme values. K-Medians, especially with Manhattan distance, is significantly **more robust to outliers**. Outliers have less influence on the medoid location and the overall sum of L1 distances.\n",
    "*   **Output Interpretability:** K-Means centroids are synthetic points (vectors of mean values). K-Medians' medoids are actual data points from the dataset, which can be more interpretable as \"exemplars\" of their clusters.\n",
    "*   **Computational Cost:** K-Means' centroid update is generally faster (O(N*m) where N is #points, m is dimensionality). K-Medians' medoid update can be more expensive (potentially O(N*N*m) in naive cases for a single cluster or requiring more complex selection if not).\n",
    "\n",
    "**K-Prototypes:**\n",
    "K-Prototypes is designed for **mixed data types** (numerical and categorical).\n",
    "*   It combines K-Means logic for numerical attributes (using means and squared Euclidean distance) and K-Modes logic for categorical attributes (using modes and simple matching dissimilarity).\n",
    "*   K-Medians is for **purely numerical data**. It could theoretically be a component in a K-Prototypes-like algorithm if one wanted robust handling of the numerical part (e.g., \"K-Medoid-Prototypes\"), but standard K-Prototypes uses K-Means for the numerical part.\n",
    "*   If your data is purely numerical and you need robustness to outliers, K-Medians is a direct choice. If you have mixed data, K-Prototypes is appropriate. K-Modes is for purely categorical data.\n",
    "\n",
    "**When to prefer K-Medians:**\n",
    "*   When the dataset is **purely numerical** and is suspected to contain **outliers** or has skewed distributions.\n",
    "*   When it's desirable for cluster centers to be **actual data points (medoids)**, enhancing interpretability.\n",
    "*   When robustness to the choice of distance metric (beyond squared Euclidean) is needed (e.g., using Manhattan distance).\n",
    "*   When the slightly higher computational cost (compared to K-Means) is acceptable for the benefit of robustness.\n",
    "\n",
    "### Preprocessing and Data Handling\n",
    "\n",
    "Effective preprocessing is critical for K-Medians.\n",
    "\n",
    "**Feature Scaling:**\n",
    "*   This is arguably the **most important preprocessing step for K-Medians** (and K-Means). Since K-Medians relies on distance calculations (e.g., Manhattan distance), attributes with larger scales (e.g., salary in dollars vs. age in years) will disproportionately influence the distances and thus the clustering outcome.\n",
    "*   **Standardization (Z-score scaling):** Transforms data to have a mean of 0 and standard deviation of 1. `sklearn.preprocessing.StandardScaler` is commonly used. Good if you assume data is somewhat normally distributed or want to handle outliers by reducing their influence relatively.\n",
    "*   **Normalization (Min-Max scaling):** Scales data to a specific range, usually [0, 1]. `sklearn.preprocessing.MinMaxScaler`. Can be sensitive if outliers create very large or small min/max values.\n",
    "*   The choice of scaling method can impact results, but some form of scaling is almost always necessary.\n",
    "\n",
    "**Handling Missing Data:**\n",
    "Missing values in numerical attributes must be handled before applying K-Medians.\n",
    "*   **Imputation:**\n",
    "    *   **Mean/Median Imputation:** Replace missing values with the mean or median of the respective attribute. Median imputation is often preferred if outliers are present, aligning with K-Medians' philosophy.\n",
    "    *   **K-Nearest Neighbors (KNN) Imputation:** Impute missing values based on the values of their k-nearest neighbors.\n",
    "    *   Model-based imputation (e.g., using regression).\n",
    "*   **Row Deletion:** If only a very small fraction of data points have missing values, and the dataset is large, deletion might be an option, but it's generally less preferred.\n",
    "\n",
    "**Feature Selection/Engineering:**\n",
    "*   Removing irrelevant or redundant numerical features can improve clustering quality and reduce computational cost.\n",
    "*   Engineering new, more informative numerical features from existing ones can sometimes enhance cluster separation.\n",
    "\n",
    "### Applications of K-Medians\n",
    "\n",
    "K-Medians' robustness makes it suitable for various applications where numerical data might be noisy or contain outliers.\n",
    "\n",
    "1.  **Customer Segmentation:** Grouping customers based on numerical purchase data (e.g., spending amount, frequency of purchase, recency) where some customers might be extreme outliers (e.g., very high spenders or very infrequent ones). The medoids would represent typical customer profiles resilient to these extremes.\n",
    "2.  **Anomaly Detection:** While primarily a clustering algorithm, points that are very far from any cluster medoid after convergence can be considered potential anomalies or outliers. This is because K-Medians tries to find representative medoids that are not easily swayed by a few extreme points.\n",
    "3.  **Image Segmentation/Color Quantization:** Clustering pixel colors (represented as RGB vectors) to reduce the number of distinct colors in an image. K-Medians can be more robust to outlier pixels (e.g., noise) than K-Means, leading to more stable representative colors (medoids).\n",
    "4.  **Bioinformatics:** Clustering gene expression data or other biological measurements that can be noisy or contain experimental outliers. K-Medians can help identify robust groups of genes or samples with similar numerical profiles.\n",
    "5.  **Sensor Data Analysis:** Clustering numerical readings from sensors where some sensors might occasionally provide erroneous or extreme values (outliers). K-Medians can help find typical operational states or patterns that are not skewed by these faulty readings.\n",
    "6.  **Financial Data Analysis:** Analyzing financial ratios or market data where extreme events or specific entities can create outliers. K-Medians can help in forming more stable clusters of companies or assets.\n",
    "\n",
    "In summary, K-Medians is a valuable clustering algorithm for numerical datasets, particularly when robustness to outliers and interpretable, data-point exemplars (medoids) are desired. Its careful application, including proper preprocessing like feature scaling, can yield meaningful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c1f26-89bd-4738-99f2-ec7f920a6a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
