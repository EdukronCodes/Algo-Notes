{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2439987a-09b0-4536-8245-a48112dca120",
   "metadata": {},
   "source": [
    "Okay, here are detailed, structured, and deeply explanatory notes on K-Prototypes Clustering, addressing each sub-topic as requested.\n",
    "\n",
    "---\n",
    "\n",
    "## K-Prototypes Clustering: Comprehensive Notes\n",
    "\n",
    "### Introduction to K-Prototypes Clustering\n",
    "\n",
    "K-Prototypes is a powerful unsupervised machine learning algorithm specifically engineered to perform clustering on datasets that contain a **mix of numerical (continuous) and categorical (discrete) features**. This mixed-data scenario is extremely common in real-world applications, such as customer databases, patient records, or product inventories, where attributes like age (numerical) and gender (categorical) coexist. The algorithm stands out because many traditional clustering algorithms, like K-Means, are designed primarily for numerical data, while others, like K-Modes, are for categorical data. K-Prototypes bridges this gap by offering an integrated solution.\n",
    "\n",
    "Its **hybrid nature** is a key characteristic. It intelligently and cohesively combines the core principles of two well-known partitioning clustering algorithms: K-Means for handling the numerical features and K-Modes for handling the categorical features. For numerical attributes, it uses squared Euclidean distance to measure dissimilarity and calculates cluster centers (means). For categorical attributes, it employs a simple matching dissimilarity (Hamming distance for nominal attributes) and identifies cluster centers (modes, i.e., the most frequent categories).\n",
    "\n",
    "The primary **objective of K-Prototypes** is to partition a given dataset of `n` mixed-type data points into `K` distinct, non-overlapping clusters. This partitioning is achieved by minimizing a combined cost function, which is the sum of dissimilarities of each point to the prototype of its assigned cluster. The cluster centers, referred to as \"prototypes,\" are themselves mixed-type: the numerical components of a prototype are the means of the numerical features of the points in that cluster, and the categorical components are the modes of the categorical features of the points in that cluster.\n",
    "\n",
    "The **utility of K-Prototypes** is most apparent when dealing with datasets that naturally contain both types of information. Applying K-Means after one-hot encoding categorical features can lead to high dimensionality and potentially less meaningful distance calculations, while K-Modes would ignore valuable numerical information. K-Prototypes provides a more direct and potentially more interpretable way to cluster such mixed-type data, allowing for richer insights by considering all available feature types simultaneously in a balanced manner. Its ability to handle these mixed data types natively makes it a valuable tool in exploratory data analysis and pattern discovery.\n",
    "\n",
    "---\n",
    "\n",
    "### The K-Prototypes Algorithm Workflow (Detailed Explanation)\n",
    "\n",
    "The K-Prototypes algorithm iteratively refines cluster assignments and prototype locations to minimize the overall dissimilarity within clusters. Its workflow is an extension of the Expectation-Maximization (EM) like procedure found in K-Means and K-Modes.\n",
    "\n",
    "1.  **Initialization:**\n",
    "    *   **Methods for selecting K initial prototypes:** The algorithm begins by selecting `K` initial prototypes, where `K` is a user-defined number of clusters. Common initialization strategies include:\n",
    "        *   **Random Selection:** Selecting `K` data points randomly from the dataset to serve as the initial prototypes. This is simple but can lead to poor convergence or suboptimal solutions.\n",
    "        *   **Huang's Initialization (1997, 1998):** A more sophisticated method adapted for K-Modes and K-Prototypes. It aims to select initial modes/prototypes that are well-separated by first calculating frequencies of categories and then selecting initial points based on these frequencies and distances. This method tends to be less sensitive to the order of input data points and often leads to better clustering results compared to random initialization.\n",
    "        *   **Cao's Initialization (2009):** Another density-based method that focuses on selecting initial prototypes based on the density of their surrounding data points and their distance from already chosen prototypes. This method often improves clustering quality and convergence speed.\n",
    "        The `kmodes` library provides options like `'Huang'`, `'Cao'`, and `'random'`.\n",
    "    *   **Prototype Structure:** Each initial prototype is a mixed-type vector. Its numerical components are initialized using the numerical values of the chosen data point (or means if aggregated initialization is used), and its categorical components are initialized using the categorical values of that chosen point.\n",
    "    *   **Importance of Good Initialization:** Like K-Means and K-Modes, K-Prototypes is sensitive to the initial placement of prototypes. A poor initialization can lead to convergence to a local optimum rather than the global optimum, resulting in suboptimal cluster quality. Running the algorithm multiple times with different initializations (`n_init` parameter in implementations) and choosing the solution with the lowest cost is a common best practice.\n",
    "\n",
    "2.  **Distance/Dissimilarity Calculation:**\n",
    "    This is a cornerstone of K-Prototypes, defining how \"similarity\" or \"closeness\" is measured between a data point and a prototype, considering both feature types. The combined dissimilarity measure `d(X, P)` between a data point `X` and a prototype `P` is given by:\n",
    "    `d(X, P) = d_numeric(X_num, P_num) + γ * d_categorical(X_cat, P_cat)`\n",
    "    *   **`d_numeric(X_num, P_num)`:** This is the dissimilarity for the numerical part. It is typically the **squared Euclidean distance** between the numerical components of the data point (`X_num`) and the numerical components of the prototype (`P_num`). If `X_num = (x_1, ..., x_m)` and `P_num = (p_1, ..., p_m)`, then `d_numeric = Σ_{j=1}^{m} (x_j - p_j)^2`.\n",
    "    *   **`d_categorical(X_cat, P_cat)`:** This is the dissimilarity for the categorical part. It is usually the **simple matching dissimilarity** (or Hamming distance) between the categorical components of the data point (`X_cat`) and the prototype (`P_cat`). If `X_cat = (c_1, ..., c_s)` and `P_cat = (q_1, ..., q_s)`, then `d_categorical = Σ_{j=1}^{s} δ(c_j, q_j)`, where `δ(a, b) = 0` if `a = b` and `δ(a, b) = 1` if `a ≠ b`. This simply counts the number of mismatches.\n",
    "    *   **`γ` (gamma):** This is a crucial **weighting parameter**. It balances the influence of the numerical features versus the categorical features in the total dissimilarity calculation. A higher `γ` gives more weight to the categorical attributes, meaning mismatches in categorical features contribute more to the total dissimilarity. Conversely, a lower `γ` (closer to 0) gives more weight to numerical attributes. If `γ = 0`, categorical attributes are effectively ignored (reducing to K-Means on numerical parts). If numerical variance is very low or `γ` is very high, it approaches K-Modes. The choice of `γ` is critical and data-dependent.\n",
    "\n",
    "3.  **Assignment Step:**\n",
    "    In this step, each data point `X_i` in the dataset is assigned to the cluster `C_l` whose prototype `P_l` is \"closest\" to `X_i`. The \"closeness\" is determined by the combined dissimilarity measure `d(X_i, P_l)` defined above. Formally, data point `X_i` is assigned to cluster `C_l` if `d(X_i, P_l) ≤ d(X_i, P_j)` for all `j = 1, ..., K`. This step effectively groups data points that are similar in both their numerical aspects (e.g., close in Euclidean space) and their categorical aspects (e.g., share many common categories), with the relative importance of these aspects being modulated by the `γ` parameter. This ensures a holistic similarity assessment, respecting the distinct nature of each feature type.\n",
    "\n",
    "4.  **Prototype Update Step:**\n",
    "    After all data points have been assigned to clusters, the prototypes themselves are re-calculated to better represent the current members of their respective clusters. For each cluster `C_l`:\n",
    "    *   **Numerical Components Update:** The numerical components of the prototype `P_l` are updated by calculating the **mean** of the numerical feature values for all data points currently assigned to cluster `C_l`. This is identical to the centroid update rule in K-Means. If `X_{i,num}` are the numerical parts of points in `C_l`, then `P_{l,num,j} = (1/|C_l|) * Σ_{X_i ∈ C_l} X_{i,num,j}` for each numerical feature `j`.\n",
    "    *   **Categorical Components Update:** The categorical components of the prototype `P_l` are updated by finding the **mode** (the most frequent category) for each categorical feature among all data points currently assigned to cluster `C_l`. This is identical to the mode update rule in K-Modes. For each categorical feature `j`, `P_{l,cat,j}` becomes the category that appears most often for that feature among points in `C_l`. Ties in modes can be broken randomly or by some predefined rule.\n",
    "\n",
    "5.  **Convergence Criteria:**\n",
    "    The Assignment and Prototype Update steps are repeated iteratively until one or more convergence criteria are met. Common criteria include:\n",
    "    *   **No (or minimal) change in prototype positions:** The algorithm stops if the prototypes (both numerical means and categorical modes) do not change significantly between iterations.\n",
    "    *   **No (or minimal) change in point assignments:** If data points no longer switch clusters between iterations, the algorithm has stabilized.\n",
    "    *   **Maximum number of iterations reached:** A predefined limit on the number of iterations is often set to prevent excessively long runtimes, especially if convergence is slow.\n",
    "    *   **Cost function stabilization:** The total within-cluster sum of dissimilarities (the objective function) decreases or stays the same with each iteration. Convergence is declared when the improvement in the cost function falls below a small threshold.\n",
    "\n",
    "This iterative process ensures that the prototypes gradually move towards regions of higher data density, and cluster assignments become more stable, ultimately partitioning the data based on its mixed-feature characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Basis and the Gamma (γ) Parameter\n",
    "\n",
    "The K-Prototypes algorithm is mathematically grounded in the minimization of a specific objective function that quantifies the total dissimilarity within clusters. This function accounts for both numerical and categorical feature types.\n",
    "\n",
    "1.  **Objective Function:**\n",
    "    The K-Prototypes algorithm aims to minimize the following cost function, `J`, which represents the sum of the combined dissimilarities from each data point to the prototype of its assigned cluster:\n",
    "\n",
    "    `J = Σ_{l=1}^{K} Σ_{X_i ∈ C_l} d(X_i, P_l)`\n",
    "\n",
    "    where:\n",
    "    *   `K` is the number of clusters.\n",
    "    *   `C_l` is the set of data points assigned to cluster `l`.\n",
    "    *   `P_l` is the prototype of cluster `l`.\n",
    "    *   `d(X_i, P_l)` is the combined dissimilarity between data point `X_i` and prototype `P_l`, further defined as:\n",
    "        `d(X_i, P_l) = d_numeric(X_{i,num}, P_{l,num}) + γ * d_categorical(X_{i,cat}, P_{l,cat})`\n",
    "        *   `d_numeric(X_{i,num}, P_{l,num}) = Σ_{j=1}^{m} (X_{i,num,j} - P_{l,num,j})^2` (sum of squared Euclidean distances for `m` numerical features).\n",
    "        *   `d_categorical(X_{i,cat}, P_{l,cat}) = Σ_{j=1}^{s} δ(X_{i,cat,j}, P_{l,cat,j})` (sum of simple matching dissimilarities for `s` categorical features, where `δ(a,b)=1` if `a≠b`, `0` otherwise).\n",
    "\n",
    "    The algorithm iteratively performs the assignment and update steps, each designed to reduce this objective function `J` until a local minimum is reached. The assignment step assigns each point to the cluster that minimizes its contribution to `J`. The update step recalculates prototypes to be the \"centers\" (mean/mode) of their current member points, which also aims to minimize `J` for the given assignments.\n",
    "\n",
    "2.  **The Gamma (γ) Parameter:**\n",
    "    The gamma (`γ`) parameter is a non-negative weight that plays a pivotal role in the K-Prototypes algorithm.\n",
    "    *   **Significance:** `γ` controls the **trade-off** between the contribution of numerical attributes and categorical attributes to the total dissimilarity measure. It effectively scales the categorical dissimilarity relative to the numerical dissimilarity. This is essential because numerical distances (e.g., squared Euclidean) and categorical distances (e.g., count of mismatches) are on different scales and have different interpretations. Without `γ`, one type of feature might inherently dominate the clustering process simply due to the nature of its scale or variance.\n",
    "    *   **Choosing/Tuning Gamma:**\n",
    "        *   **Heuristic Setting:** A common heuristic, proposed by Huang (the originator of K-Prototypes), is to set `γ` based on the data itself. For instance, `γ` can be set to half the average standard deviation of the numerical features, or a value within the range `[1/3 * avg_std_dev, 2/3 * avg_std_dev]`. The `kmodes` library can automatically estimate `γ` if it's not provided, often using a similar heuristic related to the variance of numerical features.\n",
    "        *   **Domain Expertise:** If domain knowledge suggests that one type of feature is more important for defining clusters, `γ` can be adjusted accordingly. If categorical features are deemed more discriminative, `γ` should be increased. If numerical features are more critical, `γ` should be decreased.\n",
    "        *   **Experimentation:** Often, `γ` is tuned via experimentation. This involves running K-Prototypes with different values of `γ` and evaluating the resulting clusters based on metrics like silhouette scores (adapted for mixed data), cluster stability, or, most importantly, the interpretability and meaningfulness of the clusters in the context of the problem.\n",
    "    *   **Impact of Extreme Values:**\n",
    "        *   If `γ = 0`, the categorical dissimilarity term `γ * d_categorical` becomes zero. The algorithm then effectively performs K-Means clustering only on the numerical features, ignoring all categorical information.\n",
    "        *   If numerical features have very little variance (or are removed), or if `γ` is set to a very large value, the numerical dissimilarity term becomes negligible compared to the categorical term. The algorithm's behavior then approaches that of K-Modes clustering, focusing primarily on categorical attributes.\n",
    "    *   **Inappropriate Gamma:** Choosing an inappropriate `γ` can lead to poor clustering results. If `γ` is too small, numerical features might dominate even if categorical distinctions are important. If `γ` is too large, categorical features might overshadow significant numerical patterns. The goal is to find a balance that reflects the true underlying structure of the mixed-type data. This tuning process is one of the practical challenges of using K-Prototypes.\n",
    "\n",
    "---\n",
    "\n",
    "### Strengths and Limitations\n",
    "\n",
    "K-Prototypes offers a specialized solution for mixed-type data, but like any algorithm, it comes with its own set of advantages and disadvantages.\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "1.  **Handles Mixed Data Types Natively:** This is its primary and most significant advantage. K-Prototypes is specifically designed to cluster datasets containing both numerical and categorical features without requiring complex pre-processing like one-hot encoding all categorical variables (which can inflate dimensionality) or forcing categorical variables into a less meaningful numerical scale.\n",
    "2.  **Relatively Simple and Interpretable:** The algorithm extends the well-understood concepts of K-Means (for numerical parts) and K-Modes (for categorical parts). The resulting prototypes, consisting of means for numerical attributes and modes for categorical attributes, are generally easy to interpret, providing a clear profile for each cluster. This makes it easier to understand the characteristics of the identified groups.\n",
    "3.  **Efficient for Large Datasets:** K-Prototypes generally inherits computational efficiency from K-Means and K-Modes. Its time complexity is typically linear with respect to the number of data points, number of features, number of clusters, and number of iterations (`O(N*K*M*I)`, where `N` is data points, `K` clusters, `M` features, `I` iterations). This makes it scalable to moderately large datasets, especially compared to some more complex methods like Gower's distance with hierarchical clustering.\n",
    "4.  **Flexibility with Gamma Parameter:** The `γ` parameter, while requiring tuning, offers flexibility in controlling the relative importance of numerical versus categorical features, allowing users to tailor the clustering process to the specific characteristics of their data and the problem domain.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "1.  **Need to Pre-specify K:** Like K-Means and K-Modes, the number of clusters, `K`, must be specified beforehand. Determining the optimal `K` can be challenging and often requires domain knowledge or the use of heuristic methods like the elbow method or silhouette analysis, which themselves have limitations.\n",
    "2.  **Sensitivity to Initialization:** The algorithm can converge to local optima rather than the global optimum, meaning the quality of the final clusters can depend heavily on the initial selection of prototypes. To mitigate this, it's standard practice to run the algorithm multiple times with different random initializations (`n_init > 1`) and choose the clustering solution with the lowest cost (total dissimilarity). More sophisticated initialization methods (e.g., Huang's, Cao's) also help.\n",
    "3.  **Tuning the Gamma (γ) Parameter:** Finding the optimal value for the `γ` parameter, which balances the influence of numerical and categorical features, can be tricky and data-dependent. It often requires trial and error, experimentation, or deep domain expertise. An inappropriate `γ` can lead to one feature type dominating the clustering process, masking relevant patterns in the other type.\n",
    "4.  **Assumptions from K-Means/K-Modes:** K-Prototypes inherits some assumptions from its parent algorithms. For numerical features, it implicitly assumes clusters are roughly spherical and of similar size due to the use of means and Euclidean distance. For categorical features, the simple matching dissimilarity treats all mismatches equally and may not capture more nuanced relationships between categories. It may struggle with clusters of arbitrary shapes or varying densities.\n",
    "5.  **Impact of Feature Scaling (Numerical):** Numerical features with larger values or variances can disproportionately influence the Euclidean distance calculation if not properly scaled. Therefore, scaling numerical features (e.g., standardization or normalization) is a crucial preprocessing step, similar to K-Means.\n",
    "6.  **Handling of Categorical Modes:** When updating categorical components of prototypes, if multiple categories have the same highest frequency (tied modes), the choice of which one becomes the mode can be arbitrary (e.g., the first one encountered). This can slightly affect stability in some edge cases, though usually, it's a minor issue.\n",
    "\n",
    "---\n",
    "\n",
    "### Python Implementation with `kmodes` library\n",
    "\n",
    "The `kmodes` Python library provides a convenient implementation of K-Prototypes. Let's walk through an example using a synthetic dataset.\n",
    "\n",
    "**Code Walkthrough:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Data Loading and Preparation (Synthetic Data)\n",
    "# Let's create a synthetic dataset with mixed features\n",
    "np.random.seed(42) # for reproducibility\n",
    "n_samples = 300\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Age': np.random.randint(20, 65, n_samples),\n",
    "    'Income': np.random.normal(50000, 15000, n_samples).astype(int),\n",
    "    'Gender': np.random.choice(['Male', 'Female'], n_samples, p=[0.5, 0.5]),\n",
    "    'Education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
    "    'Has_Children': np.random.choice([0, 1], n_samples, p=[0.6, 0.4]) # 0: No, 1: Yes\n",
    "})\n",
    "\n",
    "# Add some structure for clustering (example modification)\n",
    "# Cluster 0: Younger, lower income, often HS/Bachelor, more likely no children\n",
    "data.loc[data['Age'] < 35, 'Income'] -= 10000\n",
    "data.loc[data['Age'] < 35, 'Education'] = np.random.choice(['High School', 'Bachelor'], data[data['Age'] < 35].shape[0])\n",
    "\n",
    "# Cluster 1: Middle-aged, mid-high income, often Bachelor/Master, mixed children status\n",
    "data.loc[(data['Age'] >= 35) & (data['Age'] < 50), 'Income'] += 5000\n",
    "data.loc[(data['Age'] >= 35) & (data['Age'] < 50), 'Education'] = np.random.choice(['Bachelor', 'Master'], data[(data['Age'] >= 35) & (data['Age'] < 50)].shape[0])\n",
    "\n",
    "\n",
    "# Cluster 2: Older, higher income, often Master/PhD, more likely has children\n",
    "data.loc[data['Age'] >= 50, 'Income'] += 15000\n",
    "data.loc[data['Age'] >= 50, 'Education'] = np.random.choice(['Master', 'PhD'], data[data['Age'] >= 50].shape[0])\n",
    "data.loc[data['Age'] >= 50, 'Has_Children'] = np.random.choice([0, 1], data[data['Age'] >= 50].shape[0], p=[0.2, 0.8])\n",
    "\n",
    "\n",
    "# Ensure income is not negative\n",
    "data['Income'] = data['Income'].clip(lower=10000)\n",
    "\n",
    "print(\"Original Data Head:\")\n",
    "print(data.head())\n",
    "print(\"\\nData Types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# Identify numerical and categorical column indices\n",
    "numerical_cols = ['Age', 'Income']\n",
    "categorical_cols = ['Gender', 'Education', 'Has_Children']\n",
    "\n",
    "# Get indices of categorical columns\n",
    "# Important: KPrototypes expects a list of *indices* of categorical columns\n",
    "categorical_col_indices = [data.columns.get_loc(col) for col in categorical_cols]\n",
    "print(f\"\\nCategorical column indices: {categorical_col_indices}\")\n",
    "\n",
    "# Make a copy for clustering\n",
    "data_cluster = data.copy()\n",
    "\n",
    "# 2. Preprocessing\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "data_cluster[numerical_cols] = scaler.fit_transform(data_cluster[numerical_cols])\n",
    "\n",
    "print(\"\\nScaled Data Head (Numerical Features):\")\n",
    "print(data_cluster[numerical_cols].head())\n",
    "\n",
    "# Categorical features are often strings/objects. kmodes handles them directly.\n",
    "# If they were numbers representing categories, ensure they are treated as such.\n",
    "# For 'Has_Children', it's 0/1 but represents categories.\n",
    "# The kmodes library will treat columns specified in `categorical` as categorical,\n",
    "# regardless of their original dtype if they can be compared for equality.\n",
    "\n",
    "# 3. Instantiating KPrototypes\n",
    "# Parameters:\n",
    "# - n_clusters: The number of clusters to form (K)\n",
    "# - init: Method for initialization ('Cao', 'Huang', 'random')\n",
    "# - n_init: Number of time the K-Prototypes algorithm will be run with different centroid seeds.\n",
    "#           The final results will be the best output of n_init consecutive runs in terms of cost.\n",
    "# - gamma: Weighting factor for categorical features.\n",
    "#          If None, it's automatically estimated based on the std of numerical features.\n",
    "# - verbose: Verbosity mode.\n",
    "# - random_state: for reproducibility of initializations\n",
    "kproto = KPrototypes(n_clusters=3,\n",
    "                     init='Cao',      # Cao initialization is often good\n",
    "                     n_init=10,       # Run 10 times with different initializations\n",
    "                     gamma=None,      # Let the model estimate gamma (or set it, e.g., 0.5 * data_cluster[numerical_cols].std().mean())\n",
    "                     verbose=0,       # Set to 1 or 2 for more output during fitting\n",
    "                     random_state=42)\n",
    "\n",
    "# 4. Fitting the model and accessing results\n",
    "# The `categorical` parameter in `fit_predict` takes the list of categorical column indices.\n",
    "# It's crucial to pass this so the algorithm knows which columns to treat as categorical.\n",
    "clusters = kproto.fit_predict(data_cluster.values, categorical=categorical_col_indices)\n",
    "\n",
    "# Accessing results\n",
    "labels = kproto.labels_\n",
    "cluster_centroids = kproto.cluster_centroids_\n",
    "cost = kproto.cost_\n",
    "\n",
    "print(f\"\\nCluster labels for first 10 points: {labels[:10]}\")\n",
    "print(f\"\\nCost (total dissimilarity): {cost}\")\n",
    "\n",
    "# The cluster_centroids_ attribute returns an array where:\n",
    "# - Rows correspond to clusters.\n",
    "# - Columns correspond to features in their original order.\n",
    "# - Numerical feature values are the means (on the scaled data).\n",
    "# - Categorical feature values are the modes.\n",
    "print(\"\\nCluster Prototypes (centroids):\")\n",
    "print(\"Note: Numerical parts are scaled, categorical parts are original values.\")\n",
    "# For easier interpretation, create a DataFrame for prototypes\n",
    "# Numerical parts need inverse transform if we want to see original scale\n",
    "# Categorical parts are directly interpretable\n",
    "num_centroids_scaled = cluster_centroids[:, [data.columns.get_loc(col) for col in numerical_cols]]\n",
    "cat_centroids = cluster_centroids[:, categorical_col_indices]\n",
    "\n",
    "# Inverse transform numerical centroids to original scale\n",
    "num_centroids_original_scale = scaler.inverse_transform(num_centroids_scaled.astype(float)) # Ensure float for inverse_transform\n",
    "\n",
    "# Combine into a nice DataFrame\n",
    "prototype_df_list = []\n",
    "for i in range(kproto.n_clusters):\n",
    "    proto_num = num_centroids_original_scale[i]\n",
    "    proto_cat = cat_centroids[i]\n",
    "    proto_dict = {}\n",
    "    for j, col_name in enumerate(numerical_cols):\n",
    "        proto_dict[col_name] = proto_num[j]\n",
    "    for j, col_name in enumerate(categorical_cols):\n",
    "        proto_dict[col_name] = proto_cat[j]\n",
    "    prototype_df_list.append(proto_dict)\n",
    "\n",
    "prototypes_df = pd.DataFrame(prototype_df_list)\n",
    "print(\"\\nInterpretable Cluster Prototypes (Numerical in original scale, Categorical are modes):\")\n",
    "print(prototypes_df)\n",
    "\n",
    "# Add cluster labels to the original (unscaled) data for analysis\n",
    "data_original_with_labels = data.copy()\n",
    "data_original_with_labels['Cluster'] = labels\n",
    "\n",
    "# 5. Visualizations\n",
    "\n",
    "# Cluster Profiles:\n",
    "print(\"\\n--- Cluster Profiles ---\")\n",
    "for i in range(kproto.n_clusters):\n",
    "    print(f\"\\n--- Profile for Cluster {i} ---\")\n",
    "    cluster_data = data_original_with_labels[data_original_with_labels['Cluster'] == i]\n",
    "    \n",
    "    # Numerical features: Show means/medians or boxplots\n",
    "    print(\"Numerical Feature Means:\")\n",
    "    print(cluster_data[numerical_cols].mean())\n",
    "    \n",
    "    # Categorical features: Show bar plots of category frequencies (modes are in prototypes_df)\n",
    "    print(\"\\nCategorical Feature Mode (from prototype):\")\n",
    "    print(prototypes_df.loc[i, categorical_cols])\n",
    "    \n",
    "    # Detailed frequency plots for categorical features per cluster\n",
    "    for col in categorical_cols:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.countplot(data=cluster_data, x=col, order=data[col].unique()) # Use original data unique values for consistent order\n",
    "        plt.title(f'Cluster {i}: Frequency of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        # plt.show() # Uncomment to display plots one by one\n",
    "        print(f\"Plot generated for Cluster {i} - {col} (not shown in text output)\")\n",
    "        plt.close() # Close plot to prevent display in non-interactive environment\n",
    "\n",
    "# Boxplots for numerical features by cluster\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=data_original_with_labels, x='Cluster', y=col)\n",
    "    plt.title(f'{col} Distribution by Cluster')\n",
    "    # plt.show()\n",
    "    print(f\"Boxplot generated for {col} by Cluster (not shown in text output)\")\n",
    "    plt.close()\n",
    "\n",
    "# Example of 2D projection (using PCA on numerical features for simplicity)\n",
    "if len(numerical_cols) >= 2:\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca_num_data = data_cluster[numerical_cols].copy() # Use scaled numerical data for PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(pca_num_data)\n",
    "    pc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "    pc_df['Cluster'] = labels\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pc_df, palette='viridis', s=50, alpha=0.7)\n",
    "    plt.title('Clusters visualized with PCA on Numerical Features (Scaled)')\n",
    "    # plt.show()\n",
    "    print(\"PCA plot generated (not shown in text output)\")\n",
    "    plt.close()\n",
    "\n",
    "# Plotting the cost vs. K (Elbow method)\n",
    "# This requires running KPrototypes for different K values\n",
    "costs = []\n",
    "k_values = range(2, 8) # Example range for K\n",
    "print(\"\\nCalculating costs for different K values (Elbow Method)...\")\n",
    "for k_val in k_values:\n",
    "    print(f\"  Running for K={k_val}\")\n",
    "    temp_kproto = KPrototypes(n_clusters=k_val, init='Cao', n_init=5, gamma=kproto.gamma_, verbose=0, random_state=42) # Use estimated gamma\n",
    "    temp_kproto.fit_predict(data_cluster.values, categorical=categorical_col_indices)\n",
    "    costs.append(temp_kproto.cost_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, costs, marker='o')\n",
    "plt.title('Elbow Method for Optimal K (Cost vs. K)')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Cost (Total Within-Cluster Dissimilarity)')\n",
    "plt.xticks(k_values)\n",
    "# plt.show()\n",
    "print(\"Elbow plot generated (not shown in text output)\")\n",
    "plt.close()\n",
    "\n",
    "# Exploring different gamma values (example)\n",
    "# This would involve a similar loop, fixing K and varying gamma\n",
    "# E.g., for K=3:\n",
    "# gammas = [0.1, 0.5, 1.0, 5.0, 10.0, kproto.gamma_] # kproto.gamma_ is the auto-estimated one\n",
    "# for g_val in gammas:\n",
    "#     temp_kproto_gamma = KPrototypes(n_clusters=3, init='Cao', n_init=5, gamma=g_val, verbose=0)\n",
    "#     clusters_g = temp_kproto_gamma.fit_predict(data_cluster.values, categorical=categorical_col_indices)\n",
    "#     print(f\"Gamma: {g_val}, Cost: {temp_kproto_gamma.cost_}\")\n",
    "#     # Further evaluation needed (e.g., silhouette, interpretability)\n",
    "\n",
    "# Interpreting mixed-type prototypes:\n",
    "# The `prototypes_df` provides a clear summary:\n",
    "# - Cluster 0 might be younger individuals, lower income, with specific gender/education modes.\n",
    "# - Cluster 1 might be middle-aged, higher income, different education patterns.\n",
    "# This helps in creating personas or understanding segment characteristics.\n",
    "# For example, if prototypes_df shows Cluster 0 has Age mean ~28, Income mean ~35k, Gender mode 'Female', Education mode 'Bachelor',\n",
    "# this defines a \"young, lower-to-mid income, female, bachelor-educated\" segment.\n",
    "```\n",
    "\n",
    "**Explanation of Code and Visualizations:**\n",
    "\n",
    "*   **Import Libraries:** Standard data science libraries along with `KPrototypes`.\n",
    "*   **Data Preparation:** A synthetic dataset is created. Numerical and categorical columns are identified, and critically, the *indices* of categorical columns are stored. This is essential for the `KPrototypes` model.\n",
    "*   **Preprocessing (Scaling Numerical Features):** Numerical features (`Age`, `Income`) are scaled using `StandardScaler`. This prevents features with larger magnitudes/variances from dominating the Euclidean distance calculation. Categorical features are left as is; `kmodes` handles string/object types directly by using simple matching dissimilarity.\n",
    "*   **Instantiating `KPrototypes`:**\n",
    "    *   `n_clusters`: Set to the desired number of clusters (here, 3). This usually requires prior analysis (e.g., elbow method).\n",
    "    *   `init='Cao'`: Uses Cao's initialization method, generally robust.\n",
    "    *   `n_init=10`: Runs the algorithm 10 times with different random seeds for initialization and picks the best result (lowest cost). This helps avoid poor local optima.\n",
    "    *   `gamma=None`: Allows the library to automatically estimate a suitable `gamma` based on the variance of numerical data. Alternatively, a specific float value can be provided.\n",
    "    *   `verbose=0`: Suppresses output during fitting.\n",
    "*   **Fitting the Model:** The `fit_predict` method is called on the preprocessed data (numerical features scaled, categorical features as they are). The `categorical` argument is crucial, taking the list of integer indices of the categorical columns. It returns the cluster assignments for each data point.\n",
    "*   **Accessing Results:**\n",
    "    *   `labels_`: An array of cluster labels for each data point.\n",
    "    *   `cluster_centroids_`: An array containing the prototypes. Numerical parts are scaled means, categorical parts are modes. We then inverse-transform the numerical parts to original scale for better interpretability.\n",
    "    *   `cost_`: The final sum of dissimilarities.\n",
    "*   **Visualizations for Cluster Profiles (Key for Interpretation):**\n",
    "    *   **Numerical Features:** For each cluster, we calculate and print the means of numerical features (from the original, unscaled data for that cluster). Boxplots (`sns.boxplot`) are excellent for comparing distributions of numerical features across clusters.\n",
    "    *   **Categorical Features:** For each cluster, the modes are directly available from the `prototypes_df`. Frequency plots (`sns.countplot`) for each categorical feature within each cluster show the distribution of categories, highlighting the mode and other prevalent categories.\n",
    "*   **2D Projection (PCA):** If there are at least two numerical features, PCA can be applied to the (scaled) numerical part to project it onto 2 dimensions. These can then be plotted and colored by cluster label. This gives a visual sense of separation for the numerical component, but it's important to remember it doesn't capture the categorical aspect's contribution to clustering.\n",
    "*   **Plotting Cost vs. K (Elbow Method):** To help choose `K`, the algorithm is run for a range of `K` values, and the `cost_` is plotted. An \"elbow\" or point of diminishing returns in the plot can suggest a suitable `K`.\n",
    "*   **Interpreting Prototypes:** The `prototypes_df` (with numerical features back-scaled) is the most direct way to understand clusters. Each row is a prototype: numerical values are means, categorical values are modes. This profile (e.g., \"Cluster 0: Young, low-income, mostly female, bachelor's degree, no children\") is the primary output for understanding cluster characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "### Preprocessing for K-Prototypes\n",
    "\n",
    "Proper preprocessing is vital for K-Prototypes to perform effectively, ensuring that both numerical and categorical features contribute appropriately to the clustering process.\n",
    "\n",
    "1.  **Scaling Numeric Features:**\n",
    "    *   **Necessity:** Numerical features often exist on vastly different scales (e.g., age in years, income in tens of thousands). The Euclidean distance component `d_numeric` is sensitive to these scales. Features with larger values or higher variances can dominate the distance calculation, effectively overshadowing the influence of other numerical features or even the categorical component if `gamma` is not carefully tuned. Scaling brings all numerical features to a comparable range.\n",
    "    *   **Methods:**\n",
    "        *   **Standardization (Z-score scaling):** Transforms data to have a mean of 0 and a standard deviation of 1 (`(x - mean) / std_dev`). `sklearn.preprocessing.StandardScaler` is commonly used. This is generally preferred if the data follows a Gaussian distribution or when outliers are less of a concern.\n",
    "        *   **Normalization (Min-Max scaling):** Rescales features to a specific range, typically [0, 1] (`(x - min) / (max - min)`). `sklearn.preprocessing.MinMaxScaler` is used. This is useful when the algorithm expects features within a bounded interval or when the distribution is not Gaussian.\n",
    "    *   **Best Practice:** Standardization is often a good default for K-Prototypes' numerical part, as K-Means (which it emulates for numerical data) works well with standardized data.\n",
    "\n",
    "2.  **Encoding Categorical Features:**\n",
    "    *   **`kmodes` Library Handling:** The `kmodes.kprototypes.KPrototypes` implementation is quite convenient as it can directly handle categorical features represented as strings (object dtype in Pandas) or numerical codes (e.g., integers representing categories). The key is to specify which columns are categorical using the `categorical` parameter (list of column indices) during model fitting.\n",
    "    *   **No Need for One-Hot Encoding (OHE):** Unlike algorithms that require all input to be numerical (like standard K-Means), K-Prototypes uses the simple matching dissimilarity for categorical features. OHE creates many new binary columns, which can increase dimensionality, lead to sparsity, and make Euclidean distance on these OHE features less semantically meaningful for representing categorical similarity. K-Prototypes avoids these issues by design.\n",
    "    *   **Label Encoding:** If categorical features are already label encoded (e.g., 'Low'=0, 'Medium'=1, 'High'=2), this is generally acceptable as long as these are passed as categorical features. The algorithm will treat these numbers as distinct categories, not as ordinal values with inherent numeric distances, for the `d_categorical` part.\n",
    "\n",
    "3.  **Handling Missing Values:**\n",
    "    K-Prototypes, like K-Means and K-Modes, cannot directly handle missing values (NaNs). They must be imputed or rows containing them removed before fitting the model.\n",
    "    *   **Numerical Features:**\n",
    "        *   **Mean/Median Imputation:** Replace missing values with the mean or median of the respective column. Median is often preferred if the feature has outliers.\n",
    "        *   **More Sophisticated Methods:** Algorithms like K-Nearest Neighbors (KNN) imputer or model-based imputation (e.g., using regression) can provide more accurate imputations but add complexity.\n",
    "    *   **Categorical Features:**\n",
    "        *   **Mode Imputation:** Replace missing values with the mode (most frequent category) of the respective column.\n",
    "        *   **Treat as a Distinct Category:** If \"missing\" itself carries information, missing values can be replaced with a new category like \"Unknown\" or \"Missing.\"\n",
    "    *   **Important Note:** Imputation should ideally be done using statistics derived only from the training set if you are in a predictive modeling pipeline context, to avoid data leakage. For purely exploratory clustering, imputing based on the full dataset is common.\n",
    "\n",
    "4.  **Identifying Categorical Columns:**\n",
    "    This is a crucial step for the `kmodes` library implementation. The `KPrototypes` constructor or its `fit`/`fit_predict` method requires the `categorical` parameter. This parameter must be a list or array-like object containing the **integer indices** of the columns in your input data array that are to be treated as categorical. Forgetting this, or providing incorrect indices, will lead to the algorithm misinterpreting feature types and producing incorrect or meaningless results. For instance, if a numerical column is mistakenly listed as categorical, its values will be treated as distinct categories. If a categorical column (especially if numerically encoded) is not listed, it will be treated as numerical, applying Euclidean distance, which is usually inappropriate.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison with Other Clustering Algorithms\n",
    "\n",
    "K-Prototypes fills a specific niche, but it's useful to understand how it compares to other common approaches for clustering, especially when mixed data is involved.\n",
    "\n",
    "1.  **K-Means / K-Modes:**\n",
    "    *   **K-Means:** Designed for numerical data only. It minimizes the sum of squared Euclidean distances to cluster centroids (means). If applied to mixed data, categorical features must be converted to numerical (e.g., via one-hot encoding), which has drawbacks (see below).\n",
    "    *   **K-Modes:** Designed for categorical data only. It minimizes the sum of simple matching dissimilarities to cluster modes. If applied to mixed data, numerical features must be discretized (losing information) or ignored.\n",
    "    *   **K-Prototypes:** K-Prototypes is the direct solution when both numerical and categorical data are present and considered important. It elegantly combines K-Means's approach for numerical parts and K-Modes's approach for categorical parts, using a weighted dissimilarity measure. It is superior to using K-Means or K-Modes alone on mixed data as it avoids forceful data type conversion or information loss.\n",
    "\n",
    "2.  **One-Hot Encoding (OHE) + K-Means:**\n",
    "    This is a common workaround to use K-Means on mixed datasets. Categorical features are transformed into multiple binary (0/1) features via OHE, and then K-Means is applied to the entirely numerical dataset (original numerical features + new OHE features).\n",
    "    *   **Pros:**\n",
    "        *   Allows use of the well-understood and widely available K-Means algorithm.\n",
    "        *   Can sometimes yield reasonable results, especially if categorical features don't have too many unique values.\n",
    "    *   **Cons:**\n",
    "        *   **Increases Dimensionality:** OHE can significantly increase the number of features, especially for categorical variables with high cardinality. This can lead to the \"curse of dimensionality,\" making distance calculations less meaningful and clustering harder.\n",
    "        *   **Sparsity:** The resulting dataset becomes sparse, which can be problematic for K-Means.\n",
    "        *   **Euclidean Distance on OHE Features:** Euclidean distance applied to OHE features might not be semantically optimal. For a multi-category feature, a mismatch results in a distance of `sqrt(2)`, and it treats all OHE binary features independently and equally, which may not reflect the true dissimilarity between categories.\n",
    "        *   **Equal Weighting of Binary Features:** Each new binary feature from OHE contributes equally to the distance, which might overemphasize categorical features with many categories.\n",
    "    *   **K-Prototypes Advantage:** K-Prototypes aims for a more direct and balanced handling by using simple matching for categorical features and Euclidean for numerical, with the `γ` parameter to explicitly control the balance, avoiding the pitfalls of OHE.\n",
    "\n",
    "3.  **Gower's Distance + Hierarchical Clustering (or PAM):**\n",
    "    *   **Gower's Distance:** This is a dissimilarity measure specifically designed for mixed data types. It computes a dissimilarity value between 0 and 1 for each variable (scaled range for numerical, Jaccard for binary, Dice for asymmetric binary, simple matching for nominal), then takes a weighted average.\n",
    "    *   **Usage:** Gower's distance matrix can be fed into algorithms like Hierarchical Clustering (e.g., Agglomerative Clustering) or Partitioning Around Medoids (PAM, also known as K-Medoids).\n",
    "    *   **Pros:**\n",
    "        *   Provides a theoretically sound way to measure dissimilarity in mixed-type data.\n",
    "        *   Can be more flexible in capturing different types of relationships.\n",
    "        *   Hierarchical clustering doesn't require pre-specifying `K` (though a cut-off is needed) and provides a dendrogram.\n",
    "    *   **Cons:**\n",
    "        *   **Scalability:** Calculating a full N x N distance matrix for Gower's distance is computationally expensive (`O(N^2 * M)`). Hierarchical clustering is often `O(N^2 log N)` or `O(N^3)`. This makes it less suitable for large datasets compared to K-Prototypes (`O(N*K*M*I)`).\n",
    "        *   **PAM Scalability:** PAM is also more computationally intensive than K-Means/K-Prototypes, often `O(K*N^2)`.\n",
    "    *   **K-Prototypes Advantage:** K-Prototypes is generally much more scalable for larger datasets due to its iterative, K-Means-like nature. While Gower's distance is powerful, its computational cost can be prohibitive.\n",
    "\n",
    "In summary, K-Prototypes offers a pragmatic and efficient approach for mixed-type data, especially when scalability and interpretability of prototypes are key concerns, striking a balance between the simplicity of K-Means/K-Modes and the complexity of methods like Gower's distance with hierarchical clustering.\n",
    "\n",
    "---\n",
    "\n",
    "### Use-Case Scenarios\n",
    "\n",
    "K-Prototypes is applicable in a wide array of domains where datasets naturally consist of both quantitative measurements and qualitative descriptors. Its ability to find patterns across these diverse data types makes it valuable for:\n",
    "\n",
    "1.  **Customer Segmentation:**\n",
    "    *   **Data:** Demographics (categorical: gender, region, education level; numerical: age, income, loyalty points), purchasing behavior (numerical: total spending, frequency of purchase; categorical: preferred product categories, subscription type).\n",
    "    *   **Goal:** Group customers into distinct segments for targeted marketing campaigns, personalized offers, or product recommendations. For example, identifying a segment of \"high-income, middle-aged males, preferring luxury tech gadgets.\"\n",
    "2.  **Healthcare and Patient Profiling:**\n",
    "    *   **Data:** Patient records with medical history (categorical: pre-existing conditions, prescribed medications, smoking status; numerical: lab test results like blood pressure, cholesterol levels, age, BMI).\n",
    "    *   **Goal:** Cluster patients to identify groups with similar health profiles for risk stratification, personalized treatment plans, or understanding disease subtypes. For instance, finding a cluster of \"older patients with diabetes and high blood pressure but controlled cholesterol.\"\n",
    "3.  **Anomaly Detection / Fraud Detection:**\n",
    "    *   **Data:** Transaction records (numerical: transaction amount, frequency; categorical: transaction type, merchant category, location), user activity logs.\n",
    "    *   **Goal:** Identify data points that are significantly dissimilar to all cluster prototypes. These outliers might represent fraudulent activities, system errors, or novel behaviors. Points that don't fit well into any cluster (high dissimilarity to their assigned prototype) are candidates for anomalies.\n",
    "4.  **Survey Analysis:**\n",
    "    *   **Data:** Responses to surveys containing mixed-type questions (e.g., Likert scales often treated as numerical or ordinal, demographic questions as categorical, open-ended questions processed into categorical themes).\n",
    "    *   **Goal:** Segment respondents based on their answers to understand different opinion groups, attitudes, or preferences. For example, clustering survey respondents based on their political views (categorical) and satisfaction scores (numerical).\n",
    "5.  **Manufacturing and Quality Control:**\n",
    "    *   **Data:** Product or process data including sensor readings (numerical: temperature, pressure, dimensions), quality control flags (categorical: pass/fail, defect type), machine settings (categorical: operator ID, machine type).\n",
    "    *   **Goal:** Profile products or manufacturing batches to identify common characteristics of high-quality vs. low-quality outputs, or to find operational regimes that lead to specific outcomes. For instance, clustering production runs to find \"runs with high temperature and specific material type leading to higher defect rates.\"\n",
    "6.  **E-commerce Product Categorization or Recommendation:**\n",
    "    *   **Data:** Product attributes (numerical: price, weight, user ratings; categorical: brand, color, material, category tags).\n",
    "    *   **Goal:** Group similar products together for better inventory management, navigation, or to power \"similar items\" recommendations by finding products that fall into the same cluster based on their mixed features.\n",
    "7.  **Human Resources Analytics:**\n",
    "    *   **Data:** Employee information (numerical: salary, tenure, performance scores; categorical: department, job role, education level).\n",
    "    *   **Goal:** Identify employee segments for targeted development programs, understanding attrition risks, or optimizing team composition.\n",
    "\n",
    "In each of these scenarios, K-Prototypes allows for the discovery of meaningful groups by simultaneously considering all available feature types, leading to more holistic and actionable insights than if only one type of data was analyzed.\n",
    "\n",
    "---\n",
    "\n",
    "### Choosing K and Gamma (γ)\n",
    "\n",
    "Selecting appropriate values for the number of clusters (`K`) and the weighting parameter (`γ`) is crucial for obtaining meaningful results from K-Prototypes. These are typically the most challenging hyperparameters to tune.\n",
    "\n",
    "1.  **Choosing K (Number of Clusters):**\n",
    "    Determining the optimal `K` is a common challenge in partitioning clustering. There's no single definitive method, and often a combination of techniques and domain knowledge is used.\n",
    "    *   **Elbow Method (Cost Function Plot):**\n",
    "        *   **Procedure:** Run K-Prototypes for a range of `K` values (e.g., from 2 to 10 or more). For each `K`, record the final cost (total within-cluster sum of dissimilarities, `kproto.cost_`). Plot `K` against the cost.\n",
    "        *   **Interpretation:** Look for an \"elbow\" point in the plot – a point where increasing `K` further results in a much smaller decrease in the cost. This point suggests a `K` beyond which adding more clusters yields diminishing returns in terms of explaining the data's variance. The `kmodes` library implementation makes this straightforward.\n",
    "        *   **Caveat:** The elbow can sometimes be ambiguous or non-existent.\n",
    "    *   **Silhouette Analysis (Adapted):**\n",
    "        *   **Procedure:** The silhouette score measures how similar a point is to its own cluster compared to other clusters. It ranges from -1 to 1. Higher values indicate better-defined clusters. To use it with K-Prototypes, the silhouette score calculation must use the same mixed dissimilarity measure `d(X, P)` that K-Prototypes uses.\n",
    "        *   **Interpretation:** Calculate the average silhouette score for different values of `K`. The `K` that maximizes the average silhouette score is often considered optimal.\n",
    "        *   **Practicality:** Implementing a custom silhouette score for K-Prototypes' specific distance metric can be involved if not directly supported by standard libraries for this mixed metric.\n",
    "    *   **Business Interpretability and Stability:**\n",
    "        *   **Interpretability:** Choose a `K` that results in clusters that are meaningful and actionable from a business or domain perspective. If the prototypes are hard to distinguish or don't represent intuitive groupings, the `K` might be too high or too low.\n",
    "        *   **Stability:** If you run the clustering multiple times (e.g., with different subsets of data or `n_init`), check if the cluster assignments are stable for a given `K`. Highly unstable clusters might indicate an inappropriate `K`.\n",
    "    *   **Gap Statistic:** A more statistically formal method, but can be computationally intensive. It compares the within-cluster dispersion of the data to that of random reference datasets.\n",
    "\n",
    "2.  **Choosing Gamma (γ):**\n",
    "    The `γ` parameter balances the influence of numerical and categorical features. Its optimal value is highly data-dependent.\n",
    "    *   **Automatic Estimation (Library Default):** The `kmodes` library can automatically estimate `γ` if `gamma=None` is passed to `KPrototypes`. This is often based on a heuristic like `0.5 * mean_standard_deviation_of_numerical_features`. This can be a good starting point. The actual estimated gamma can be accessed via `kproto.gamma_` after fitting.\n",
    "    *   **Heuristic Based on Data Characteristics:** As proposed by Huang, `γ` could be the average standard deviation (or variance) of the numerical features, or a fraction of it (e.g., between `1/3` and `2/3` of the average standard deviation). The idea is to scale categorical mismatches to be roughly comparable to typical numerical distances.\n",
    "    *   **Experimentation and Iteration:**\n",
    "        *   Try a range of `γ` values (e.g., 0.1, 0.5, 1.0, the auto-estimated value, values proportional to numerical feature variance like 2.0, 5.0, 10.0).\n",
    "        *   For each `γ`, evaluate the resulting clusters. This evaluation is often qualitative:\n",
    "            *   **Interpretability:** Do the prototypes make sense? Do they represent distinct, understandable groups?\n",
    "            *   **Balance:** Observe the prototypes. If numerical features are nearly identical across prototypes while categorical features vary widely, `γ` might be too high. If categorical features are nearly identical while numerical features vary, `γ` might be too low.\n",
    "            *   **Cluster Stability:** How much do cluster assignments or prototype characteristics change with slight variations in `γ`? Very sensitive results might indicate a poorly chosen `γ`.\n",
    "            *   **Domain Knowledge:** If domain experts believe categorical features are more (or less) important than numerical ones for defining meaningful groups, adjust `γ` upwards (or downwards) accordingly.\n",
    "    *   **Grid Search with Evaluation Metric:** If an appropriate evaluation metric (like an adapted silhouette score or a domain-specific metric) is available, you can perform a grid search over a range of `K` and `γ` values, choosing the combination that optimizes the metric. This can be computationally intensive.\n",
    "    *   **Objective:** The goal is to find a `γ` that allows both numerical and categorical features to contribute meaningfully to the clustering process, reflecting their relative importance in the context of the specific dataset and analytical problem. If categorical features have few unique values but are highly discriminative, a higher `γ` might be needed to give them enough weight against continuous numerical features that might have larger inherent variance.\n",
    "\n",
    "Ultimately, choosing `K` and `γ` often involves an iterative process of experimentation, evaluation of cluster quality (both quantitatively if possible, and qualitatively based on interpretability), and leveraging domain expertise. There's rarely a single \"correct\" answer, but rather a range of parameters that might yield useful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d85f2-52fc-45ea-826c-6626853c5f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
