{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ca1634-32bb-41a8-b3cc-dc44bc5f0e62",
   "metadata": {},
   "source": [
    "measures the model's ability to discriminate between positive and negative classes across all possible classification thresholds. An AUC of 1 represents a perfect classifier, while an AUC of 0.5 suggests a model no better than random guessing. It's particularly useful for imbalanced datasets as it is threshold-independent.\n",
    "6.  **LogLoss (Logarithmic Loss or Cross-Entropy Loss)**: This metric quantifies the performance of a classification model whose output is a probability value between 0 and 1. LogLoss penalizes confident and wrong predictions more heavily than less confident but wrong predictions. Lower LogLoss values indicate better model performance. For a binary classification problem, the LogLoss for a single instance is `- (y log(p) + (1-y) log(1-p))`, where `y` is the true label (0 or 1) and `p` is the predicted probability of the instance belonging to class 1. This is often the default evaluation metric minimized by XGBoost during training for classification tasks.\n",
    "\n",
    "#### 7.2 Regression Metrics\n",
    "\n",
    "For regression tasks, where the goal is to predict a continuous numerical value, common evaluation metrics include:\n",
    "1.  **Mean Squared Error (MSE)**: `(1/n) * Σ(yᵢ - ŷᵢ)²`. It measures the average of the squares of the errors between the actual (`yᵢ`) and predicted (`ŷᵢ`) values. MSE penalizes larger errors more heavily due to the squaring. The units are the square of the target variable's units, which can sometimes make interpretation difficult.\n",
    "2.  **Root Mean Squared Error (RMSE)**: `sqrt(MSE)`. This is the square root of the MSE. It is more interpretable than MSE as its units are the same as the target variable. Like MSE, it penalizes large errors significantly. Lower RMSE values indicate better model fit. This is a very common metric for regression problems and often used as an evaluation metric in XGBoost training.\n",
    "3.  **Mean Absolute Error (MAE)**: `(1/n) * Σ|yᵢ - ŷᵢ|`. It measures the average of the absolute differences between actual and predicted values. MAE is less sensitive to outliers compared to MSE or RMSE because it does not square the errors. Its units are the same as the target variable, making it easily interpretable as the average absolute deviation.\n",
    "4.  **R-squared (R² or Coefficient of Determination)**: `1 - (SS_res / SS_tot)`, where `SS_res` is the sum of squares of residuals (`Σ(yᵢ - ŷᵢ)²`) and `SS_tot` is the total sum of squares (`Σ(yᵢ - ȳ)²`, where `ȳ` is the mean of the actual values). R² represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1 (though it can be negative for very poor models). An R² of 1 indicates that the model perfectly predicts the target variable, while an R² of 0 indicates that the model performs no better than predicting the mean of the target. Higher R² values are generally better.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Practical Implementation: Classification (Titanic Survival Prediction)\n",
    "\n",
    "Let's demonstrate XGBoost for a classic binary classification problem: predicting survival on the Titanic. We'll use `pandas` for data manipulation, `scikit-learn` for preprocessing and evaluation, and `xgboost` for the model.\n",
    "\n",
    "#### 8.1 Data Loading and Preprocessing\n",
    "\n",
    "First, we load the Titanic dataset (often available through seaborn or Kaggle) and perform necessary preprocessing steps. This includes handling missing values, encoding categorical features, and splitting the data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset (assuming you have titanic.csv)\n",
    "# For demonstration, let's create a simplified DataFrame structure\n",
    "# or load from seaborn if available\n",
    "try:\n",
    "    df = sns.load_dataset('titanic')\n",
    "except:\n",
    "    print(\"Seaborn titanic dataset not found. Please download 'titanic.csv' or use another dataset.\")\n",
    "    # Create a dummy dataframe for code structure to run\n",
    "    data = {'Survived': [0, 1, 1, 0, 0, 1],\n",
    "            'Pclass': [3, 1, 3, 1, 3, 2],\n",
    "            'Sex': ['male', 'female', 'female', 'female', 'male', 'male'],\n",
    "            'Age': [22, 38, 26, 35, 35, np.nan],\n",
    "            'SibSp': [1, 1, 0, 1, 0, 0],\n",
    "            'Parch': [0, 0, 0, 0, 0, 1],\n",
    "            'Fare': [7.25, 71.28, 7.92, 53.1, 8.05, 10.5],\n",
    "            'Embarked': ['S', 'C', 'S', 'C', 'S', 'Q']}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame head:\\n\", df.head())\n",
    "\n",
    "# Preprocessing\n",
    "# 1. Handle Missing Values\n",
    "# For 'Age', fill with median\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "# For 'Embarked', fill with mode (most frequent)\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "# 'deck' and 'embark_town' often have many NaNs, let's drop 'deck' if it exists\n",
    "if 'deck' in df.columns:\n",
    "    df.drop('deck', axis=1, inplace=True)\n",
    "\n",
    "# 2. Encode Categorical Features\n",
    "# 'Sex' and 'Embarked'\n",
    "le_sex = LabelEncoder()\n",
    "df['Sex'] = le_sex.fit_transform(df['Sex']) # male:1, female:0 (check classes_ if needed)\n",
    "\n",
    "# One-hot encode 'Embarked' and 'Pclass' (treating Pclass as categorical)\n",
    "df = pd.get_dummies(df, columns=['Embarked', 'Pclass'], drop_first=True)\n",
    "\n",
    "# 3. Select Features and Target\n",
    "# Dropping less relevant or redundant columns if any (e.g., 'who', 'adult_male', 'alive', 'class', 'embark_town' if loaded from seaborn)\n",
    "# For this example, ensure 'Survived' is the target and other columns are features\n",
    "columns_to_drop = ['who', 'adult_male', 'alive', 'class', 'embark_town', 'passenger_id', 'name', 'ticket'] # Example, adjust as per actual dataset\n",
    "for col in columns_to_drop:\n",
    "    if col in df.columns:\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "if 'Survived' not in df.columns:\n",
    "    raise ValueError(\"'Survived' column not found. Please ensure it's in your DataFrame.\")\n",
    "\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "print(\"\\nProcessed DataFrame head:\\n\", X.head())\n",
    "print(\"\\nTarget variable head:\\n\", y.head())\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining set shape: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Testing set shape: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "```\n",
    "**Explanation:**\n",
    "1.  **Load Data**: We load the Titanic dataset. If using `sns.load_dataset('titanic')`, it comes with several columns.\n",
    "2.  **Handle Missing Values**: `Age` is filled with the median, a robust measure for skewed distributions. `Embarked` is filled with the mode. The `deck` column, which typically has many missing values, is dropped.\n",
    "3.  **Encode Categorical Features**: `Sex` is label encoded. `Embarked` and `Pclass` (even though numerical, it's categorical in nature) are one-hot encoded using `pd.get_dummies` to create binary columns for each category, `drop_first=True` avoids multicollinearity.\n",
    "4.  **Feature Selection**: Unnecessary columns like 'name', 'ticket', 'passenger_id' are dropped. 'Survived' is separated as the target variable `y`, and the rest become features `X`.\n",
    "5.  **Split Data**: The data is split into 80% training and 20% testing sets using `train_test_split`. `stratify=y` ensures that the proportion of the target variable is similar in both train and test sets, which is important for classification, especially with imbalanced classes.\n",
    "\n",
    "#### 8.2 Model Training and Evaluation\n",
    "\n",
    "Now, we train an XGBoost classifier and evaluate its performance. We'll use `early_stopping_rounds` for efficiency.\n",
    "\n",
    "```python\n",
    "# Initialize XGBoost Classifier\n",
    "# Common starting parameters\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    objective='binary:logistic', # for binary classification\n",
    "    n_estimators=1000,           # High number, will be cut by early stopping\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.01,               # L1 regularization\n",
    "    reg_lambda=0.1,               # L2 regularization\n",
    "    use_label_encoder=False,     # Suppress a warning, as we've already encoded\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'        # Evaluation metric for early stopping\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "# eval_set requires a list of (X, y) tuples\n",
    "eval_set = [(X_test, y_test)] # Using test set for early stopping monitoring\n",
    "# For a more robust approach, a separate validation set should be carved out from X_train\n",
    "\n",
    "xgb_classifier.fit(X_train, y_train,\n",
    "                   early_stopping_rounds=50, # Stop if no improvement after 50 rounds\n",
    "                   eval_set=eval_set,\n",
    "                   verbose=False) # Set to True or a number to see training progress\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_proba = xgb_classifier.predict_proba(X_test)[:, 1] # Probabilities for ROC-AUC\n",
    "y_pred_class = xgb_classifier.predict(X_test)           # Class predictions\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"\\n--- Model Evaluation ---\")\n",
    "print(f\"Best N Estimators (due to early stopping): {xgb_classifier.best_iteration}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_class))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class))\n",
    "\n",
    "# Optional: Hyperparameter Tuning with GridSearchCV (can be time-consuming)\n",
    "# param_grid = {\n",
    "#     'max_depth': [3, 4, 5],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'n_estimators': [100, 200, 300], # Reduced for faster grid search\n",
    "#     'subsample': [0.7, 0.8],\n",
    "#     'colsample_bytree': [0.7, 0.8]\n",
    "# }\n",
    "# grid_search = GridSearchCV(estimator=xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, random_state=42, eval_metric='logloss'),\n",
    "#                            param_grid=param_grid,\n",
    "#                            scoring='roc_auc',\n",
    "#                            cv=3, # 3-fold cross-validation\n",
    "#                            verbose=1,\n",
    "#                            n_jobs=-1) # Use all available cores\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# print(\"\\nBest parameters found by GridSearchCV:\", grid_search.best_params_)\n",
    "# best_xgb_classifier = grid_search.best_estimator_\n",
    "# y_pred_proba_tuned = best_xgb_classifier.predict_proba(X_test)[:, 1]\n",
    "# y_pred_class_tuned = best_xgb_classifier.predict(X_test)\n",
    "# print(f\"ROC-AUC Score (Tuned Model): {roc_auc_score(y_test, y_pred_proba_tuned):.4f}\")\n",
    "```\n",
    "**Explanation:**\n",
    "1.  **Initialize `XGBClassifier`**: We set up the classifier with `objective='binary:logistic'` for binary classification. `eval_metric='logloss'` or `'auc'` can be used for monitoring. A high `n_estimators` is set, expecting early stopping to find the optimal number.\n",
    "2.  **Train Model**: The `fit` method trains the model. `eval_set=[(X_test, y_test)]` provides data to monitor for `early_stopping_rounds`. `early_stopping_rounds=50` means training will stop if the `logloss` on `X_test` doesn't improve for 50 consecutive trees. Ideally, a separate validation set (split from `X_train`) should be used for `eval_set` to prevent data leakage from the test set into the training process, and `X_test` should only be used for final evaluation.\n",
    "3.  **Make Predictions**: `predict_proba` gives class probabilities (needed for ROC-AUC), and `predict` gives direct class labels.\n",
    "4.  **Evaluate**: Accuracy, ROC-AUC, classification report (precision, recall, F1-score), and confusion matrix are calculated. The `xgb_classifier.best_iteration` shows how many trees were actually used thanks to early stopping.\n",
    "5.  **Hyperparameter Tuning (Optional)**: A `GridSearchCV` example is commented out. It systematically searches for the best hyperparameter combination using cross-validation. This is computationally intensive but can lead to better performance.\n",
    "\n",
    "#### 8.3 Feature Importance\n",
    "\n",
    "XGBoost can provide insights into which features are most influential in making predictions.\n",
    "\n",
    "```python\n",
    "# Plot Feature Importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "xgb.plot_importance(xgb_classifier, ax=ax, importance_type='gain') # 'weight', 'gain', 'cover'\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.show()\n",
    "\n",
    "# Get feature importance scores as a dictionary\n",
    "importance_scores_gain = xgb_classifier.get_booster().get_score(importance_type='gain')\n",
    "print(\"\\nFeature Importance (Gain):\\n\", sorted(importance_scores_gain.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "importance_scores_weight = xgb_classifier.get_booster().get_score(importance_type='weight')\n",
    "print(\"\\nFeature Importance (Weight - number of times a feature appears in a tree):\\n\", sorted(importance_scores_weight.items(), key=lambda item: item[1], reverse=True))\n",
    "```\n",
    "*(Visual Aid: Feature Importance Plot)*\n",
    "The `xgb.plot_importance` function generates a horizontal bar chart showing features ranked by their importance.\n",
    "*   **`importance_type='weight'`**: The number of times a feature appears in a tree (i.e., used for a split).\n",
    "*   **`importance_type='gain'`**: The average gain (improvement in accuracy or reduction in loss) brought by splits on this feature across all trees. This is generally a more robust measure.\n",
    "*   **`importance_type='cover'`**: The average coverage (number of samples affected) of splits which use the feature.\n",
    "\n",
    "**Explanation:**\n",
    "The plot and printed scores help identify which features (e.g., `Sex`, `Fare`, `Age`) contribute most to the model's predictions. This can be valuable for understanding the data and potentially for feature selection in future model iterations. 'Gain' usually provides a more meaningful measure of importance than 'weight'.\n",
    "\n",
    "#### 8.4 Learning Curves\n",
    "\n",
    "Learning curves can help diagnose if the model is overfitting, underfitting, or if more data/training would be beneficial. XGBoost allows retrieval of evaluation results logged during training if `eval_set` was used.\n",
    "\n",
    "```python\n",
    "# Retrieve evaluation results\n",
    "results = xgb_classifier.evals_result()\n",
    "epochs = len(results['validation_0']['logloss']) # 'validation_0' corresponds to the first item in eval_set\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_axis, results['validation_0']['logloss'], label='Validation LogLoss')\n",
    "# If you also had a training set in eval_set, e.g., eval_set=[(X_train, y_train), (X_test, y_test)]\n",
    "# you could plot it too:\n",
    "# plt.plot(x_axis, results['learn']['logloss'], label='Train LogLoss') # 'learn' might be 'training' or 'validation_0' if X_train was first\n",
    "\n",
    "plt.axvline(xgb_classifier.best_iteration, color='r', linestyle='--', label=f'Best Iteration ({xgb_classifier.best_iteration})')\n",
    "plt.xlabel('Number of Boosting Rounds (Trees)')\n",
    "plt.ylabel('LogLoss')\n",
    "plt.title('XGBoost Learning Curve (LogLoss)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "*(Visual Aid: Learning Curve Plot)*\n",
    "This plot shows the `logloss` (or other `eval_metric`) on the validation set as the number of trees increases. You'd typically see the loss decrease and then plateau or start to increase if overfitting occurs. The vertical dashed line indicates where early stopping halted training. If a training loss curve were also plotted, a large gap between training and validation loss would indicate overfitting.\n",
    "\n",
    "**Explanation:**\n",
    "The learning curve visualizes the model's performance metric (here, `logloss` on the validation set) at each boosting round. The point where early stopping intervened (`xgb_classifier.best_iteration`) is marked. This helps confirm that early stopping worked as intended and gives a visual sense of the training dynamics. If training loss was also monitored and plotted, it would typically continue to decrease, while validation loss might start to increase, indicating overfitting beyond the `best_iteration`.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Practical Implementation: Regression (Housing Price Prediction)\n",
    "\n",
    "Let's use XGBoost for a regression task, such as predicting housing prices. We'll use a common dataset like the Boston Housing dataset (though it has ethical concerns and is deprecated in newer scikit-learn; California Housing is a good alternative) or a generic housing price dataset. For this example, let's assume a `housing.csv` file or use California Housing.\n",
    "\n",
    "#### 9.1 Data Loading and Preprocessing\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X_reg, y_reg = housing.data, housing.target\n",
    "\n",
    "print(\"California Housing Features head:\\n\", X_reg.head())\n",
    "print(\"\\nCalifornia Housing Target (MedHouseVal) head:\\n\", y_reg.head())\n",
    "\n",
    "# No significant categorical features in California Housing to encode, mostly numerical.\n",
    "# Check for missing values (California housing usually doesn't have them)\n",
    "print(\"\\nMissing values in features:\\n\", X_reg.isnull().sum())\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nRegression Training set shape: X_train_reg: {X_train_reg.shape}, y_train_reg: {y_train_reg.shape}\")\n",
    "print(f\"Regression Testing set shape: X_test_reg: {X_test_reg.shape}, y_test_reg: {y_test_reg.shape}\")\n",
    "```\n",
    "**Explanation:**\n",
    "1.  **Load Data**: We use `fetch_california_housing` from `sklearn.datasets`. This dataset contains features related to housing districts in California and the target is the median house value.\n",
    "2.  **Missing Values**: The California Housing dataset is typically clean and doesn't have missing values. If using another dataset, imputation (e.g., median for numerical features) would be needed if XGBoost's internal handling isn't preferred for certain columns.\n",
    "3.  **Feature Engineering/Encoding**: This dataset is mostly numerical. If categorical features were present, they'd need encoding (e.g., one-hot or label encoding).\n",
    "4.  **Split Data**: The data is split into training and testing sets.\n",
    "\n",
    "#### 9.2 Model Training and Evaluation\n",
    "\n",
    "Training an XGBoost regressor and evaluating its performance.\n",
    "\n",
    "```python\n",
    "# Initialize XGBoost Regressor\n",
    "xgb_regressor = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror', # for regression, predicts the squared error\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    gamma=0,                      # Less aggressive pruning initially for regression\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    eval_metric='rmse'            # Root Mean Squared Error for evaluation\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "eval_set_reg = [(X_test_reg, y_test_reg)]\n",
    "xgb_regressor.fit(X_train_reg, y_train_reg,\n",
    "                  early_stopping_rounds=50,\n",
    "                  eval_set=eval_set_reg,\n",
    "                  verbose=False)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = xgb_regressor.predict(X_test_reg)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"\\n--- Regression Model Evaluation ---\")\n",
    "print(f\"Best N Estimators (due to early stopping): {xgb_regressor.best_iteration}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_reg, y_pred_reg, alpha=0.3, edgecolors='w', linewidth=0.5)\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Median House Value\")\n",
    "plt.ylabel(\"Predicted Median House Value\")\n",
    "plt.title(\"Actual vs. Predicted Housing Prices (XGBoost Regressor)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "*(Visual Aid: Actual vs. Predicted Plot)*\n",
    "A scatter plot with actual values on the x-axis and predicted values on the y-axis. A diagonal line (y=x) represents perfect predictions. Points clustered closely around this line indicate good model performance.\n",
    "\n",
    "**Explanation:**\n",
    "1.  **Initialize `XGBRegressor`**: `objective='reg:squarederror'` is standard for regression tasks aiming to minimize MSE. `eval_metric='rmse'` is used for monitoring.\n",
    "2.  **Train Model**: Similar to classification, `fit` is called with `early_stopping_rounds` and an `eval_set`.\n",
    "3.  **Make Predictions**: `predict` method returns the continuous predicted values.\n",
    "4.  **Evaluate**: RMSE, MAE, and R-squared are calculated to assess regression performance.\n",
    "5.  **Visualization**: A scatter plot of actual vs. predicted values provides a visual assessment of the model's accuracy. Ideally, points should lie close to the diagonal line.\n",
    "\n",
    "#### 9.3 Feature Importance (Regression)\n",
    "\n",
    "Feature importance can also be plotted for regression models.\n",
    "\n",
    "```python\n",
    "# Plot Feature Importance for Regressor\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "xgb.plot_importance(xgb_regressor, ax=ax, importance_type='gain')\n",
    "plt.title('Feature Importance (Gain) - Housing Price Prediction')\n",
    "plt.show()\n",
    "\n",
    "# Get feature importance scores\n",
    "importance_scores_reg = xgb_regressor.get_booster().get_score(importance_type='gain')\n",
    "print(\"\\nFeature Importance (Gain) - Regression:\\n\", sorted(importance_scores_reg.items(), key=lambda item: item[1], reverse=True))\n",
    "```\n",
    "*(Visual Aid: Feature Importance Plot for Regression)*\n",
    "Similar to the classification example, this plot will show which features (e.g., `MedInc` - median income, `AveRooms` - average rooms) are most influential in predicting house prices.\n",
    "\n",
    "**Explanation:**\n",
    "The `plot_importance` function visualizes the contribution of each feature to the regression model. This helps in understanding the key drivers of housing prices as identified by the XGBoost model. For instance, median income (`MedInc`) is often a very strong predictor.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Model Interpretability with SHAP Values\n",
    "\n",
    "While feature importance gives a global overview, SHAP (SHapley Additive exPlanations) values provide more detailed, instance-level explanations of model predictions. SHAP values quantify the contribution of each feature to the prediction for a specific instance, explaining *why* a particular prediction was made. This is based on Shapley values from cooperative game theory, ensuring fair distribution of the \"payout\" (prediction) among features.\n",
    "\n",
    "```python\n",
    "import shap\n",
    "\n",
    "# Explain the model's predictions using SHAP\n",
    "# For classification (using the Titanic model)\n",
    "# Create an explainer object\n",
    "explainer_class = shap.TreeExplainer(xgb_classifier) # For tree-based models\n",
    "# Calculate SHAP values for the test set (can be computationally intensive for large datasets)\n",
    "# Using a subset for demonstration if X_test is large\n",
    "shap_values_class = explainer_class.shap_values(X_test) # For binary classification, this gives values for the positive class\n",
    "\n",
    "print(f\"\\nSHAP values shape for classification: {shap_values_class.shape}\") # Should be (n_samples, n_features)\n",
    "\n",
    "# Visualize the first prediction's explanation\n",
    "# (requires JS in Jupyter notebook/lab, or save as HTML)\n",
    "# shap.initjs() # Initialize JavaScript visualization\n",
    "# shap.force_plot(explainer_class.expected_value, shap_values_class[0,:], X_test.iloc[0,:], matplotlib=False)\n",
    "# For a summary plot:\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values_class, X_test, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Summary Plot (Bar) - Titanic Classification\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values_class, X_test, show=False) # Dot plot\n",
    "plt.title(\"SHAP Summary Plot (Dot) - Titanic Classification\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# For regression (using the Housing Price model)\n",
    "explainer_reg = shap.TreeExplainer(xgb_regressor)\n",
    "shap_values_reg = explainer_reg.shap_values(X_test_reg)\n",
    "\n",
    "print(f\"\\nSHAP values shape for regression: {shap_values_reg.shape}\")\n",
    "\n",
    "# shap.initjs()\n",
    "# shap.force_plot(explainer_reg.expected_value, shap_values_reg[0,:], X_test_reg.iloc[0,:], matplotlib=False)\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values_reg, X_test_reg, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Summary Plot (Bar) - Housing Regression\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values_reg, X_test_reg, show=False) # Dot plot\n",
    "plt.title(\"SHAP Summary Plot (Dot) - Housing Regression\")\n",
    "plt.show()\n",
    "\n",
    "# Dependence plot for a specific feature (e.g., 'MedInc' for regression)\n",
    "if 'MedInc' in X_test_reg.columns:\n",
    "    plt.figure()\n",
    "    shap.dependence_plot(\"MedInc\", shap_values_reg, X_test_reg, interaction_index=None, show=False)\n",
    "    plt.title(\"SHAP Dependence Plot for MedInc\")\n",
    "    plt.show()\n",
    "```\n",
    "*(Visual Aids: SHAP Summary Plots, Force Plots, Dependence Plots)*\n",
    "*   **SHAP Summary Plot (Bar type)**: Shows the mean absolute SHAP value for each feature, indicating global feature importance.\n",
    "*   **SHAP Summary Plot (Dot type / Beeswarm plot)**: Shows the SHAP value for every feature for every sample. Each dot is a single prediction/feature. The color can represent the feature's original value (high/low). This reveals not just importance but also the direction and distribution of effects.\n",
    "*   **SHAP Force Plot (for individual predictions)**: Visualizes how features contribute to push the prediction away from the base value (average prediction). Red arrows increase the prediction, blue arrows decrease it. (Often rendered with JS).\n",
    "*   **SHAP Dependence Plot**: Shows how the model's output for a single feature changes as the feature's value changes. It can also show interaction effects by coloring points with another feature.\n",
    "\n",
    "**Explanation:**\n",
    "1.  **Install `shap`**: `pip install shap`.\n",
    "2.  **Create Explainer**: A `shap.TreeExplainer` is used for tree-based models like XGBoost.\n",
    "3.  **Calculate SHAP Values**: `explainer.shap_values(X_data)` computes SHAP values for each feature of each instance in `X_data`.\n",
    "4.  **Summary Plots**:\n",
    "    *   The `bar` plot provides a global ranking of features similar to traditional feature importance but based on the magnitude of SHAP values.\n",
    "    *   The `dot` (or beeswarm) plot is very informative: it shows for each feature, how higher/lower values of that feature impact the prediction (positive or negative SHAP value) and the distribution of these impacts.\n",
    "5.  **Force Plots (Individual Explanations)**: These are excellent for explaining individual predictions to stakeholders (e.g., why a particular customer was predicted to churn). (Code for individual force plot often better in Jupyter).\n",
    "6.  **Dependence Plots**: These plots show the relationship between a feature's value and its SHAP value (i.e., its impact on the prediction). They can also highlight interaction effects if `interaction_index` is specified. For example, a dependence plot for 'Age' might show how increasing age affects the prediction of survival, potentially colored by 'Sex' to see if the effect differs for males and females.\n",
    "\n",
    "SHAP values greatly enhance model transparency, which is crucial for building trust and understanding complex models like XGBoost.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Comparison with Other Boosting Algorithms\n",
    "\n",
    "#### 11.1 vs. Gradient Boosting Machine (GBM)\n",
    "\n",
    "Traditional Gradient Boosting Machines (GBM), like Scikit-learn's `GradientBoostingClassifier` or `GradientBoostingRegressor`, form the foundation upon which XGBoost was built. Both use gradient boosting principles: sequentially adding weak learners (trees) to correct the errors of previous ones. However, XGBoost introduces several key improvements:\n",
    "1.  **Regularization**: XGBoost includes L1 and L2 regularization in its objective function (`gamma`, `alpha`, `lambda`), which helps prevent overfitting more effectively than standard GBMs that might only offer tree-specific constraints like `max_depth`.\n",
    "2.  **Speed and Performance**: XGBoost is designed for efficiency. It employs parallel processing for tree construction (at the node level using a histogram-based algorithm or pre-sorted data), cache-aware access, and out-of-core computation. This makes it significantly faster than most traditional GBM implementations, especially on large datasets.\n",
    "3.  **Handling Missing Values**: XGBoost has a built-in routine to handle missing data by learning default directions for NaNs during training, whereas standard GBMs often require imputation.\n",
    "4.  **Sparsity Awareness**: XGBoost efficiently handles sparse data by only iterating over non-missing entries.\n",
    "5.  **Second-Order Taylor Expansion**: XGBoost uses both first and second-order derivatives (gradient and Hessian) to approximate the loss function, often leading to more accurate optimization and faster convergence. Standard GBMs typically only use the first-order gradient.\n",
    "6.  **Hardware Optimization**: Better utilization of CPU and memory.\n",
    "While scikit-learn's GBM is robust and useful, XGBoost generally offers superior performance, speed, and more features for customization and regularization. LightGBM and CatBoost are other modern gradient boosting libraries that also offer significant advantages over traditional GBMs and are competitive with XGBoost.\n",
    "\n",
    "#### 11.2 vs. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "AdaBoost is one of the earliest and most fundamental boosting algorithms. It differs from gradient boosting (and thus XGBoost) in how it iteratively improves the model:\n",
    "1.  **Weighting Instances**: AdaBoost adjusts the weights of training instances at each iteration. Misclassified instances from the previous iteration are given higher weights in the current iteration, forcing the new weak learner to focus more on these \"hard\" examples. Gradient boosting, in contrast, fits new learners to the residual errors of the previous ensemble.\n",
    "2.  **Weak Learner Contribution**: In AdaBoost, weak learners (often decision stumps) are weighted based on their individual accuracy. More accurate learners contribute more to the final prediction. In gradient boosting, learners are typically added with a fixed (or shrinking) learning rate.\n",
    "3.  **Loss Function**: AdaBoost typically minimizes an exponential loss function, which makes it sensitive to outliers. Gradient boosting is more flexible and can optimize various differentiable loss functions (squared error, logistic loss, etc.). XGBoost extends this by allowing custom loss functions and using second-order derivatives.\n",
    "4.  **Complexity**: AdaBoost is generally simpler to implement and understand than gradient boosting. XGBoost, being an advanced form of gradient boosting, is considerably more complex but also more powerful and flexible.\n",
    "In summary, XGBoost is a gradient boosting algorithm, while AdaBoost uses a different re-weighting scheme. XGBoost is generally more robust to outliers (due to flexible loss functions and regularization), more accurate, and more feature-rich than AdaBoost, especially on complex, large-scale datasets. AdaBoost can still be effective for simpler problems or as a baseline.\n",
    "\n",
    "#### 11.3 Advantages of XGBoost and When to Use It\n",
    "\n",
    "**Advantages of XGBoost:**\n",
    "1.  **High Predictive Accuracy**: Consistently ranks among the top-performing algorithms for structured/tabular data, often winning machine learning competitions.\n",
    "2.  **Speed and Scalability**: Optimized for computational efficiency through parallel processing, cache awareness, and out-of-core computation. Handles large datasets well.\n",
    "3.  **Regularization**: Built-in L1 and L2 regularization, plus `gamma` for tree pruning, helps prevent overfitting and improves generalization.\n",
    "4.  **Handling Missing Values**: Natively handles missing data by learning optimal default directions.\n",
    "5.  **Flexibility**: Supports custom objective functions and evaluation metrics. Can be used for classification, regression, ranking, and survival analysis.\n",
    "6.  **Tree Pruning**: Employs `max_depth` and `gamma` (min_split_loss) for effective pruning, controlling tree complexity.\n",
    "7.  **Cross-Validation Built-in**: Provides a `cv` function for efficient hyperparameter tuning.\n",
    "8.  **Feature Importance and Interpretability**: Offers insights into feature contributions and can be paired with tools like SHAP for deeper explanations.\n",
    "9.  **Sparsity Awareness**: Efficiently handles sparse feature matrices.\n",
    "\n",
    "**When to Use XGBoost:**\n",
    "*   **Structured/Tabular Data Problems**: XGBoost excels when dealing with datasets in table format (rows are observations, columns are features).\n",
    "*   **High-Performance Requirements**: When accuracy is paramount, such as in Kaggle competitions or critical business applications (fraud detection, churn prediction, risk assessment).\n",
    "*   **Large Datasets**: Its scalability makes it suitable for datasets that might be too large or slow for other algorithms.\n",
    "*   **Complex Relationships**: When data contains non-linear relationships and feature interactions that simpler models might miss.\n",
    "*   **Need for Robustness**: When you need a model that handles missing data well and is less prone to overfitting due to regularization.\n",
    "*   **Baseline for Complex Tasks**: Even if other models are considered (like deep learning for tabular data), XGBoost often serves as a very strong baseline or a component in an ensemble.\n",
    "\n",
    "However, XGBoost might be overkill for very small datasets or problems where linear models perform sufficiently well and offer better interpretability out-of-the-box. It also has more hyperparameters to tune than simpler models, which can require more effort in model selection.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Conclusion\n",
    "\n",
    "XGBoost has firmly established itself as a powerhouse in the realm of machine learning, particularly for tasks involving structured or tabular data. Its strength lies in the sophisticated amalgamation of gradient boosting principles with a suite of algorithmic and system-level optimizations. By incorporating features like advanced regularization (L1, L2, gamma), efficient handling of missing values, parallel and distributed processing capabilities, and the use of second-order gradient information, XGBoost delivers exceptional predictive accuracy and computational speed. Its wide array of tunable hyperparameters allows for fine-grained control over the model building process, enabling data scientists to tailor models precisely to their specific problem and dataset characteristics. Furthermore, its built-in support for cross-validation and early stopping streamlines the training workflow and helps in building robust, generalizable models. The ability to extract feature importances and integrate with interpretability tools like SHAP makes XGBoost not just a black box, but a model whose predictions can be understood and explained. While newer algorithms like LightGBM and CatBoost offer competitive performance and sometimes even faster training times, XGBoost remains a go-to algorithm due to its proven track record, extensive community support, and comprehensive feature set, making it an indispensable tool in any data scientist's arsenal for tackling complex classification and regression challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42b833-d3eb-4488-9a7f-10ebd149297a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
