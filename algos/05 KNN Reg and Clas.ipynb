{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4bee119-1d42-4758-9cf2-32a9eb976482",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Comprehensive Notes on K-Nearest Neighbors (KNN) Classification\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction to KNN Classification\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a foundational algorithm in machine learning, primarily used for classification and regression tasks. It belongs to the family of **non-parametric** learning algorithms, meaning it doesn't make strong assumptions about the underlying data distribution (like linear regression assuming a linear relationship). Instead, it learns the decision boundary directly from the training data. KNN is also classified as an **instance-based** or **lazy learning** algorithm. This \"laziness\" refers to the fact that it doesn't explicitly build a model during the training phase. The training phase simply involves storing all the training data points and their corresponding labels. The actual computation or \"learning\" happens during the prediction phase, when a new, unseen data point needs to be classified. To classify a new point, KNN looks at the 'K' closest data points (neighbors) from the training set in the feature space and assigns the class label that is most frequent among these K neighbors.\n",
    "\n",
    "Real-world applications of KNN are diverse and showcase its versatility. In **handwriting recognition**, KNN can classify scanned images of handwritten digits by comparing a new digit's pixel patterns to those of known digits in a database. For **recommendation systems**, KNN can suggest items (e.g., movies, products) to users based on the preferences of \"similar\" users (user-based collaborative filtering) or by finding items similar to those a user has liked (item-based collaborative filtering). In **anomaly detection**, KNN can identify unusual data points that are far from their nearest neighbors, suggesting they might be outliers or anomalies, which is crucial in fraud detection or network intrusion systems. It's also used in **image recognition**, **medical diagnosis** (e.g., predicting if a tumor is benign or malignant based on features of similar known cases), and **financial modeling**. The simplicity of its core concept and its ability to adapt to complex decision boundaries make it a valuable tool, especially when the underlying data structure is not well understood.\n",
    "\n",
    "```python\n",
    "# Illustrative: KNN is part of scikit-learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Imagine some training data (features and labels)\n",
    "# X_train = [[feature1_obj1, feature2_obj1], [feature1_obj2, feature2_obj2], ...]\n",
    "# y_train = [label_obj1, label_obj2, ...]\n",
    "\n",
    "# KNN doesn't \"train\" in the traditional sense, it just stores data.\n",
    "# The \"model\" is the data itself.\n",
    "# When a new point comes, it calculates distances to all stored points.\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `from sklearn.neighbors import KNeighborsClassifier`: Imports the KNN classifier class from the scikit-learn library.\n",
    "2.  `import numpy as np`: Imports the NumPy library, commonly used for numerical operations, though not directly used in this conceptual snippet.\n",
    "3.  `# X_train = ...`: This is a comment indicating where your training features (e.g., a 2D array or list of lists) would be defined. Each inner list/row represents a data point, and elements within are its features.\n",
    "4.  `# y_train = ...`: This comment indicates where your training labels (e.g., a list or 1D array) corresponding to `X_train` would be defined.\n",
    "5.  `# KNN doesn't \"train\" ...`: These comments explain the lazy learning nature of KNN, emphasizing that the training phase is primarily data storage.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Intuition Behind the Algorithm and Distance Metrics\n",
    "\n",
    "The core intuition of KNN is \"tell me who your neighbors are, and I'll tell you who you are.\" It operates on the principle that data points with similar features are likely to belong to the same class. When a new, unclassified data point arrives, KNN identifies the 'K' training data points that are \"closest\" to this new point in the feature space. The closeness is determined using a **distance metric**. The most common distance metrics include:\n",
    "\n",
    "1.  **Euclidean Distance:** This is the most common and intuitive distance metric, representing the straight-line distance between two points in an N-dimensional space. For two points `p = (p1, p2, ..., pn)` and `q = (q1, q2, ..., qn)`, the Euclidean distance is:\n",
    "    `d(p, q) = sqrt((p1-q1)^2 + (p2-q2)^2 + ... + (pn-qn)^2)`\n",
    "    It's suitable when the magnitude and direction of differences between features are important and features are on a similar scale.\n",
    "\n",
    "2.  **Manhattan Distance (L1 Norm):** This metric calculates the distance as the sum of the absolute differences of their Cartesian coordinates. Imagine navigating a city grid where you can only travel along horizontal or vertical streets. For points `p` and `q`:\n",
    "    `d(p, q) = |p1-q1| + |p2-q2| + ... + |pn-qn|`\n",
    "    Manhattan distance can be preferred over Euclidean when dealing with high-dimensional data or when features represent distinct concepts whose differences shouldn't be squared (which would amplify larger differences).\n",
    "\n",
    "3.  **Minkowski Distance:** This is a generalized distance metric. With a parameter `p` (not to be confused with the point p), it's defined as:\n",
    "    `d(p, q) = (sum(|pi-qi|^p))^(1/p)`\n",
    "    Euclidean distance is a special case of Minkowski distance where `p=2`. Manhattan distance is a special case where `p=1`. Other values of `p` can be used, but `p=1` and `p=2` are by far the most common. Using `p < 1` is rare as it doesn't satisfy the triangle inequality (a property of a true metric).\n",
    "\n",
    "The choice of distance metric can significantly impact the performance of the KNN algorithm. It should be chosen based on the nature of the data and the problem domain. For instance, if features have different units or scales (e.g., age in years and income in dollars), feature scaling becomes crucial before applying these distance metrics to prevent features with larger magnitudes from dominating the distance calculation.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example points\n",
    "point_a = np.array([1, 2])\n",
    "point_b = np.array([4, 6])\n",
    "new_point = np.array([2, 4])\n",
    "\n",
    "# Euclidean Distance\n",
    "euclidean_dist_ab = np.linalg.norm(point_a - point_b) # Default is L2 norm (Euclidean)\n",
    "euclidean_dist_new_a = np.linalg.norm(new_point - point_a)\n",
    "euclidean_dist_new_b = np.linalg.norm(new_point - point_b)\n",
    "print(f\"Euclidean Distance between A and B: {euclidean_dist_ab:.2f}\")\n",
    "print(f\"Euclidean Distance between New Point and A: {euclidean_dist_new_a:.2f}\")\n",
    "print(f\"Euclidean Distance between New Point and B: {euclidean_dist_new_b:.2f}\")\n",
    "\n",
    "# Manhattan Distance\n",
    "manhattan_dist_ab = np.sum(np.abs(point_a - point_b)) # L1 norm\n",
    "manhattan_dist_new_a = np.sum(np.abs(new_point - point_a))\n",
    "manhattan_dist_new_b = np.sum(np.abs(new_point - point_b))\n",
    "print(f\"Manhattan Distance between A and B: {manhattan_dist_ab:.2f}\")\n",
    "print(f\"Manhattan Distance between New Point and A: {manhattan_dist_new_a:.2f}\")\n",
    "print(f\"Manhattan Distance between New Point and B: {manhattan_dist_new_b:.2f}\")\n",
    "\n",
    "# Minkowski Distance (p=3)\n",
    "minkowski_dist_ab_p3 = np.power(np.sum(np.power(np.abs(point_a - point_b), 3)), 1/3)\n",
    "print(f\"Minkowski Distance (p=3) between A and B: {minkowski_dist_ab_p3:.2f}\")\n",
    "\n",
    "# Visualizing neighbors (conceptual)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter([point_a[0], point_b[0]], [point_a[1], point_b[1]], color=['blue', 'red'], s=100, label='Training Points (A, B)')\n",
    "plt.scatter(new_point[0], new_point[1], color='green', s=150, marker='X', label='New Point')\n",
    "# Lines to show distances\n",
    "plt.plot([new_point[0], point_a[0]], [new_point[1], point_a[1]], 'k--', alpha=0.5, label=f'Dist to A: {euclidean_dist_new_a:.2f}')\n",
    "plt.plot([new_point[0], point_b[0]], [new_point[1], point_b[1]], 'k:', alpha=0.5, label=f'Dist to B: {euclidean_dist_new_b:.2f}')\n",
    "plt.title('Distance-based Neighbor Visual')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `import numpy as np`: Imports NumPy for numerical calculations.\n",
    "2.  `import matplotlib.pyplot as plt`: Imports Matplotlib for plotting.\n",
    "3.  `import seaborn as sns`: Imports Seaborn for enhanced visualizations (though not heavily used in this snippet, good practice to include if generally used).\n",
    "4.  `point_a = np.array([1, 2])`: Defines the coordinates of point A.\n",
    "5.  `point_b = np.array([4, 6])`: Defines the coordinates of point B.\n",
    "6.  `new_point = np.array([2, 4])`: Defines the coordinates of a new point for which we might want to find neighbors.\n",
    "7.  `euclidean_dist_ab = np.linalg.norm(point_a - point_b)`: Calculates Euclidean distance between A and B using `np.linalg.norm` which defaults to L2 norm.\n",
    "8.  `euclidean_dist_new_a = np.linalg.norm(new_point - point_a)`: Euclidean distance between New Point and A.\n",
    "9.  `euclidean_dist_new_b = np.linalg.norm(new_point - point_b)`: Euclidean distance between New Point and B.\n",
    "10. `print(...)`: Prints the calculated Euclidean distances.\n",
    "11. `manhattan_dist_ab = np.sum(np.abs(point_a - point_b))`: Calculates Manhattan distance between A and B by summing absolute differences.\n",
    "12. `manhattan_dist_new_a = np.sum(np.abs(new_point - point_a))`: Manhattan distance between New Point and A.\n",
    "13. `manhattan_dist_new_b = np.sum(np.abs(new_point - point_b))`: Manhattan distance between New Point and B.\n",
    "14. `print(...)`: Prints the calculated Manhattan distances.\n",
    "15. `minkowski_dist_ab_p3 = np.power(np.sum(np.power(np.abs(point_a - point_b), 3)), 1/3)`: Calculates Minkowski distance with p=3.\n",
    "16. `print(...)`: Prints the calculated Minkowski distance.\n",
    "17. `plt.figure(figsize=(6, 5))`: Creates a new Matplotlib figure with a specified size.\n",
    "18. `plt.scatter(...)`: Plots points A, B, and the New Point with different colors and markers.\n",
    "19. `plt.plot(...)`: Draws dashed and dotted lines representing the Euclidean distances from the New Point to A and B.\n",
    "20. `plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`, `plt.legend()`, `plt.grid(True)`: Standard Matplotlib commands to add plot details.\n",
    "21. `plt.show()`: Displays the plot.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The Value of K: Bias-Variance Tradeoff and Tuning\n",
    "\n",
    "The choice of 'K', the number of neighbors to consider, is a critical hyperparameter in the KNN algorithm. It significantly influences the model's performance and directly impacts the bias-variance tradeoff.\n",
    "**Low K (e.g., K=1):**\n",
    "*   **Low Bias:** The model is very flexible and can capture fine-grained patterns in the data, fitting closely to the training examples. The decision boundary will be highly irregular and sensitive to individual data points.\n",
    "*   **High Variance:** The model is susceptible to noise in the training data. A slight change in the training set can lead to a drastically different decision boundary. It tends to overfit, performing well on training data but poorly on unseen test data.\n",
    "**High K (e.g., K=N, where N is the total number of training points):**\n",
    "*   **High Bias:** The model becomes overly simplistic. For classification, it would predict the majority class of the entire training set for every new point, ignoring local data structure. The decision boundary becomes very smooth.\n",
    "*   **Low Variance:** The model is stable and less affected by noise or small changes in the training data. However, it tends to underfit, failing to capture the underlying patterns and performing poorly on both training and test data.\n",
    "\n",
    "The **bias-variance tradeoff** implies that there's an optimal value of K that balances these two extremes. A K that is too small leads to overfitting (high variance), while a K that is too large leads to underfitting (high bias). The goal is to find a K that generalizes well to new, unseen data.\n",
    "\n",
    "**Tuning K using Cross-Validation:**\n",
    "The most common method to find the optimal K is through **cross-validation**. The k-fold cross-validation process is typically used:\n",
    "1.  Split the training data into 'k_cv' folds (e.g., 5 or 10 folds).\n",
    "2.  For each potential value of K (e.g., K=1, 3, 5, ..., up to a reasonable limit):\n",
    "    a.  Iterate 'k_cv' times: In each iteration, use one fold as the validation set and the remaining 'k_cv-1' folds as the training set.\n",
    "    b.  Train the KNN model with the current K on the 'k_cv-1' folds.\n",
    "    c.  Evaluate its performance (e.g., accuracy) on the held-out validation fold.\n",
    "3.  Average the performance scores across all 'k_cv' folds for that specific K.\n",
    "4.  Select the K value that yields the best average performance.\n",
    "A common rule of thumb is to choose K as an odd number to avoid ties in binary classification, though scikit-learn's KNN handles ties systematically. Generally, K is chosen to be `sqrt(N)`, where N is the number of samples, as a starting point for experimentation.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Scale features (important for KNN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data (though for CV on full dataset, this isn't strictly needed for this demo)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Method 1: Manual Cross-Validation Loop for K\n",
    "k_values = range(1, 31, 2) # Test odd K values from 1 to 29\n",
    "cv_scores = []\n",
    "\n",
    "for k_val in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_val)\n",
    "    # Perform 5-fold cross-validation\n",
    "    scores = cross_val_score(knn, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "    print(f\"K={k_val}, Mean CV Accuracy: {scores.mean():.4f}\")\n",
    "\n",
    "# Find optimal K\n",
    "optimal_k_manual = k_values[np.argmax(cv_scores)]\n",
    "print(f\"\\nOptimal K (manual CV) = {optimal_k_manual} with accuracy {max(cv_scores):.4f}\")\n",
    "\n",
    "# Plot K vs. Accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, cv_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title('K Value vs. Cross-Validated Accuracy')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Method 2: Using GridSearchCV (more automated)\n",
    "param_grid = {'n_neighbors': range(1, 31, 2)}\n",
    "knn_grid = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_grid, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_scaled, y) # Fit on the entire available data for hyperparameter tuning\n",
    "\n",
    "print(f\"\\nBest K (GridSearchCV): {grid_search.best_params_['n_neighbors']}\")\n",
    "print(f\"Best CV Accuracy (GridSearchCV): {grid_search.best_score_:.4f}\")\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `from sklearn.datasets import load_iris`: Imports the Iris dataset.\n",
    "2.  `from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV`: Imports tools for data splitting, cross-validation, and grid search.\n",
    "3.  `from sklearn.preprocessing import StandardScaler`: Imports the feature scaler.\n",
    "4.  `from sklearn.neighbors import KNeighborsClassifier`: Imports the KNN classifier.\n",
    "5.  `iris = load_iris()`: Loads the dataset.\n",
    "6.  `X, y = iris.data, iris.target`: Separates features (X) and target (y).\n",
    "7.  `scaler = StandardScaler()`: Initializes the scaler.\n",
    "8.  `X_scaled = scaler.fit_transform(X)`: Scales the features.\n",
    "9.  `k_values = range(1, 31, 2)`: Defines a range of K values to test (odd numbers from 1 to 29).\n",
    "10. `cv_scores = []`: Initializes a list to store cross-validation scores for each K.\n",
    "11. `for k_val in k_values:`: Loop through each K value.\n",
    "12. `knn = KNeighborsClassifier(n_neighbors=k_val)`: Initializes KNN with the current K.\n",
    "13. `scores = cross_val_score(knn, X_scaled, y, cv=5, scoring='accuracy')`: Performs 5-fold cross-validation, getting accuracy scores for each fold.\n",
    "14. `cv_scores.append(scores.mean())`: Appends the mean accuracy for the current K to `cv_scores`.\n",
    "15. `print(...)`: Prints the K value and its mean CV accuracy.\n",
    "16. `optimal_k_manual = k_values[np.argmax(cv_scores)]`: Finds the K value that resulted in the highest mean accuracy.\n",
    "17. `print(...)`: Prints the optimal K found manually.\n",
    "18. `plt.figure(...)`, `plt.plot(...)`, `plt.title(...)`, etc.: Plotting commands to visualize K vs. accuracy.\n",
    "19. `plt.show()`: Displays the plot.\n",
    "20. `param_grid = {'n_neighbors': range(1, 31, 2)}`: Defines the parameter grid for GridSearchCV.\n",
    "21. `knn_grid = KNeighborsClassifier()`: Initializes a new KNN classifier for GridSearchCV.\n",
    "22. `grid_search = GridSearchCV(knn_grid, param_grid, cv=5, scoring='accuracy', verbose=1)`: Initializes GridSearchCV to search for the best `n_neighbors` using 5-fold CV.\n",
    "23. `grid_search.fit(X_scaled, y)`: Runs the grid search.\n",
    "24. `print(...)`: Prints the best K and best CV accuracy found by GridSearchCV.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Data Preprocessing for KNN\n",
    "\n",
    "Data preprocessing is a critical step before applying the KNN algorithm, as its performance is highly sensitive to the characteristics of the input data. Key preprocessing steps include:\n",
    "\n",
    "1.  **Feature Scaling (Standardization or Normalization):**\n",
    "    KNN relies on distance metrics (like Euclidean) to determine nearness. If features are on different scales (e.g., one feature ranges from 0-1, another from 0-1000), the feature with the larger range will disproportionately dominate the distance calculation, leading to biased results.\n",
    "    *   **Standardization (Z-score normalization):** Transforms data to have a mean of 0 and a standard deviation of 1. `X_scaled = (X - mean(X)) / std(X)`. It's generally preferred if the data follows a Gaussian distribution, but works well even if it doesn't.\n",
    "    *   **Normalization (Min-Max scaling):** Scales data to a fixed range, usually [0, 1] or [-1, 1]. `X_scaled = (X - min(X)) / (max(X) - min(X))`. Useful when you need bounded values, but sensitive to outliers.\n",
    "    This step ensures that all features contribute equally to the distance computation.\n",
    "\n",
    "2.  **Handling Missing Values:**\n",
    "    KNN cannot directly handle missing values because distance metrics require complete numerical data. Common strategies include:\n",
    "    *   **Imputation:** Replacing missing values with a substitute value.\n",
    "        *   **Mean/Median Imputation:** For numerical features, replace missing values with the mean or median of the column. Median is robust to outliers.\n",
    "        *   **Mode Imputation:** For categorical features, replace missing values with the most frequent category (mode).\n",
    "        *   **KNN Imputation:** Ironically, KNN itself can be used to impute missing values. It finds the k-nearest neighbors to the sample with the missing value (based on other available features) and imputes the missing value based on the values of these neighbors (e.g., mean for numerical, mode for categorical).\n",
    "    *   **Deletion:** Removing rows with missing values (if few) or columns (if many values are missing and the feature isn't critical). This can lead to loss of valuable data.\n",
    "\n",
    "3.  **Encoding Categorical Data:**\n",
    "    Distance metrics are defined for numerical data. Categorical features (e.g., 'color': 'Red', 'Blue', 'Green') must be converted into a numerical format.\n",
    "    *   **One-Hot Encoding:** Creates new binary (0 or 1) columns for each category. For 'color', it would create 'Color_Red', 'Color_Blue', 'Color_Green'. This is suitable for nominal categorical data where there's no inherent order. It can lead to high dimensionality if the categorical feature has many unique values.\n",
    "    *   **Label Encoding:** Assigns a unique integer to each category (e.g., Red=0, Blue=1, Green=2). This implies an ordinal relationship, which might mislead KNN if no such order exists. It's suitable for ordinal data (e.g., 'Size': 'Small', 'Medium', 'Large').\n",
    "    When using one-hot encoding, the resulting binary features are already on a similar scale (0 or 1), but if mixed with continuous features, the continuous features still need scaling.\n",
    "\n",
    "Proper preprocessing ensures that the KNN algorithm can operate effectively and produce meaningful results by providing it with clean, consistently scaled, and numerically represented data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame with mixed data types and missing values\n",
    "data = {\n",
    "    'age': [25, 30, np.nan, 35, 22],\n",
    "    'income': [50000, 60000, 75000, np.nan, 45000],\n",
    "    'gender': ['Male', 'Female', 'Female', 'Male', 'Male'],\n",
    "    'city': ['New York', 'London', 'Paris', 'New York', 'London'],\n",
    "    'target': [0, 1, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = ['age', 'income']\n",
    "categorical_features = ['gender', 'city']\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical data\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')), # 1. Impute missing values with median\n",
    "    ('scaler', StandardScaler())                  # 2. Scale numerical features\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # 1. Impute missing cat values with mode\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))    # 2. One-hot encode categorical features\n",
    "])\n",
    "\n",
    "# Create a column transformer to apply different pipelines to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep any other columns (if any)\n",
    ")\n",
    "\n",
    "# Apply the preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"Original X:\")\n",
    "print(X)\n",
    "print(\"\\nProcessed X (shape):\", X_processed.shape)\n",
    "print(\"Processed X (first row, can be sparse if one-hot encoded):\")\n",
    "print(X_processed[0]) # This might be a sparse matrix row\n",
    "\n",
    "# To see it as a dense array (for understanding)\n",
    "print(\"\\nProcessed X (dense format, first row):\")\n",
    "print(X_processed.toarray()[0] if hasattr(X_processed, 'toarray') else X_processed[0])\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `import pandas as pd`: Imports Pandas for DataFrame manipulation.\n",
    "2.  `from sklearn.preprocessing import StandardScaler, OneHotEncoder`: Imports scalers and encoders.\n",
    "3.  `from sklearn.impute import SimpleImputer`: Imports imputer for handling missing values.\n",
    "4.  `from sklearn.compose import ColumnTransformer`: Imports tool to apply different transformations to different columns.\n",
    "5.  `from sklearn.pipeline import Pipeline`: Imports tool to chain multiple preprocessing steps.\n",
    "6.  `data = {...}`: Defines sample raw data with numerical, categorical, and missing values.\n",
    "7.  `df = pd.DataFrame(data)`: Creates a Pandas DataFrame.\n",
    "8.  `X = df.drop('target', axis=1)`: Separates features.\n",
    "9.  `y = df['target']`: Separates the target variable.\n",
    "10. `numerical_features = ['age', 'income']`: Lists numerical column names.\n",
    "11. `categorical_features = ['gender', 'city']`: Lists categorical column names.\n",
    "12. `numerical_pipeline = Pipeline(...)`: Defines a pipeline for numerical features:\n",
    "    *   `('imputer', SimpleImputer(strategy='median'))`: First, impute missing values using the median.\n",
    "    *   `('scaler', StandardScaler())`: Then, scale the features using Standardization.\n",
    "13. `categorical_pipeline = Pipeline(...)`: Defines a pipeline for categorical features:\n",
    "    *   `('imputer', SimpleImputer(strategy='most_frequent'))`: First, impute missing values using the mode.\n",
    "    *   `('onehot', OneHotEncoder(handle_unknown='ignore'))`: Then, apply One-Hot Encoding. `handle_unknown='ignore'` means if a new category appears in test data, its one-hot columns will be all zeros.\n",
    "14. `preprocessor = ColumnTransformer(...)`: Initializes a ColumnTransformer:\n",
    "    *   `('num', numerical_pipeline, numerical_features)`: Applies `numerical_pipeline` to `numerical_features`.\n",
    "    *   `('cat', categorical_pipeline, categorical_features)`: Applies `categorical_pipeline` to `categorical_features`.\n",
    "    *   `remainder='passthrough'`: Any columns not specified in `numerical_features` or `categorical_features` will be passed through unchanged (though in this example, all are covered).\n",
    "15. `X_processed = preprocessor.fit_transform(X)`: Fits the preprocessor on `X` and transforms it. This learns imputation values (median, mode) and scaling parameters (mean, std) from `X`, and also identifies unique categories for one-hot encoding, then applies all transformations.\n",
    "16. `print(...)`: Prints the original and processed data to show the transformation. The processed data will have imputed values, scaled numerical features, and one-hot encoded categorical features, resulting in more columns.\n",
    "17. `print(X_processed.toarray()[0] ...)`: If `X_processed` is a sparse matrix (common with `OneHotEncoder`), this converts the first row to a dense array for easier inspection.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. KNN Prediction and Decision Boundaries\n",
    "\n",
    "Once the KNN model (which is essentially the stored training data) is ready and a new data point arrives for classification, the prediction process involves two main steps:\n",
    "\n",
    "1.  **Finding the K Nearest Neighbors:**\n",
    "    For the new, unclassified data point, the algorithm calculates the distance (e.g., Euclidean, Manhattan) to every single data point in the stored training set. After computing all these distances, it identifies the 'K' training data points that have the smallest distances to the new point. These K points are its \"nearest neighbors.\" The choice of K and the distance metric are crucial here, as discussed earlier.\n",
    "\n",
    "2.  **Majority Voting for Classification:**\n",
    "    After identifying the K nearest neighbors, the algorithm looks at their class labels. The new data point is assigned the class label that is most frequent among these K neighbors. This is known as **majority voting**. For example, if K=5 and among the 5 nearest neighbors, 3 belong to Class A, 1 to Class B, and 1 to Class C, the new point will be classified as Class A. In case of a tie (e.g., for K=4, 2 neighbors are Class A and 2 are Class B), the tie-breaking mechanism can vary. Scikit-learn's `KNeighborsClassifier` will, by default, pick the class of the neighbor that is closer among the tied groups or, if distances are also equal, pick the one that appeared first in the training data. It's often recommended to use an odd K for binary classification to avoid ties, though this isn't a strict requirement. Some implementations might also use **weighted voting**, where closer neighbors have a greater influence on the final vote (e.g., weight = 1/distance).\n",
    "\n",
    "**Decision Boundaries:**\n",
    "The decision boundary in KNN is the imaginary line or surface that separates different classes in the feature space. For KNN, these boundaries are formed implicitly by the locations of the training data points.\n",
    "*   The decision boundary is locally linear but can form complex, non-linear overall shapes.\n",
    "*   **Effect of K:**\n",
    "    *   **Small K (e.g., K=1):** The decision boundary will be highly irregular, jagged, and sensitive to individual training points, potentially capturing noise (high variance). Each training point effectively carves out its own region of influence.\n",
    "    *   **Large K:** The decision boundary becomes smoother and less complex, generalizing more but potentially missing finer patterns (high bias).\n",
    "*   **Effect of Distance Metric:** Different distance metrics can lead to different shapes of the \"neighborhood\" and thus alter the decision boundaries. For example, Euclidean distance defines spherical neighborhoods, while Manhattan distance defines diamond-shaped (in 2D) or hyper-rectangular neighborhoods.\n",
    "\n",
    "Visualizing decision boundaries, especially in 2D, provides great insight into how KNN works and how K affects its behavior.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# A library for plotting decision regions (install if you don't have it: pip install mlxtend)\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Generate synthetic 2-feature data for easy visualization\n",
    "X_vis, y_vis = make_classification(n_samples=100, n_features=2, n_informative=2,\n",
    "                                   n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_vis = StandardScaler()\n",
    "X_vis_scaled = scaler_vis.fit_transform(X_vis)\n",
    "\n",
    "# New point to classify (example)\n",
    "new_point_vis = np.array([[0.5, 0.5]]) # This point needs to be scaled like the training data\n",
    "new_point_vis_scaled = scaler_vis.transform(new_point_vis)\n",
    "\n",
    "# Plot decision boundaries for different K values\n",
    "k_options = [1, 5, 15]\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, k_val in enumerate(k_options):\n",
    "    knn_vis = KNeighborsClassifier(n_neighbors=k_val)\n",
    "    knn_vis.fit(X_vis_scaled, y_vis)\n",
    "\n",
    "    # Predict class for the new point\n",
    "    pred_class = knn_vis.predict(new_point_vis_scaled)\n",
    "    pred_proba = knn_vis.predict_proba(new_point_vis_scaled) # Shows probability for each class\n",
    "\n",
    "    plt.subplot(1, len(k_options), i + 1)\n",
    "    plot_decision_regions(X_vis_scaled, y_vis, clf=knn_vis, legend=2)\n",
    "    plt.scatter(new_point_vis_scaled[:, 0], new_point_vis_scaled[:, 1],\n",
    "                marker='x', color='red', s=100, label=f'New Point (Pred: {pred_class[0]})')\n",
    "    plt.title(f'KNN Decision Boundary (K={k_val})\\nNew point probs: {np.round(pred_proba[0],2)}')\n",
    "    plt.xlabel('Feature 1 (Scaled)')\n",
    "    plt.ylabel('Feature 2 (Scaled)')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Illustrating majority vote conceptually (not direct code for voting logic, as sklearn handles it)\n",
    "# For K=5, if distances to new_point are calculated, and the 5 closest neighbors have labels:\n",
    "# [Class_A, Class_A, Class_B, Class_A, Class_B]\n",
    "# Majority vote: Class_A (3 votes) vs Class_B (2 votes) -> Predict Class_A\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `from sklearn.datasets import make_classification`: Imports a function to generate synthetic classification datasets.\n",
    "2.  `from mlxtend.plotting import plot_decision_regions`: Imports a utility function from `mlxtend` for plotting decision boundaries (requires `pip install mlxtend`).\n",
    "3.  `X_vis, y_vis = make_classification(...)`: Generates a 2D dataset with 100 samples and 2 classes.\n",
    "4.  `scaler_vis = StandardScaler()`: Initializes a scaler.\n",
    "5.  `X_vis_scaled = scaler_vis.fit_transform(X_vis)`: Scales the features.\n",
    "6.  `new_point_vis = np.array([[0.5, 0.5]])`: Defines a new point for classification.\n",
    "7.  `new_point_vis_scaled = scaler_vis.transform(new_point_vis)`: Scales the new point using the *same* scaler fitted on the training data.\n",
    "8.  `k_options = [1, 5, 15]`: Defines a list of K values to test.\n",
    "9.  `plt.figure(figsize=(15, 5))`: Creates a Matplotlib figure for subplots.\n",
    "10. `for i, k_val in enumerate(k_options):`: Loops through each K value.\n",
    "11. `knn_vis = KNeighborsClassifier(n_neighbors=k_val)`: Initializes KNN with the current K.\n",
    "12. `knn_vis.fit(X_vis_scaled, y_vis)`: \"Trains\" the KNN (stores the data).\n",
    "13. `pred_class = knn_vis.predict(new_point_vis_scaled)`: Predicts the class for the new point.\n",
    "14. `pred_proba = knn_vis.predict_proba(new_point_vis_scaled)`: Gets class probabilities for the new point (proportion of neighbors belonging to each class).\n",
    "15. `plt.subplot(1, len(k_options), i + 1)`: Creates a subplot for the current K.\n",
    "16. `plot_decision_regions(X_vis_scaled, y_vis, clf=knn_vis, legend=2)`: Plots the decision regions for the trained KNN model on the scaled data.\n",
    "17. `plt.scatter(...)`: Plots the new point on the decision boundary plot, labeled with its predicted class.\n",
    "18. `plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`, `plt.legend()`: Sets plot details. The title includes the probabilities.\n",
    "19. `plt.tight_layout()`: Adjusts subplot parameters for a tight layout.\n",
    "20. `plt.show()`: Displays the plot.\n",
    "21. `# Illustrating majority vote...`: Conceptual comment explaining how majority vote works.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Model Evaluation Techniques for KNN Classification\n",
    "\n",
    "Evaluating the performance of a KNN classification model is crucial to understand its effectiveness and compare it with other models or different KNN configurations (e.g., different K values). Common evaluation metrics include:\n",
    "\n",
    "1.  **Accuracy:** The proportion of correctly classified instances out of the total instances.\n",
    "    `Accuracy = (True Positives + True Negatives) / (Total Instances)`\n",
    "    While intuitive, accuracy can be misleading for imbalanced datasets where one class significantly outnumbers others. A model predicting the majority class all the time might have high accuracy but be useless.\n",
    "\n",
    "2.  **Confusion Matrix:** A table that summarizes the performance of a classification algorithm. For a binary classification problem, it has four cells:\n",
    "    *   **True Positives (TP):** Instances correctly predicted as positive.\n",
    "    *   **True Negatives (TN):** Instances correctly predicted as negative.\n",
    "    *   **False Positives (FP) (Type I Error):** Instances incorrectly predicted as positive (actually negative).\n",
    "    *   **False Negatives (FN) (Type II Error):** Instances incorrectly predicted as negative (actually positive).\n",
    "    The confusion matrix provides a detailed breakdown of correct and incorrect classifications for each class.\n",
    "\n",
    "3.  **Precision:** Measures the proportion of correctly predicted positive instances among all instances predicted as positive.\n",
    "    `Precision = TP / (TP + FP)`\n",
    "    High precision means that when the model predicts a positive class, it is very likely correct. It answers: \"Of all instances the model labeled as positive, how many were actually positive?\"\n",
    "\n",
    "4.  **Recall (Sensitivity or True Positive Rate):** Measures the proportion of actual positive instances that were correctly identified by the model.\n",
    "    `Recall = TP / (TP + FN)`\n",
    "    High recall means the model is good at finding all the positive instances. It answers: \"Of all actual positive instances, how many did the model correctly identify?\"\n",
    "\n",
    "5.  **F1-Score:** The harmonic mean of precision and recall. It provides a single score that balances both concerns.\n",
    "    `F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "    It's useful when you need a balance between precision and recall, especially if the class distribution is uneven.\n",
    "\n",
    "6.  **ROC (Receiver Operating Characteristic) Curve and AUC (Area Under the ROC Curve):**\n",
    "    *   The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate (`FPR = FP / (FP + TN)`) at various threshold settings for a classifier that outputs probabilities.\n",
    "    *   AUC represents the area under the ROC curve. An AUC of 1.0 indicates a perfect classifier, while an AUC of 0.5 suggests a classifier performing no better than random guessing. AUC is a good measure of the model's ability to distinguish between classes, irrespective of the classification threshold.\n",
    "\n",
    "These metrics should be calculated on a separate test set (or via cross-validation) that was not used during the training or K-tuning phase to get an unbiased estimate of the model's generalization performance. Visual interpretations, like plotting the confusion matrix as a heatmap or the ROC curve, greatly aid in understanding model behavior.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \\\n",
    "                            confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load Iris dataset (multi-class, but we can adapt for ROC/AUC for one-vs-rest or choose a binary problem)\n",
    "# For simplicity with ROC/AUC, let's make it binary: class 0 vs class 1+2\n",
    "iris = load_iris()\n",
    "X, y_orig = iris.data, iris.target\n",
    "y = np.where(y_orig == 0, 0, 1) # Class 0 vs Rest (Class 1 and 2 combined into 1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Preprocess: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train KNN model (assuming K=5 is chosen after tuning)\n",
    "knn_eval = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_eval.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_eval.predict(X_test_scaled)\n",
    "y_pred_proba = knn_eval.predict_proba(X_test_scaled)[:, 1] # Probabilities for the positive class (class 1)\n",
    "\n",
    "# 1. Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred Neg (0)', 'Pred Pos (1)'], yticklabels=['Actual Neg (0)', 'Actual Pos (1)'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 3. Precision, Recall, F1-Score (can also use classification_report)\n",
    "precision = precision_score(y_test, y_pred) # For positive class (1) by default\n",
    "recall = recall_score(y_test, y_pred)       # For positive class (1) by default\n",
    "f1 = f1_score(y_test, y_pred)               # For positive class (1) by default\n",
    "print(f\"\\nPrecision (for class 1): {precision:.4f}\")\n",
    "print(f\"Recall (for class 1): {recall:.4f}\")\n",
    "print(f\"F1-Score (for class 1): {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1 (Rest)']))\n",
    "\n",
    "# 4. ROC Curve and AUC (for binary classification)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `from sklearn.metrics import ...`: Imports various evaluation metrics.\n",
    "2.  `y = np.where(y_orig == 0, 0, 1)`: Converts the multi-class Iris target into a binary target (class 0 vs. the rest) for easier ROC/AUC demonstration.\n",
    "3.  `X_train, X_test, y_train, y_test = train_test_split(...)`: Splits data, `stratify=y` ensures class proportions are similar in train/test.\n",
    "4.  `scaler = StandardScaler()`: Initializes scaler.\n",
    "5.  `X_train_scaled = scaler.fit_transform(X_train)`: Fits scaler on training data and transforms it.\n",
    "6.  `X_test_scaled = scaler.transform(X_test)`: Transforms test data using the *fitted* scaler.\n",
    "7.  `knn_eval = KNeighborsClassifier(n_neighbors=5)`: Initializes KNN (K=5 assumed optimal).\n",
    "8.  `knn_eval.fit(X_train_scaled, y_train)`: Trains KNN.\n",
    "9.  `y_pred = knn_eval.predict(X_test_scaled)`: Gets class predictions.\n",
    "10. `y_pred_proba = knn_eval.predict_proba(X_test_scaled)[:, 1]`: Gets probability estimates for the positive class (class 1). This is needed for ROC AUC.\n",
    "11. `acc = accuracy_score(y_test, y_pred)`: Calculates accuracy.\n",
    "12. `cm = confusion_matrix(y_test, y_pred)`: Generates the confusion matrix.\n",
    "13. `sns.heatmap(cm, ...)`: Visualizes the confusion matrix as a heatmap.\n",
    "14. `precision = precision_score(...)`, `recall = recall_score(...)`, `f1 = f1_score(...)`: Calculates precision, recall, and F1 for the positive class.\n",
    "15. `print(classification_report(...))`: Prints a comprehensive report including precision, recall, F1-score for each class, and support.\n",
    "16. `fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)`: Calculates False Positive Rate, True Positive Rate, and thresholds for the ROC curve.\n",
    "17. `auc_score = roc_auc_score(y_test, y_pred_proba)`: Calculates the Area Under the ROC Curve.\n",
    "18. `plt.plot(fpr, tpr, ...)`: Plots the ROC curve.\n",
    "19. `plt.plot([0, 1], [0, 1], ...)`: Plots the diagonal line representing a random classifier.\n",
    "20. `plt.xlabel(...)`, `plt.ylabel(...)`, `plt.title(...)`, `plt.legend(...)`, `plt.grid(True)`, `plt.show()`: Standard plot formatting.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Challenges with KNN and How to Overcome Them\n",
    "\n",
    "While KNN is simple and intuitive, it faces several challenges that can affect its performance and applicability:\n",
    "\n",
    "1.  **Curse of Dimensionality:**\n",
    "    *   **Challenge:** As the number of features (dimensions) increases, the distance between any two points in a high-dimensional space tends to become very similar (distances concentrate). This makes the concept of \"nearest\" neighbors less meaningful, as points become almost equidistant from each other. Consequently, the predictive power of KNN degrades significantly. Moreover, the volume of the feature space grows exponentially with dimensions, requiring a much larger dataset to maintain the same density of data points.\n",
    "    *   **Overcoming:**\n",
    "        *   **Dimensionality Reduction:** Techniques like Principal Component Analysis (PCA) or Linear Discriminant Analysis (LDA) can be used to project the data onto a lower-dimensional subspace while retaining most of the important information. PCA finds directions of maximum variance, while LDA aims to find directions that maximize class separability.\n",
    "        *   **Feature Selection:** Select only the most relevant features and discard redundant or irrelevant ones. This can be done using filter methods (e.g., chi-squared test, information gain), wrapper methods (e.g., recursive feature elimination), or embedded methods.\n",
    "        *   Using distance metrics less sensitive to high dimensions (e.g., Manhattan distance can sometimes outperform Euclidean in very high dimensions, or cosine similarity for sparse data).\n",
    "\n",
    "2.  **Computational Cost:**\n",
    "    *   **Challenge:** KNN is a lazy learner, meaning it does no explicit training. However, during prediction, it needs to compute distances from the new point to *all* training points. This can be computationally expensive, especially with large datasets (many samples) and high-dimensional data (many features). Prediction time complexity is O(N*D) for N samples and D dimensions, plus sorting time (O(N log N) or O(N) if K is small and specialized algorithms are used).\n",
    "    *   **Overcoming:**\n",
    "        *   **Approximate Nearest Neighbor (ANN) Algorithms:** Techniques like KD-Trees or Ball Trees can organize the training data into a tree-like structure, allowing for much faster querying of nearest neighbors (often O(log N) on average), though they can suffer in very high dimensions. Scikit-learn's `KNeighborsClassifier` can use these (`algorithm='kd_tree'` or `algorithm='ball_tree'`).\n",
    "        *   **Data Reduction/Prototyping:** Select a subset of representative prototypes from the training data to reduce the number of distance calculations.\n",
    "\n",
    "3.  **Sensitivity to Feature Scaling and Irrelevant Features:**\n",
    "    *   **Challenge:** As discussed in preprocessing, KNN is highly sensitive to the scale of features. Features with larger magnitudes can dominate distance calculations. Additionally, irrelevant features can mislead the algorithm by contributing noise to the distance measures, obscuring the true similarity between points.\n",
    "    *   **Overcoming:**\n",
    "        *   **Feature Scaling:** Always apply standardization or normalization.\n",
    "        *   **Feature Selection/Engineering:** Remove irrelevant features or create new, more informative features. Weighted KNN, where features are assigned weights based on their importance, can also be an option, though less common in standard libraries.\n",
    "\n",
    "4.  **Imbalanced Datasets:**\n",
    "    *   **Challenge:** If one class is much more frequent than others, KNN (using majority vote) will tend to be biased towards predicting the majority class, as it's more likely that neighbors will belong to it. This leads to poor performance on minority classes.\n",
    "    *   **Overcoming:**\n",
    "        *   **Resampling Techniques:**\n",
    "            *   **Oversampling:** Increase the number of instances in the minority class (e.g., SMOTE - Synthetic Minority Over-sampling Technique, which creates synthetic samples).\n",
    "            *   **Undersampling:** Decrease the number of instances in the majority class (e.g., randomly removing samples, or more advanced methods like Tomek Links).\n",
    "        *   **Cost-Sensitive Learning:** Assign different misclassification costs to different classes. (Not directly supported by standard KNN, but can be implemented by modifying the voting mechanism or using weighted samples).\n",
    "        *   **Choosing appropriate K:** A smaller K might be more sensitive to local minority class structures.\n",
    "        *   **Using different evaluation metrics:** Focus on metrics like Precision, Recall, F1-score, or AUC for minority classes rather than just accuracy.\n",
    "\n",
    "By being aware of these challenges and applying appropriate mitigation strategies, the effectiveness of KNN can be significantly improved.\n",
    "\n",
    "```python\n",
    "# Example of PCA for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits # MNIST digits subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load digits dataset (64 features - 8x8 images)\n",
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "# Scale data before PCA\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "\n",
    "# Apply PCA to reduce to a smaller number of components (e.g., 10)\n",
    "n_components_pca = 10\n",
    "pca = PCA(n_components=n_components_pca, random_state=42)\n",
    "X_digits_pca = pca.fit_transform(X_digits_scaled)\n",
    "\n",
    "print(f\"Original number of features: {X_digits_scaled.shape[1]}\")\n",
    "print(f\"Reduced number of features after PCA: {X_digits_pca.shape[1]}\")\n",
    "print(f\"Explained variance by {n_components_pca} components: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# Plot explained variance ratio to see how many components are needed\n",
    "pca_full = PCA(random_state=42).fit(X_digits_scaled)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by PCA Components (Digits Dataset)')\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance') # Mark 95% variance\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# This X_digits_pca can now be used to train a KNN model\n",
    "# knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "# knn_pca.fit(X_digits_pca, y_digits)\n",
    "# ... further evaluation ...\n",
    "\n",
    "# Example of using KD-Tree for faster neighbor search (default is 'auto' in scikit-learn)\n",
    "# For larger datasets, explicitly setting algorithm='kd_tree' or 'ball_tree' can be beneficial\n",
    "# knn_fast = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
    "# This is more about internal optimization and might not show different results, just faster prediction.\n",
    "```\n",
    "*Line-by-line Explanation (PCA part):*\n",
    "1.  `from sklearn.decomposition import PCA`: Imports PCA.\n",
    "2.  `from sklearn.datasets import load_digits`: Imports the digits dataset (a common example for dimensionality).\n",
    "3.  `digits = load_digits()`: Loads the dataset.\n",
    "4.  `X_digits, y_digits = digits.data, digits.target`: Separates features and target. `digits.data` has 64 features (8x8 pixel images flattened).\n",
    "5.  `scaler_digits = StandardScaler()`: Initializes scaler.\n",
    "6.  `X_digits_scaled = scaler_digits.fit_transform(X_digits)`: Scales the digit features. PCA is sensitive to feature scaling.\n",
    "7.  `n_components_pca = 10`: Sets the desired number of principal components.\n",
    "8.  `pca = PCA(n_components=n_components_pca, random_state=42)`: Initializes PCA to reduce to 10 components.\n",
    "9.  `X_digits_pca = pca.fit_transform(X_digits_scaled)`: Fits PCA on scaled data and transforms it to the lower-dimensional space.\n",
    "10. `print(...)`: Shows the change in feature dimensions and the total variance explained by the selected components.\n",
    "11. `pca_full = PCA(random_state=42).fit(X_digits_scaled)`: Fits PCA with all possible components to see the cumulative explained variance.\n",
    "12. `plt.plot(np.cumsum(pca_full.explained_variance_ratio_))`: Plots the cumulative sum of explained variance by each component.\n",
    "13. `plt.axhline(...)`: Adds a horizontal line at 95% explained variance for reference.\n",
    "14. `plt.xlabel(...)`, `plt.ylabel(...)`, `plt.title(...)`, `plt.legend()`, `plt.grid(True)`, `plt.show()`: Standard plot formatting.\n",
    "15. `# knn_pca = ...`: Comments indicating how the PCA-transformed data would be used with KNN.\n",
    "16. `# knn_fast = ...`: Comment illustrating how to specify the algorithm for neighbor search in `KNeighborsClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Full Practical Implementation (e.g., Iris Dataset)\n",
    "\n",
    "Let's go through an end-to-end KNN classification example using the Iris dataset. This will cover:\n",
    "1.  **Exploratory Data Analysis (EDA):** Basic understanding of the data.\n",
    "2.  **Preprocessing:** Feature scaling.\n",
    "3.  **Train-Test Split:** Separating data for training and evaluation.\n",
    "4.  **Model Training and Tuning:** Finding the optimal K using `GridSearchCV`.\n",
    "5.  **Prediction:** Making predictions on the test set.\n",
    "6.  **Evaluation:** Using various metrics and visualizations.\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**\n",
    "The Iris dataset contains 150 samples of iris flowers, each with 4 features (sepal length, sepal width, petal length, petal width) and a target variable indicating the species (Setosa, Versicolor, Virginica).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y_iris = pd.Series(iris.target, name='species')\n",
    "df_iris = pd.concat([X_iris, y_iris], axis=1)\n",
    "\n",
    "# Basic info\n",
    "print(\"Dataset Info:\")\n",
    "df_iris.info()\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_iris.head())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df_iris['species'].value_counts())\n",
    "\n",
    "# Pairplot for visualization\n",
    "sns.pairplot(df_iris, hue='species', markers=[\"o\", \"s\", \"D\"])\n",
    "plt.suptitle(\"Pairplot of Iris Dataset Features\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(X_iris.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "```\n",
    "*Line-by-line Explanation (EDA):*\n",
    "1.  `iris = load_iris()`: Loads the Iris dataset from scikit-learn.\n",
    "2.  `X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)`: Creates a Pandas DataFrame for features with appropriate column names.\n",
    "3.  `y_iris = pd.Series(iris.target, name='species')`: Creates a Pandas Series for the target variable.\n",
    "4.  `df_iris = pd.concat([X_iris, y_iris], axis=1)`: Combines features and target into a single DataFrame for easier EDA.\n",
    "5.  `df_iris.info()`: Prints a summary of the DataFrame, including data types and non-null counts.\n",
    "6.  `df_iris.head()`: Displays the first 5 rows of the DataFrame.\n",
    "7.  `df_iris['species'].value_counts()`: Shows the distribution of classes (species), revealing it's a balanced dataset (50 samples per class).\n",
    "8.  `sns.pairplot(df_iris, hue='species', ...)`: Creates a matrix of scatter plots for each pair of features, colored by species. This helps visualize class separability.\n",
    "9.  `plt.suptitle(...)`: Adds a title to the pairplot.\n",
    "10. `sns.heatmap(X_iris.corr(), ...)`: Generates a heatmap of the correlation matrix for the features, showing how features relate to each other.\n",
    "\n",
    "**2. Preprocessing & Train-Test Split**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Features and Target\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) # Use transform only on test data\n",
    "```\n",
    "*Line-by-line Explanation (Preprocessing & Split):*\n",
    "1.  `X = iris.data`, `y = iris.target`: Assigns the raw numpy arrays for features and target.\n",
    "2.  `X_train, X_test, y_train, y_test = train_test_split(X, y, ...)`: Splits the data: 70% for training, 30% for testing. `random_state` ensures reproducibility. `stratify=y` ensures that the class proportions are maintained in both train and test splits, which is good practice.\n",
    "3.  `scaler = StandardScaler()`: Initializes the StandardScaler.\n",
    "4.  `X_train_scaled = scaler.fit_transform(X_train)`: Fits the scaler on the training data (calculates mean and std) and then transforms it.\n",
    "5.  `X_test_scaled = scaler.transform(X_test)`: Transforms the test data using the mean and std *learned from the training data*. This prevents data leakage from the test set into the training process.\n",
    "\n",
    "**3. Model Training and Tuning (Finding Optimal K)**\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for K\n",
    "param_grid = {'n_neighbors': np.arange(1, 26, 2)} # Test odd K values from 1 to 25\n",
    "\n",
    "# Initialize KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "# cv=5 means 5-fold cross-validation\n",
    "# scoring='accuracy' means we use accuracy to evaluate K\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Fit GridSearchCV to find the best K\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best K and best score\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"\\nBest K found by GridSearchCV: {best_k}\")\n",
    "print(f\"Best cross-validated accuracy on training data: {best_score:.4f}\")\n",
    "\n",
    "# Train the final model with the best K\n",
    "final_knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "final_knn_model.fit(X_train_scaled, y_train)\n",
    "```\n",
    "*Line-by-line Explanation (Training & Tuning):*\n",
    "1.  `param_grid = {'n_neighbors': np.arange(1, 26, 2)}`: Defines the range of K values (number of neighbors) to test during grid search.\n",
    "2.  `knn = KNeighborsClassifier()`: Initializes a KNN classifier instance.\n",
    "3.  `grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', verbose=1)`: Sets up GridSearchCV. It will try each K in `param_grid`, perform 5-fold cross-validation for each, and use 'accuracy' as the scoring metric. `verbose=1` shows progress.\n",
    "4.  `grid_search.fit(X_train_scaled, y_train)`: Runs the grid search on the scaled training data.\n",
    "5.  `best_k = grid_search.best_params_['n_neighbors']`: Retrieves the best K value found.\n",
    "6.  `best_score = grid_search.best_score_`: Retrieves the mean cross-validated score of the best K.\n",
    "7.  `final_knn_model = KNeighborsClassifier(n_neighbors=best_k)`: Initializes a new KNN classifier with the optimal K.\n",
    "8.  `final_knn_model.fit(X_train_scaled, y_train)`: Trains the final KNN model on the entire scaled training set using the best K.\n",
    "\n",
    "**4. Prediction and Evaluation**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# Ensure mlxtend is installed for decision region plotting: pip install mlxtend\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # For X_combined_std, y_combined in plot_decision_regions\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "y_pred_iris = final_knn_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_test = accuracy_score(y_test, y_pred_iris)\n",
    "print(f\"\\nAccuracy on Test Set: {accuracy_test:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred_iris, target_names=iris.target_names))\n",
    "\n",
    "print(\"\\nConfusion Matrix on Test Set:\")\n",
    "cm_iris = confusion_matrix(y_test, y_pred_iris)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm_iris, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'Confusion Matrix for Iris (K={best_k})')\n",
    "plt.show()\n",
    "\n",
    "# Decision Boundary Plot (only works well for 2 features)\n",
    "# We'll use the first two features for visualization: Sepal Length and Sepal Width\n",
    "X_train_scaled_2features = X_train_scaled[:, :2]\n",
    "knn_vis_iris = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_vis_iris.fit(X_train_scaled_2features, y_train)\n",
    "\n",
    "# Plotting decision regions\n",
    "# For mlxtend, ensure X is a NumPy array\n",
    "X_combined_std = np.vstack((X_train_scaled_2features, X_test_scaled[:, :2]))\n",
    "y_combined = np.hstack((y_train, y_test)) # Not ideal to combine for actual boundary, but for visual on all points.\n",
    "# Better: plot boundary on training data, overlay test points. Let's stick to training data for boundary.\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_decision_regions(X_train_scaled_2features, y_train, clf=knn_vis_iris, legend=2)\n",
    "# Highlight test set points\n",
    "plt.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], c=y_test, marker='x', s=100, alpha=0.7, edgecolor='k', label='Test data')\n",
    "plt.xlabel(iris.feature_names[0] + ' (scaled)')\n",
    "plt.ylabel(iris.feature_names[1] + ' (scaled)')\n",
    "plt.title(f'KNN Decision Boundary for Iris (K={best_k}, First Two Features)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "```\n",
    "*Line-by-line Explanation (Prediction & Evaluation):*\n",
    "1.  `y_pred_iris = final_knn_model.predict(X_test_scaled)`: Makes predictions on the (scaled) test data.\n",
    "2.  `accuracy_test = accuracy_score(y_test, y_pred_iris)`: Calculates the accuracy on the test set.\n",
    "3.  `print(classification_report(...))`: Prints precision, recall, F1-score, and support for each class.\n",
    "4.  `cm_iris = confusion_matrix(y_test, y_pred_iris)`: Generates the confusion matrix.\n",
    "5.  `sns.heatmap(cm_iris, ...)`: Visualizes the confusion matrix.\n",
    "6.  `X_train_scaled_2features = X_train_scaled[:, :2]`: Selects only the first two features from the scaled training data for 2D visualization.\n",
    "7.  `knn_vis_iris = KNeighborsClassifier(n_neighbors=best_k)`: Creates a new KNN model for visualization.\n",
    "8.  `knn_vis_iris.fit(X_train_scaled_2features, y_train)`: Trains this KNN model on the 2 selected features of the training data.\n",
    "9.  `plot_decision_regions(X_train_scaled_2features, y_train, clf=knn_vis_iris, legend=2)`: Plots the decision boundaries using `mlxtend`. This shows how the feature space is partitioned based on the training data (first two features).\n",
    "10. `plt.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], ...)`: Overlays the test data points (first two features) on the decision boundary plot to see how well they are classified.\n",
    "11. Standard `plt` commands for labels, title, and legend.\n",
    "\n",
    "This end-to-end example demonstrates the typical workflow for applying KNN, from data exploration to model evaluation and visualization, highlighting the importance of each step.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Comparison between KNN and Other Classification Models\n",
    "\n",
    "KNN stands out due to its simplicity and instance-based nature, but it's useful to compare it with other common classification algorithms like Decision Trees and Logistic Regression to understand its relative strengths and weaknesses.\n",
    "\n",
    "**KNN vs. Decision Trees:**\n",
    "*   **Model Representation:**\n",
    "    *   KNN: Stores the entire training dataset. No explicit model is built. The \"model\" *is* the data.\n",
    "    *   Decision Tree: Builds an explicit tree-like structure of if-else rules.\n",
    "*   **Learning Type:**\n",
    "    *   KNN: Lazy learner (defers computation until prediction).\n",
    "    *   Decision Tree: Eager learner (builds model during training).\n",
    "*   **Training/Prediction Time:**\n",
    "    *   KNN: Fast training (just data storage), slow prediction (calculates distances to all training points).\n",
    "    *   Decision Tree: Slower training (finds optimal splits), fast prediction (traverses the tree).\n",
    "*   **Interpretability:**\n",
    "    *   KNN: Less interpretable, as predictions depend on local neighbors; understanding global patterns is hard.\n",
    "    *   Decision Tree: Highly interpretable due to its rule-based structure.\n",
    "*   **Feature Scaling:**\n",
    "    *   KNN: Highly sensitive to feature scaling because it's distance-based.\n",
    "    *   Decision Tree: Insensitive to feature scaling (splits are based on single feature thresholds).\n",
    "*   **Decision Boundary:**\n",
    "    *   KNN: Can form complex, non-linear decision boundaries that are locally determined.\n",
    "    *   Decision Tree: Creates axis-parallel (piecewise constant) decision boundaries.\n",
    "*   **Handling Non-linear Data:**\n",
    "    *   KNN: Naturally handles non-linear data well.\n",
    "    *   Decision Tree: Handles non-linear data by making many splits, potentially leading to complex trees.\n",
    "*   **Parameters:**\n",
    "    *   KNN: K (number of neighbors), distance metric.\n",
    "    *   Decision Tree: Max depth, min samples per split/leaf, criterion (gini/entropy).\n",
    "\n",
    "**KNN vs. Logistic Regression:**\n",
    "*   **Model Type:**\n",
    "    *   KNN: Non-parametric, instance-based.\n",
    "    *   Logistic Regression: Parametric, linear model (learns weights for features).\n",
    "*   **Assumptions:**\n",
    "    *   KNN: No assumptions about data distribution. Assumes nearby points are similar.\n",
    "    *   Logistic Regression: Assumes a linear relationship between features and the log-odds of the outcome.\n",
    "*   **Training/Prediction Time:**\n",
    "    *   KNN: Fast training, slow prediction.\n",
    "    *   Logistic Regression: Slower training (optimizes cost function), very fast prediction (dot product and sigmoid).\n",
    "*   **Interpretability:**\n",
    "    *   KNN: Low interpretability.\n",
    "    *   Logistic Regression: High interpretability; coefficients indicate feature importance and direction of effect.\n",
    "*   **Feature Scaling:**\n",
    "    *   KNN: Essential.\n",
    "    *   Logistic Regression: Recommended, especially with regularization, for faster convergence and to prevent features with larger values from dominating.\n",
    "*   **Decision Boundary:**\n",
    "    *   KNN: Complex, non-linear.\n",
    "    *   Logistic Regression: Linear decision boundary (in the original feature space, or in a transformed feature space if feature engineering is done).\n",
    "*   **Handling Non-linear Data:**\n",
    "    *   KNN: Handles non-linearity well.\n",
    "    *   Logistic Regression: Requires manual feature engineering (e.g., polynomial features) to model non-linear relationships.\n",
    "*   **Data Size:**\n",
    "    *   KNN: Can be problematic with very large datasets due to storage and prediction time.\n",
    "    *   Logistic Regression: Scales well to large datasets.\n",
    "\n",
    "**When KNN is or isn’t appropriate:**\n",
    "*   **Appropriate:**\n",
    "    *   When data has complex, non-linear decision boundaries.\n",
    "    *   For smaller datasets where prediction time is not a major constraint.\n",
    "    *   When little is known about the underlying data distribution (non-parametric nature is an advantage).\n",
    "    *   As a baseline model due to its simplicity.\n",
    "    *   When feature space is not excessively high-dimensional or dimensionality reduction is applied.\n",
    "*   **Not Appropriate (or less suitable):**\n",
    "    *   For very large datasets (high prediction cost and memory usage).\n",
    "    *   In high-dimensional spaces (curse of dimensionality).\n",
    "    *   When features have vastly different scales and are not scaled.\n",
    "    *   When interpretability of the model is a primary concern.\n",
    "    *   When prediction speed is critical for the application.\n",
    "    *   If data is very noisy, as KNN can be sensitive to noisy local instances, especially with small K.\n",
    "\n",
    "Understanding these comparisons helps in choosing the right algorithm based on the specific problem, dataset characteristics, and performance requirements.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Comprehensive Notes on K-Nearest Neighbors (KNN) Regression\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction to KNN Regression\n",
    "\n",
    "K-Nearest Neighbors (KNN) Regression is a non-parametric machine learning algorithm used for predicting continuous target variables. Similar to its classification counterpart, KNN Regression operates on the principle of proximity: it predicts the value for a new data point based on the average (or weighted average) of the target values of its 'K' nearest neighbors in the feature space. It's a **lazy learning** (or instance-based learning) algorithm because it doesn't build an explicit model during a distinct training phase. Instead, it stores the entire training dataset. The actual \"learning\" or computation occurs at prediction time when a new query point is presented. Its **non-parametric nature** means it makes no strong assumptions about the functional form of the relationship between features and the target variable (e.g., it doesn't assume a linear relationship like linear regression). This flexibility allows KNN Regression to capture complex, non-linear patterns in the data.\n",
    "\n",
    "Real-world applications of KNN Regression are quite common. In **real estate**, it can be used for **predicting house prices** by finding K similar houses (based on features like size, number of bedrooms, location) and averaging their sale prices. For **sales forecasting**, businesses can predict future sales of a product by looking at sales figures of similar products or sales during similar past periods, considering features like marketing spend, seasonality, and economic indicators. In finance, KNN Regression can be applied to **estimate stock values** or predict other financial metrics by identifying K similar historical market conditions or K similar companies. It's also used in **environmental science** for predicting pollution levels based on meteorological data from nearby stations or similar past conditions, and in **agriculture** for estimating crop yields based on soil properties, weather patterns, and characteristics of nearby farms. Its simplicity and ability to model local variations make it attractive, especially when the underlying data relationships are not well understood or are highly irregular.\n",
    "\n",
    "```python\n",
    "# Illustrative: KNN Regression is part of scikit-learn\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Imagine some training data (features and continuous target values)\n",
    "# X_train_reg = [[feature1_obj1, feature2_obj1], [feature1_obj2, feature2_obj2], ...]\n",
    "# y_train_reg = [target_value1, target_value2, ...] # Continuous values\n",
    "\n",
    "# KNN Regression doesn't \"train\" in the traditional sense, it just stores data.\n",
    "# When a new point comes, it calculates distances to all stored points,\n",
    "# finds K nearest neighbors, and averages their target values.\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `from sklearn.neighbors import KNeighborsRegressor`: Imports the KNN regressor class from scikit-learn.\n",
    "2.  `import numpy as np`: Imports NumPy, useful for numerical operations.\n",
    "3.  `# X_train_reg = ...`: Comment indicating where training features (numerical data) would be defined.\n",
    "4.  `# y_train_reg = ...`: Comment indicating where training target values (continuous numerical data) would be defined.\n",
    "5.  `# KNN Regression doesn't \"train\"...`: These comments explain the lazy learning nature and the core prediction mechanism for regression.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Underlying Mechanism: Distance Computation and Prediction\n",
    "\n",
    "The fundamental mechanism of KNN Regression for predicting the value of a new, unseen data point involves several steps, rooted in the concept of \"nearness\" in the feature space:\n",
    "\n",
    "1.  **Store Training Data:** During the \"training\" phase (which is minimal for KNN), the algorithm simply stores all the feature vectors (`X_train`) and their corresponding continuous target values (`y_train`) from the training dataset. No explicit model function is learned at this stage.\n",
    "\n",
    "2.  **Distance Computation:** When a new data point (query point `x_q`) for which a prediction is needed arrives, KNN Regression calculates the distance between `x_q` and every single data point in the stored training set (`X_train`). The choice of distance metric is crucial and the same metrics used in KNN Classification apply here:\n",
    "    *   **Euclidean Distance (L2 norm):** Most common, `sqrt(sum((x_qi - x_train_ij)^2))`.\n",
    "    *   **Manhattan Distance (L1 norm):** `sum(|x_qi - x_train_ij|)`.\n",
    "    *   **Minkowski Distance:** A generalization, `(sum(|x_qi - x_train_ij|^p))^(1/p)`.\n",
    "    Feature scaling (e.g., standardization) is vital before distance computation if features are on different scales.\n",
    "\n",
    "3.  **Identify K Nearest Neighbors:** After computing all distances, the algorithm sorts them in ascending order and identifies the 'K' training data points that have the smallest distances to the query point `x_q`. These are its K nearest neighbors. The value of K is a user-defined hyperparameter.\n",
    "\n",
    "4.  **Predict Output Value:** The prediction for `x_q` is then made by aggregating the target values of these K nearest neighbors.\n",
    "    *   **Simple Average:** The most common method is to take the arithmetic mean of the target values (`y_i`) of the K neighbors:\n",
    "        `ŷ_q = (1/K) * sum(y_i for i in K_neighbors)`\n",
    "    *   **Weighted Average:** An alternative is to use a weighted average, where closer neighbors have a greater influence on the prediction. A common weighting scheme is the inverse of the distance:\n",
    "        `weight_i = 1 / distance(x_q, x_neighbor_i)`\n",
    "        `ŷ_q = sum(weight_i * y_i for i in K_neighbors) / sum(weight_i for i in K_neighbors)`\n",
    "        This can be particularly useful if some neighbors are significantly closer than others within the K set. Scikit-learn's `KNeighborsRegressor` supports this via the `weights` parameter (`'uniform'` for simple average, `'distance'` for weighted average).\n",
    "\n",
    "This process means predictions are highly localized and can adapt to complex, non-linear functions without assuming any specific underlying model structure. The smoothness of the resulting regression function is directly influenced by K.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample 1D data for illustration\n",
    "X_train_sample = np.array([[1], [2], [3], [6], [7], [8], [11]])\n",
    "y_train_sample = np.array([2, 2.5, 3, 7, 7.5, 8, 6]) # Continuous target values\n",
    "new_point_sample = np.array([[4]])\n",
    "K_val = 3\n",
    "\n",
    "# 1. Calculate distances from new_point_sample to all X_train_sample points\n",
    "distances = np.sqrt(np.sum((X_train_sample - new_point_sample)**2, axis=1))\n",
    "print(\"Distances:\", distances)\n",
    "\n",
    "# 2. Find indices of K nearest neighbors\n",
    "k_nearest_indices = np.argsort(distances)[:K_val]\n",
    "print(\"Indices of K nearest neighbors:\", k_nearest_indices)\n",
    "k_nearest_neighbors_X = X_train_sample[k_nearest_indices]\n",
    "k_nearest_neighbors_y = y_train_sample[k_nearest_indices]\n",
    "print(\"K nearest X values:\", k_nearest_neighbors_X.flatten())\n",
    "print(\"K nearest y values:\", k_nearest_neighbors_y)\n",
    "\n",
    "# 3. Predict using simple average\n",
    "prediction_simple_avg = np.mean(k_nearest_neighbors_y)\n",
    "print(f\"Prediction (simple average) for K={K_val}: {prediction_simple_avg:.2f}\")\n",
    "\n",
    "# 4. Predict using weighted average (inverse distance)\n",
    "# Ensure no zero distances (if new_point is identical to a training point)\n",
    "weights = 1 / (distances[k_nearest_indices] + 1e-9) # Add small epsilon to avoid division by zero\n",
    "prediction_weighted_avg = np.sum(weights * k_nearest_neighbors_y) / np.sum(weights)\n",
    "print(f\"Prediction (weighted average) for K={K_val}: {prediction_weighted_avg:.2f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train_sample, y_train_sample, color='blue', s=100, label='Training Data')\n",
    "plt.scatter(new_point_sample[0,0], prediction_simple_avg, color='red', marker='x', s=150, label=f'New Point Prediction (K={K_val}, Simple Avg)')\n",
    "# Highlight neighbors\n",
    "plt.scatter(k_nearest_neighbors_X, k_nearest_neighbors_y, color='green', s=150, facecolors='none', edgecolors='green', label='K Nearest Neighbors')\n",
    "for i in range(K_val):\n",
    "    plt.plot([new_point_sample[0,0], k_nearest_neighbors_X[i,0]], [prediction_simple_avg, k_nearest_neighbors_y[i]], 'k--', alpha=0.3)\n",
    "plt.title('KNN Regression Mechanism (1D Example)')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `X_train_sample`, `y_train_sample`: Defines sample 1D feature data and corresponding continuous target values.\n",
    "2.  `new_point_sample`: Defines a new point for which we want to predict the target value.\n",
    "3.  `K_val = 3`: Sets the number of neighbors to consider.\n",
    "4.  `distances = np.sqrt(...)`: Calculates Euclidean distances from `new_point_sample` to all points in `X_train_sample`.\n",
    "5.  `k_nearest_indices = np.argsort(distances)[:K_val]`: Sorts distances and gets the indices of the `K_val` smallest distances.\n",
    "6.  `k_nearest_neighbors_X`, `k_nearest_neighbors_y`: Retrieves the feature values and target values of the K nearest neighbors.\n",
    "7.  `prediction_simple_avg = np.mean(k_nearest_neighbors_y)`: Calculates the prediction as the simple average of the target values of the K nearest neighbors.\n",
    "8.  `weights = 1 / (distances[k_nearest_indices] + 1e-9)`: Calculates weights as the inverse of distances to the K nearest neighbors (small epsilon added to prevent division by zero if a distance is exactly 0).\n",
    "9.  `prediction_weighted_avg = np.sum(...) / np.sum(...)`: Calculates the prediction using a weighted average.\n",
    "10. `plt.figure(...)`, `plt.scatter(...)`, `plt.plot(...)`: Matplotlib commands to visualize the training data, the new point's prediction, and its K nearest neighbors, along with lines connecting the new point to its neighbors.\n",
    "11. `plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`, `plt.legend()`, `plt.grid(True)`, `plt.show()`: Standard plot formatting.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Distance Metrics in KNN Regression\n",
    "\n",
    "The choice of distance metric in KNN Regression is as crucial as in KNN Classification, as it defines what \"near\" means in the feature space. The predicted value for a new point is directly derived from the target values of its neighbors, and who these neighbors are is determined by the distance metric. The most commonly used distance metrics remain the same:\n",
    "\n",
    "1.  **Euclidean Distance (L2 Norm):**\n",
    "    This is the straight-line or \"as-the-crow-flies\" distance between two points `p` and `q` in an N-dimensional feature space:\n",
    "    `d(p, q) = sqrt(sum for i=1 to N of (pi - qi)^2)`\n",
    "    It's the default choice in many implementations, including scikit-learn's `KNeighborsRegressor`. Euclidean distance assumes that the features are commensurate and that the overall geometric distance is meaningful. It squares the differences, so larger differences in any single dimension have a more significant impact on the total distance. This metric is appropriate when the data space is relatively homogeneous and features have been scaled.\n",
    "\n",
    "2.  **Manhattan Distance (L1 Norm or City Block Distance):**\n",
    "    This metric calculates distance as the sum of the absolute differences between the coordinates of the points:\n",
    "    `d(p, q) = sum for i=1 to N of |pi - qi|`\n",
    "    It's called Manhattan distance because it's akin to navigating a grid-like city, where movement is restricted to orthogonal paths. Manhattan distance can be more robust to outliers in specific dimensions compared to Euclidean distance because it doesn't square the differences. It might be preferred in high-dimensional spaces or when individual feature differences are more directly interpretable as contributions to \"dissimilarity.\"\n",
    "\n",
    "3.  **Minkowski Distance:**\n",
    "    This is a generalized metric that encompasses both Euclidean and Manhattan distances:\n",
    "    `d(p, q) = (sum for i=1 to N of |pi - qi|^p_minkowski)^(1/p_minkowski)`\n",
    "    When `p_minkowski = 1`, it becomes Manhattan distance. When `p_minkowski = 2`, it becomes Euclidean distance. The parameter `p_minkowski` (often just `p` in formulas, but distinguished here to avoid confusion with point `p`) can be tuned. Higher values of `p_minkowski` place more emphasis on larger differences in any single dimension.\n",
    "\n",
    "The selection of a distance metric should ideally be guided by domain knowledge or empirical evaluation (e.g., cross-validation). Importantly, regardless of the metric, **feature scaling** (like standardization or normalization) is almost always necessary before applying KNN. Without scaling, features with larger numerical ranges would dominate the distance calculation, leading to suboptimal neighbor selection and biased regression estimates. For example, if one feature is 'age' (20-80) and another is 'income' (20000-200000), income differences would overwhelm age differences in Euclidean distance calculations if not scaled.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic dataset for regression\n",
    "np.random.seed(42)\n",
    "X_reg_dist = np.random.rand(100, 2) * 10 # 100 samples, 2 features\n",
    "# Make one feature have a larger scale\n",
    "X_reg_dist[:, 1] = X_reg_dist[:, 1] * 100\n",
    "y_reg_dist = X_reg_dist[:, 0] * 2 - X_reg_dist[:, 1] * 0.5 + np.random.randn(100) * 5 # Target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reg_dist, y_reg_dist, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a pipeline for scaling and KNN regression\n",
    "# We will use GridSearchCV to try different distance metrics ('p' parameter in Minkowski)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Essential step!\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=5)) # Start with K=5\n",
    "])\n",
    "\n",
    "# Define parameters for GridSearchCV, including the distance metric\n",
    "# p=1 for Manhattan, p=2 for Euclidean\n",
    "param_grid_dist = {\n",
    "    'knn__n_neighbors': [3, 5, 7],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2] # 1 for Manhattan (L1), 2 for Euclidean (L2)\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_dist = GridSearchCV(pipeline, param_grid_dist, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search_dist.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters found by GridSearchCV:\")\n",
    "print(grid_search_dist.best_params_)\n",
    "print(f\"Best CV score (Negative MSE): {grid_search_dist.best_score_:.4f}\")\n",
    "\n",
    "# The best_params_ will show which 'p' (distance metric) performed better\n",
    "# for this specific dataset and K value range.\n",
    "# For example, 'knn__p': 1 would mean Manhattan distance was better.\n",
    "# 'knn__p': 2 would mean Euclidean distance was better.\n",
    "\n",
    "# Example of how scikit-learn implements this:\n",
    "# KNeighborsRegressor(n_neighbors=5, p=1) # Manhattan\n",
    "# KNeighborsRegressor(n_neighbors=5, p=2) # Euclidean (default)\n",
    "# These are for the 'minkowski' metric. Other metrics can be passed via `metric` argument.\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `np.random.seed(42)`, `X_reg_dist = ...`, `y_reg_dist = ...`: Creates a synthetic 2D dataset where one feature has a much larger scale than the other, and a target variable.\n",
    "2.  `X_train, X_test, y_train, y_test = train_test_split(...)`: Splits data into training and testing sets.\n",
    "3.  `pipeline = Pipeline(...)`: Creates a scikit-learn pipeline that first scales the data using `StandardScaler` and then applies `KNeighborsRegressor`.\n",
    "4.  `param_grid_dist = {...}`: Defines a parameter grid for `GridSearchCV`. It will test different numbers of neighbors (`n_neighbors`), weighting schemes (`weights`), and distance metrics (`p`: 1 for Manhattan, 2 for Euclidean, which are special cases of Minkowski distance used by `KNeighborsRegressor`).\n",
    "5.  `grid_search_dist = GridSearchCV(...)`: Initializes GridSearchCV to search for the best combination of these parameters using 3-fold cross-validation and 'neg_mean_squared_error' as the scoring metric (higher is better, so MSE is negated).\n",
    "6.  `grid_search_dist.fit(X_train, y_train)`: Runs the grid search on the training data. The pipeline ensures scaling is done correctly within each CV fold.\n",
    "7.  `print(...)`: Prints the best parameters found (including `knn__p` which indicates the best distance metric parameter) and the corresponding best cross-validation score.\n",
    "8.  `# KNeighborsRegressor(n_neighbors=5, p=1)`: Comment showing how to explicitly set Manhattan distance.\n",
    "9.  `# KNeighborsRegressor(n_neighbors=5, p=2)`: Comment showing how to explicitly set Euclidean distance (which is the default).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Effects of Choosing K: Bias-Variance Tradeoff and Hyperparameter Tuning\n",
    "\n",
    "The choice of 'K', the number of neighbors to consider, is a critical hyperparameter in KNN Regression, directly influencing the model's complexity and its position on the bias-variance spectrum.\n",
    "\n",
    "*   **Low K (e.g., K=1):**\n",
    "    *   **Low Bias:** The model is highly flexible and can capture very local variations in the data. The prediction for a new point will be the exact target value of its single closest neighbor. This allows the regression function to fit the training data very closely, potentially capturing intricate patterns.\n",
    "    *   **High Variance:** The model is very sensitive to noise and outliers in the training data. A slight change in a single training point can significantly alter predictions for nearby points. This leads to overfitting, where the model performs well on training data but poorly on unseen test data. The resulting regression curve will be very jagged and unstable.\n",
    "\n",
    "*   **High K (e.g., K approaching N, where N is the total number of training points):**\n",
    "    *   **High Bias:** The model becomes overly simplistic and smooth. As K increases, the prediction for any new point tends towards the global average of all target values in the training set. This washes out local details and patterns. The model might fail to capture the true underlying relationship if it's complex.\n",
    "    *   **Low Variance:** The model is stable and less affected by individual data points or noise. Predictions change little with small variations in the training set. However, this stability comes at the cost of underfitting, where the model is too simple to learn the data's structure.\n",
    "\n",
    "The **bias-variance tradeoff** means finding an optimal K that balances these extremes. The goal is a K that allows the model to be flexible enough to capture the true underlying patterns (low bias) while remaining robust to noise in the training data (low variance), thus generalizing well to new data.\n",
    "\n",
    "**Hyperparameter Tuning (Finding Optimal K):**\n",
    "The optimal K is usually determined empirically using techniques like **cross-validation**.\n",
    "1.  **Grid Search with Cross-Validation:**\n",
    "    *   Define a range of K values to test (e.g., 1, 3, 5, ..., up to a fraction of N).\n",
    "    *   For each K:\n",
    "        *   Perform k-fold cross-validation (e.g., 5-fold or 10-fold) on the training data.\n",
    "        *   In each fold, train the KNN Regressor with the current K on the training part and evaluate it on the validation part using a regression metric (e.g., Mean Squared Error (MSE), R-squared).\n",
    "        *   Average the metric scores across all folds for the current K.\n",
    "    *   Select the K that yields the best average performance (e.g., lowest MSE or highest R-squared).\n",
    "    It's common to try odd values for K, although this is more critical in classification for tie-breaking. For regression, any integer K is valid. Scikit-learn's `GridSearchCV` automates this process efficiently.\n",
    "\n",
    "Visualizing the performance metric (e.g., RMSE) against different K values can often show a U-shaped curve, where the bottom of the 'U' indicates the optimal K range.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X_k, y_k = make_regression(n_samples=200, n_features=1, noise=20, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_k = StandardScaler()\n",
    "X_k_scaled = scaler_k.fit_transform(X_k)\n",
    "\n",
    "# Split data (optional for CV demo on full data, but good practice)\n",
    "# X_train_k, X_test_k, y_train_k, y_test_k = train_test_split(X_k_scaled, y_k, test_size=0.3, random_state=42)\n",
    "\n",
    "# Method 1: Manual Cross-Validation Loop for K\n",
    "k_values_reg = range(1, 41) # Test K values from 1 to 40\n",
    "cv_mse_scores = []\n",
    "cv_rmse_scores = []\n",
    "\n",
    "for k_val_reg in k_values_reg:\n",
    "    knn_reg_tune = KNeighborsRegressor(n_neighbors=k_val_reg)\n",
    "    # Use negative MSE because cross_val_score maximizes a score\n",
    "    mse_fold_scores = cross_val_score(knn_reg_tune, X_k_scaled, y_k, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_mse_scores.append(-mse_fold_scores.mean()) # Convert back to positive MSE\n",
    "    cv_rmse_scores.append(np.sqrt(-mse_fold_scores.mean())) # RMSE\n",
    "    # print(f\"K={k_val_reg}, Mean CV MSE: {-mse_fold_scores.mean():.4f}\")\n",
    "\n",
    "# Find optimal K based on lowest RMSE\n",
    "optimal_k_cv = k_values_reg[np.argmin(cv_rmse_scores)]\n",
    "print(f\"\\nOptimal K (manual CV via lowest RMSE) = {optimal_k_cv} with RMSE {min(cv_rmse_scores):.4f}\")\n",
    "\n",
    "# Plot K vs. RMSE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values_reg, cv_rmse_scores, marker='o', linestyle='-', color='g')\n",
    "plt.title('K Value vs. Cross-Validated RMSE (KNN Regression)')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.ylabel('Mean Root Mean Squared Error (RMSE)')\n",
    "plt.xticks(np.arange(min(k_values_reg), max(k_values_reg)+1, 2.0))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Method 2: Using GridSearchCV\n",
    "param_grid_reg = {'n_neighbors': range(1, 41)}\n",
    "knn_reg_grid = KNeighborsRegressor()\n",
    "# Use 'neg_root_mean_squared_error' if available, or 'neg_mean_squared_error'\n",
    "grid_search_reg = GridSearchCV(knn_reg_grid, param_grid_reg, cv=5, scoring='neg_root_mean_squared_error', verbose=0)\n",
    "grid_search_reg.fit(X_k_scaled, y_k)\n",
    "\n",
    "print(f\"\\nBest K (GridSearchCV): {grid_search_reg.best_params_['n_neighbors']}\")\n",
    "print(f\"Best CV Score (Negative RMSE from GridSearchCV): {grid_search_reg.best_score_:.4f}\")\n",
    "print(f\"Best CV RMSE (from GridSearchCV): {-grid_search_reg.best_score_:.4f}\")\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `X_k, y_k = make_regression(...)`: Generates synthetic 1D regression data.\n",
    "2.  `scaler_k = StandardScaler()`, `X_k_scaled = ...`: Scales the features.\n",
    "3.  `k_values_reg = range(1, 41)`: Defines a range of K values to test.\n",
    "4.  `cv_mse_scores = []`, `cv_rmse_scores = []`: Initializes lists to store MSE and RMSE from cross-validation.\n",
    "5.  `for k_val_reg in k_values_reg:`: Loop through each K value.\n",
    "6.  `knn_reg_tune = KNeighborsRegressor(n_neighbors=k_val_reg)`: Initializes KNN Regressor with current K.\n",
    "7.  `mse_fold_scores = cross_val_score(...)`: Performs 5-fold cross-validation using `neg_mean_squared_error` as scoring. `cross_val_score` expects scorers that are maximized (higher is better), so error metrics are negated.\n",
    "8.  `cv_mse_scores.append(-mse_fold_scores.mean())`: Stores the mean positive MSE.\n",
    "9.  `cv_rmse_scores.append(np.sqrt(-mse_fold_scores.mean()))`: Stores the mean RMSE.\n",
    "10. `optimal_k_cv = k_values_reg[np.argmin(cv_rmse_scores)]`: Finds the K that resulted in the minimum RMSE.\n",
    "11. `print(...)`: Prints the optimal K and its RMSE from the manual loop.\n",
    "12. `plt.figure(...)`, `plt.plot(...)`, `plt.title(...)`, etc.: Plots K vs. RMSE to visualize the tradeoff.\n",
    "13. `param_grid_reg = {'n_neighbors': range(1, 41)}`: Defines parameter grid for `GridSearchCV`.\n",
    "14. `knn_reg_grid = KNeighborsRegressor()`: Initializes KNN Regressor for `GridSearchCV`.\n",
    "15. `grid_search_reg = GridSearchCV(...)`: Initializes `GridSearchCV` with `neg_root_mean_squared_error` (if your scikit-learn version supports it, otherwise `neg_mean_squared_error`).\n",
    "16. `grid_search_reg.fit(X_k_scaled, y_k)`: Runs the grid search.\n",
    "17. `print(...)`: Prints the best K and best RMSE found by `GridSearchCV`. Note the conversion of `grid_search_reg.best_score_` back to positive RMSE.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Data Preprocessing for KNN Regression\n",
    "\n",
    "Data preprocessing is exceptionally important for KNN Regression, just as it is for KNN Classification. Since the algorithm relies heavily on distance calculations between data points, the quality and format of the input data directly impact its performance. Key steps include:\n",
    "\n",
    "1.  **Feature Scaling:** This is arguably the most critical preprocessing step for KNN. If features are measured on different scales, features with larger magnitudes will disproportionately influence the distance metric, leading to biased neighbor selection.\n",
    "    *   **Standardization (Z-score normalization):** Transforms data to have zero mean and unit variance (`X_scaled = (X - mean(X)) / std(X)`). Generally preferred as it doesn't compress data into a fixed range, preserving information about outliers.\n",
    "    *   **Normalization (Min-Max scaling):** Rescales data to a specific range, typically [0, 1] (`X_scaled = (X - min(X)) / (max(X) - min(X))`). Can be useful if features are known to be bounded or if subsequent algorithms require data in this range.\n",
    "    Applying scaling ensures all features contribute more equally to the distance computations.\n",
    "\n",
    "2.  **Handling Missing Values:** KNN Regression cannot inherently handle missing data points because distance calculations require complete numerical vectors.\n",
    "    *   **Imputation:** Replacing missing values. For numerical features (which are typical in regression inputs), common methods include:\n",
    "        *   **Mean/Median Imputation:** Replace NaNs with the mean or median of the respective feature column. Median is more robust to outliers.\n",
    "        *   **KNN Imputation:** Use the KNN algorithm itself to predict missing values based on the values of `k` nearest neighbors (that don't have missing values for that feature). This can be more accurate but is computationally more intensive.\n",
    "    *   **Deletion:** Removing rows with missing values (if the number of such rows is small) or entire features (if a feature has too many missing values and is deemed less critical). This often leads to data loss.\n",
    "\n",
    "3.  **Encoding Categorical Data (if applicable for features):** While the target variable in regression is continuous, input features can sometimes be categorical. Distance metrics used in KNN are defined for numerical spaces.\n",
    "    *   **One-Hot Encoding:** Converts a categorical feature with `C` categories into `C` binary (0/1) features. Suitable for nominal features (no inherent order). This increases dimensionality.\n",
    "    *   **Label Encoding:** Assigns a unique integer to each category. This implies an ordinal relationship, which might mislead the KNN if the categories are purely nominal. Best for ordinal features (e.g., 'low', 'medium', 'high').\n",
    "    *   **Dummy Coding:** Similar to one-hot encoding but drops one of the new binary columns to avoid multicollinearity if subsequent linear models are used (less of a direct concern for KNN's distance calculation itself, but good practice).\n",
    "    After encoding, the new binary features are on a [0,1] scale. If mixed with continuous features, those continuous features still need scaling.\n",
    "\n",
    "Effective preprocessing ensures that the distance metric accurately reflects the similarity between data points, leading to more reliable neighbor selection and, consequently, more accurate regression predictions.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame with mixed data, missing values for features\n",
    "data_reg_prep = {\n",
    "    'size_sqft': [1500, 2000, np.nan, 1200, 2500],\n",
    "    'num_bedrooms': [3, 4, 3, np.nan, 5],\n",
    "    'age_of_property': [5, 10, 2, 15, 1],\n",
    "    'location_type': ['Urban', 'Suburban', 'Urban', 'Rural', 'Suburban'], # Categorical feature\n",
    "    'target_price': [300000, 450000, 280000, 200000, 550000] # Target\n",
    "}\n",
    "df_reg_prep = pd.DataFrame(data_reg_prep)\n",
    "X_reg_prep = df_reg_prep.drop('target_price', axis=1)\n",
    "y_reg_prep = df_reg_prep['target_price']\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features_prep = ['size_sqft', 'num_bedrooms', 'age_of_property']\n",
    "categorical_features_prep = ['location_type']\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numerical_pipeline_prep = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')), # 1. Impute missing numerical with median\n",
    "    ('scaler', StandardScaler())                  # 2. Scale numerical features\n",
    "])\n",
    "\n",
    "categorical_pipeline_prep = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # 1. Impute missing categorical with mode\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # 2. One-hot encode\n",
    "]) # sparse_output=False for dense array output, easier to inspect\n",
    "\n",
    "# Create a column transformer\n",
    "preprocessor_prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline_prep, numerical_features_prep),\n",
    "        ('cat', categorical_pipeline_prep, categorical_features_prep)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Apply the preprocessing\n",
    "X_processed_prep = preprocessor_prep.fit_transform(X_reg_prep)\n",
    "# Get feature names after one-hot encoding for clarity\n",
    "feature_names_out = preprocessor_prep.get_feature_names_out()\n",
    "\n",
    "X_processed_df = pd.DataFrame(X_processed_prep, columns=feature_names_out)\n",
    "\n",
    "print(\"Original X features:\")\n",
    "print(X_reg_prep)\n",
    "print(\"\\nProcessed X features (DataFrame):\")\n",
    "print(X_processed_df)\n",
    "print(\"\\nShape of processed X:\", X_processed_df.shape)\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `data_reg_prep = {...}`: Defines sample raw data for a regression problem, including numerical features with NaNs, a categorical feature, and a continuous target.\n",
    "2.  `df_reg_prep = pd.DataFrame(data_reg_prep)`: Creates a Pandas DataFrame.\n",
    "3.  `X_reg_prep = df_reg_prep.drop('target_price', axis=1)`, `y_reg_prep = ...`: Separates features and target.\n",
    "4.  `numerical_features_prep`, `categorical_features_prep`: Lists numerical and categorical column names.\n",
    "5.  `numerical_pipeline_prep = Pipeline(...)`: Defines a pipeline for numerical features: median imputation followed by standardization.\n",
    "6.  `categorical_pipeline_prep = Pipeline(...)`: Defines a pipeline for categorical features: mode imputation followed by one-hot encoding. `sparse_output=False` makes the output a dense NumPy array instead of a sparse matrix, which can be easier to inspect for small datasets. `handle_unknown='ignore'` will create all-zero columns for unknown categories encountered during `transform`.\n",
    "7.  `preprocessor_prep = ColumnTransformer(...)`: Initializes a ColumnTransformer to apply these pipelines to the respective columns.\n",
    "8.  `X_processed_prep = preprocessor_prep.fit_transform(X_reg_prep)`: Fits the preprocessor on `X_reg_prep` and transforms it. This learns imputation values, scaling parameters, and categories, then applies transformations.\n",
    "9.  `feature_names_out = preprocessor_prep.get_feature_names_out()`: Retrieves the names of the features after transformation (e.g., one-hot encoding creates new column names).\n",
    "10. `X_processed_df = pd.DataFrame(...)`: Converts the processed NumPy array back to a Pandas DataFrame with meaningful column names for easier inspection.\n",
    "11. `print(...)`: Displays the original features, the processed features in DataFrame format, and the shape of the processed data to show the effect of preprocessing (e.g., new columns from one-hot encoding).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Model Evaluation Metrics for KNN Regression\n",
    "\n",
    "Evaluating the performance of a KNN Regression model involves using metrics that quantify the difference between the predicted continuous values and the actual continuous values. Common regression metrics include:\n",
    "\n",
    "1.  **Mean Absolute Error (MAE):**\n",
    "    Calculates the average of the absolute differences between predicted and actual values.\n",
    "    `MAE = (1/n) * sum(|y_actual_i - y_predicted_i|)`\n",
    "    MAE is easy to interpret as it's in the same units as the target variable. It gives an average magnitude of errors without considering their direction. It's less sensitive to large individual errors (outliers) compared to MSE.\n",
    "\n",
    "2.  **Mean Squared Error (MSE):**\n",
    "    Calculates the average of the squared differences between predicted and actual values.\n",
    "    `MSE = (1/n) * sum((y_actual_i - y_predicted_i)^2)`\n",
    "    MSE penalizes larger errors more heavily than smaller ones due to the squaring term. This makes it sensitive to outliers. The units are the square of the target variable's units, making it less directly interpretable than MAE or RMSE. It's widely used because of its mathematical properties (e.g., differentiability, connection to variance).\n",
    "\n",
    "3.  **Root Mean Squared Error (RMSE):**\n",
    "    The square root of the MSE.\n",
    "    `RMSE = sqrt(MSE) = sqrt((1/n) * sum((y_actual_i - y_predicted_i)^2))`\n",
    "    RMSE is also in the same units as the target variable, making it more interpretable than MSE. Like MSE, it penalizes large errors more significantly. It's one of the most popular metrics for regression tasks. A lower RMSE indicates a better fit.\n",
    "\n",
    "4.  **R-squared (R² or Coefficient of Determination):**\n",
    "    Represents the proportion of the variance in the dependent variable (target) that is predictable from the independent variables (features).\n",
    "    `R² = 1 - (Sum of Squared Residuals (SSR) / Total Sum of Squares (SST))`\n",
    "    `SSR = sum((y_actual_i - y_predicted_i)^2)`\n",
    "    `SST = sum((y_actual_i - y_mean_actual)^2)`\n",
    "    R² ranges from -∞ to 1.\n",
    "    *   An R² of 1 indicates that the model perfectly predicts the target variable.\n",
    "    *   An R² of 0 indicates that the model performs no better than a baseline model that always predicts the mean of the target variable.\n",
    "    *   A negative R² means the model performs worse than this baseline model.\n",
    "    While widely used, R² can be misleading as it tends to increase with more features, even if they are not useful. Adjusted R² (which penalizes for adding irrelevant features) can be a better alternative in such cases, though not always directly provided as a default scorer in scikit-learn's model selection tools for simple models.\n",
    "\n",
    "These metrics should always be computed on a held-out test set (or through cross-validation results) to get an unbiased estimate of the model's generalization performance. Visualizations like scatter plots of actual vs. predicted values or residual plots (predicted vs. errors) also provide valuable insights into model performance.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X_eval, y_eval = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(X_eval, y_eval, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess: Scale features\n",
    "scaler_eval = StandardScaler()\n",
    "X_train_scaled_eval = scaler_eval.fit_transform(X_train_eval)\n",
    "X_test_scaled_eval = scaler_eval.transform(X_test_eval)\n",
    "\n",
    "# Train KNN Regressor (assuming K=5 is found to be optimal)\n",
    "knn_reg_eval = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_reg_eval.fit(X_train_scaled_eval, y_train_eval)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_eval = knn_reg_eval.predict(X_test_scaled_eval)\n",
    "\n",
    "# 1. Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test_eval, y_pred_eval)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "# 2. Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test_eval, y_pred_eval)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "# 3. Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse) # Or use mean_squared_error(y_test_eval, y_pred_eval, squared=False) in newer sklearn\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "# 4. R-squared (R²)\n",
    "r2 = r2_score(y_test_eval, y_pred_eval)\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "# Visualizing predictions vs actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_eval, y_pred_eval, alpha=0.7, edgecolors='k')\n",
    "plt.plot([min(y_test_eval), max(y_test_eval)], [min(y_test_eval), max(y_test_eval)], '--', color='red', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values (KNN Regression)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualizing residuals\n",
    "residuals = y_test_eval - y_pred_eval\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_pred_eval, residuals, alpha=0.7, edgecolors='k')\n",
    "plt.axhline(y=0, color='red', linestyle='--', lw=2)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.title(\"Residual Plot (KNN Regression)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "*Line-by-line Explanation:*\n",
    "1.  `X_eval, y_eval = make_regression(...)`: Generates synthetic regression data.\n",
    "2.  `X_train_eval, ... = train_test_split(...)`: Splits data.\n",
    "3.  `scaler_eval = StandardScaler()`, `X_train_scaled_eval = ...`, `X_test_scaled_eval = ...`: Scales features.\n",
    "4.  `knn_reg_eval = KNeighborsRegressor(n_neighbors=5)`: Initializes KNN Regressor (K=5 assumed optimal).\n",
    "5.  `knn_reg_eval.fit(...)`: Trains the model.\n",
    "6.  `y_pred_eval = knn_reg_eval.predict(...)`: Makes predictions on the test set.\n",
    "7.  `mae = mean_absolute_error(...)`: Calculates MAE.\n",
    "8.  `mse = mean_squared_error(...)`: Calculates MSE.\n",
    "9.  `rmse = np.sqrt(mse)`: Calculates RMSE. (Note: `mean_squared_error` has a `squared=False` argument in newer scikit-learn versions to directly get RMSE).\n",
    "10. `r2 = r2_score(...)`: Calculates R-squared.\n",
    "11. `print(...)`: Prints all calculated metrics.\n",
    "12. `plt.scatter(y_test_eval, y_pred_eval, ...)`: Creates a scatter plot of actual vs. predicted values. Ideally, points should lie close to the diagonal line.\n",
    "13. `plt.plot(...)`: Adds the diagonal line representing perfect predictions.\n",
    "14. `residuals = y_test_eval - y_pred_eval`: Calculates residuals (errors).\n",
    "15. `plt.scatter(y_pred_eval, residuals, ...)`: Creates a residual plot (predicted values vs. residuals). Ideally, residuals should be randomly scattered around zero with no clear patterns.\n",
    "16. `plt.axhline(y=0, ...)`: Adds a horizontal line at y=0 in the residual plot.\n",
    "17. Standard `plt` commands for labels, titles, and legends.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Strengths and Limitations of KNN Regression\n",
    "\n",
    "KNN Regression, despite its simplicity, has distinct strengths and limitations that make it suitable for certain scenarios and less so for others.\n",
    "\n",
    "**Strengths:**\n",
    "1.  **Simplicity and Intuitiveness:** The core concept of averaging neighbors' values is easy to understand and implement.\n",
    "2.  **Non-Parametric Nature:** It makes no assumptions about the underlying data distribution or the functional form of the relationship between features and the target. This allows it to capture complex, non-linear relationships that parametric models like linear regression might miss.\n",
    "3.  **Adaptability:** Predictions are made locally. The model can adapt to local structures in the data without being constrained by a global model fit.\n",
    "4.  **Easy to Implement:** Many libraries like scikit-learn provide straightforward implementations.\n",
    "5.  **No Explicit Training Phase:** Being a lazy learner, it doesn't require a separate training phase to build a model, which can be an advantage if data is constantly being updated (though prediction becomes slower). The \"training\" is just storing the data.\n",
    "6.  **Versatile Distance Metrics:** Can use various distance metrics, allowing some flexibility in defining \"similarity\" based on the problem domain.\n",
    "\n",
    "**Limitations:**\n",
    "1.  **Computational Cost at Prediction Time:** To make a prediction for a new point, KNN must compute distances to all training points. This can be very slow for large datasets (N samples) and/or high-dimensional data (D features), with a complexity of roughly O(N*D) per prediction.\n",
    "2.  **Curse of Dimensionality:** Performance degrades significantly as the number of features increases. In high-dimensional spaces, the concept of \"nearest\" neighbor becomes less meaningful as points tend to be sparsely distributed and almost equidistant from each other. Distances also concentrate.\n",
    "3.  **Sensitivity to Feature Scaling:** Features with larger scales can dominate distance calculations, leading to biased neighbor selection. Proper feature scaling (e.g., standardization) is crucial.\n",
    "4.  **Sensitivity to Irrelevant or Redundant Features:** Irrelevant features can mislead the distance calculation by adding noise, making points that are truly similar in relevant dimensions appear distant. Redundant features can over-weigh certain aspects of similarity.\n",
    "5.  **Need for Optimal K Selection:** The performance is highly dependent on the choice of K. An inappropriate K can lead to overfitting (small K) or underfitting (large K). K needs to be tuned, typically via cross-validation.\n",
    "6.  **Storage Requirements:** Requires storing the entire training dataset in memory, which can be an issue for very large datasets.\n",
    "7.  **Handling Outliers:** Predictions are based on averages of neighbors. If these neighbors include outliers (in their target values), the prediction can be skewed. Using `weights='distance'` can mitigate this to some extent, as can robust preprocessing.\n",
    "8.  **Predictions Bounded by Training Data Range:** KNN Regression predicts by averaging existing target values. Thus, it cannot extrapolate and predict values outside the range of target values observed in the training set.\n",
    "\n",
    "**Mitigating Limitations:**\n",
    "*   **Curse of Dimensionality / Irrelevant Features:** Use dimensionality reduction techniques (e.g., PCA, feature selection) to reduce the number of features to only the most informative ones.\n",
    "*   **Computational Cost:** Employ approximate nearest neighbor search algorithms (e.g., KD-Trees, Ball Trees) for faster neighbor retrieval, although their effectiveness also diminishes in very high dimensions. Data reduction techniques (e.g., selecting prototypes) can also help.\n",
    "*   **Feature Scaling:** Always apply appropriate scaling methods.\n",
    "\n",
    "Understanding these trade-offs is essential for deciding if KNN Regression is the right choice for a given problem and for taking steps to optimize its performance.\n",
    "\n",
    "```python\n",
    "# Illustrating the curse of dimensionality with distances\n",
    "# (Conceptual code, not directly mitigating it with PCA here but showing the problem)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dims = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "avg_distances = []\n",
    "min_distances = []\n",
    "max_distances = []\n",
    "num_points = 100 # Number of random points\n",
    "\n",
    "for d in dims:\n",
    "    points = np.random.rand(num_points, d) # Generate 100 random points in d-dimensional unit hypercube\n",
    "    dist_matrix = np.zeros((num_points, num_points))\n",
    "    for i in range(num_points):\n",
    "        for j in range(i + 1, num_points):\n",
    "            dist = np.linalg.norm(points[i] - points[j]) # Euclidean distance\n",
    "            dist_matrix[i, j] = dist\n",
    "            dist_matrix[j, i] = dist\n",
    "\n",
    "    # Get all unique pairwise distances (excluding self-distances)\n",
    "    pairwise_distances = dist_matrix[np.triu_indices(num_points, k=1)]\n",
    "\n",
    "    if len(pairwise_distances) > 0:\n",
    "        avg_distances.append(np.mean(pairwise_distances))\n",
    "        min_distances.append(np.min(pairwise_distances))\n",
    "        max_distances.append(np.max(pairwise_distances))\n",
    "    else: # Should not happen if num_points > 1\n",
    "        avg_distances.append(0)\n",
    "        min_distances.append(0)\n",
    "        max_distances.append(0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dims, avg_distances, marker='o', label='Average Pairwise Distance')\n",
    "plt.plot(dims, min_distances, marker='x', linestyle='--', label='Min Pairwise Distance')\n",
    "plt.plot(dims, max_distances, marker='s', linestyle=':', label='Max Pairwise Distance')\n",
    "plt.xlabel('Number of Dimensions')\n",
    "plt.ylabel('Pairwise Distances')\n",
    "plt.title('Effect of Dimensionality on Pairwise Distances in Unit Hypercube')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xscale('log') # Use log scale for dimensions if they vary widely\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: As dimensionality increases, the ratio of (max_dist - min_dist) / avg_dist tends to decrease,\")\n",
    "print(\"meaning distances become more similar, making 'nearest' less distinct.\")\n",
    "# Ratio (max-min)/avg\n",
    "ratios = [(max_d - min_d) / avg_d if avg_d > 0 else 0 for avg_d, min_d, max_d in zip(avg_distances, min_distances, max_distances)]\n",
    "print(\"Relative spread of distances ((max-min)/avg):\")\n",
    "for d, r in zip(dims, ratios):\n",
    "    print(f\"Dim: {d}, Ratio: {r:.3f}\")\n",
    "```\n",
    "*Line-by-line Explanation (Curse of Dimensionality Illustration):*\n",
    "1.  `dims`: A list of dimensions to test.\n",
    "2.  `avg_distances, min_distances, max_distances`: Lists to store statistics of pairwise distances.\n",
    "3.  `num_points = 100`: Number of random points to generate in each dimension.\n",
    "4.  `for d in dims:`: Loop through each dimension.\n",
    "5.  `points = np.random.rand(num_points, d)`: Generates `num_points` random points in a `d`-dimensional unit hypercube (coordinates between 0 and 1).\n",
    "6.  `dist_matrix = np.zeros(...)`: Initializes a matrix to store pairwise distances.\n",
    "7.  Nested loops to calculate Euclidean distance between all unique pairs of points.\n",
    "8.  `pairwise_distances = dist_matrix[np.triu_indices(num_points, k=1)]`: Extracts the unique pairwise distances from the upper triangle of the distance matrix.\n",
    "9.  `avg_distances.append(...)`, etc.: Calculates and stores mean, min, and max of these distances.\n",
    "10. `plt.figure(...)`, `plt.plot(...)`: Plots these distance statistics against the number of dimensions.\n",
    "11. `plt.xscale('log')`: Uses a logarithmic scale for the x-axis (dimensions) if the range is large, making trends clearer.\n",
    "12. `print(...)`: Prints a note explaining the implication: as dimensionality grows, the contrast between smallest and largest distances (relative to the average distance) often diminishes, making it harder to distinguish \"near\" from \"far\" neighbors.\n",
    "13. `ratios = ...`: Calculates the relative spread ( (max-min)/avg ) for each dimension.\n",
    "14. Loop to print the dimension and the calculated ratio, typically showing this ratio decreases with higher dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Complete Project Example (e.g., Boston Housing or Custom Dataset)\n",
    "\n",
    "Let's use a synthetic dataset for simplicity and full control, demonstrating an end-to-end KNN Regression project. This will include EDA, preprocessing, training, tuning, and evaluation. (Note: The Boston Housing dataset was removed from scikit-learn in version 1.2 due to ethical concerns. California Housing is an alternative, but a synthetic one is clearer for demonstration).\n",
    "\n",
    "**1. Dataset Generation and EDA**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "X_proj, y_proj = make_regression(n_samples=500, n_features=3, n_informative=2, noise=25, random_state=42)\n",
    "# n_features=3, but only 2 are informative, 1 is redundant/noisy\n",
    "\n",
    "# Convert to DataFrame for easier EDA\n",
    "feature_names = [f'feature_{i+1}' for i in range(X_proj.shape[1])]\n",
    "df_X_proj = pd.DataFrame(X_proj, columns=feature_names)\n",
    "df_y_proj = pd.Series(y_proj, name='target')\n",
    "df_proj = pd.concat([df_X_proj, df_y_proj], axis=1)\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "df_proj.info()\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_proj.head())\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df_proj.describe())\n",
    "\n",
    "# Pairplot for visualization\n",
    "# Select only informative features and target for a cleaner pairplot if known, or all\n",
    "# For this synthetic set, let's assume we don't know which are informative yet for EDA\n",
    "sns.pairplot(df_proj, x_vars=feature_names, y_vars='target', kind='scatter', diag_kind=None)\n",
    "plt.suptitle(\"Pairplot of Features vs. Target\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_proj.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Feature and Target Correlation Heatmap\")\n",
    "plt.show()\n",
    "```\n",
    "*Line-by-line Explanation (Dataset & EDA):*\n",
    "1.  `X_proj, y_proj = make_regression(...)`: Generates a synthetic dataset with 500 samples, 3 features (2 informative, 1 less so), and some noise.\n",
    "2.  `feature_names = [...]`, `df_X_proj = ...`, `df_y_proj = ...`, `df_proj = ...`: Converts the NumPy arrays into Pandas DataFrames/Series for easier handling and EDA.\n",
    "3.  `df_proj.info()`, `df_proj.head()`, `df_proj.describe()`: Basic Pandas EDA functions to understand data types, see sample rows, and get summary statistics.\n",
    "4.  `sns.pairplot(...)`: Creates scatter plots of each feature against the target variable. This helps visualize potential relationships.\n",
    "5.  `sns.heatmap(...)`: Generates a heatmap of the correlation matrix, including the target. This shows linear correlations between variables.\n",
    "\n",
    "**2. Preprocessing and Train-Test Split**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Features and Target\n",
    "X_features = df_proj[feature_names]\n",
    "y_target = df_proj['target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_features, y_target, test_size=0.25, random_state=42)\n",
    "print(f\"\\nX_train shape: {X_train_p.shape}, X_test shape: {X_test_p.shape}\")\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler_p = StandardScaler()\n",
    "X_train_scaled_p = scaler_p.fit_transform(X_train_p)\n",
    "X_test_scaled_p = scaler_p.transform(X_test_p) # Use transform only on test data\n",
    "\n",
    "# Convert scaled arrays back to DataFrames for consistency (optional)\n",
    "X_train_scaled_df_p = pd.DataFrame(X_train_scaled_p, columns=feature_names, index=X_train_p.index)\n",
    "X_test_scaled_df_p = pd.DataFrame(X_test_scaled_p, columns=feature_names, index=X_test_p.index)\n",
    "```\n",
    "*Line-by-line Explanation (Preprocessing & Split):*\n",
    "1.  `X_features = ...`, `y_target = ...`: Separates features and target from the DataFrame.\n",
    "2.  `X_train_p, ... = train_test_split(...)`: Splits the data into 75% training and 25% testing.\n",
    "3.  `scaler_p = StandardScaler()`: Initializes the StandardScaler.\n",
    "4.  `X_train_scaled_p = scaler_p.fit_transform(X_train_p)`: Fits the scaler on training features and transforms them.\n",
    "5.  `X_test_scaled_p = scaler_p.transform(X_test_p)`: Transforms test features using the scaler fitted on training data.\n",
    "6.  `pd.DataFrame(...)`: Optionally converts the scaled NumPy arrays back to Pandas DataFrames, preserving column names and indices.\n",
    "\n",
    "**3. Model Training and Hyperparameter Tuning (K and weights)**\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for K and weights\n",
    "param_grid_p = {\n",
    "    'n_neighbors': np.arange(1, 31), # Test K values from 1 to 30\n",
    "    'weights': ['uniform', 'distance'], # Test both weighting schemes\n",
    "    'p': [1, 2] # Test Manhattan (1) and Euclidean (2) distances\n",
    "}\n",
    "\n",
    "# Initialize KNN Regressor\n",
    "knn_p = KNeighborsRegressor()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "# cv=5 means 5-fold cross-validation\n",
    "# scoring='neg_root_mean_squared_error' (or 'neg_mean_squared_error')\n",
    "grid_search_p = GridSearchCV(knn_p, param_grid_p, cv=5, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to find the best parameters\n",
    "grid_search_p.fit(X_train_scaled_p, y_train_p)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params_p = grid_search_p.best_params_\n",
    "best_score_p = grid_search_p.best_score_ # This will be negative RMSE\n",
    "print(f\"\\nBest parameters found by GridSearchCV: {best_params_p}\")\n",
    "print(f\"Best cross-validated RMSE: {-best_score_p:.4f}\") # Convert to positive\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_knn_model_p = grid_search_p.best_estimator_ # This is already trained on full X_train_scaled_p\n",
    "# Or explicitly:\n",
    "# final_knn_model_p = KNeighborsRegressor(**best_params_p)\n",
    "# final_knn_model_p.fit(X_train_scaled_p, y_train_p)\n",
    "```\n",
    "*Line-by-line Explanation (Training & Tuning):*\n",
    "1.  `param_grid_p = {...}`: Defines the hyperparameter grid to search: `n_neighbors` (K), `weights` (`uniform` or `distance`), and `p` (for Minkowski distance, 1 for Manhattan, 2 for Euclidean).\n",
    "2.  `knn_p = KNeighborsRegressor()`: Initializes a KNN Regressor.\n",
    "3.  `grid_search_p = GridSearchCV(...)`: Sets up GridSearchCV to find the best combination of parameters using 5-fold cross-validation and `neg_root_mean_squared_error` as the evaluation metric. `n_jobs=-1` uses all available CPU cores.\n",
    "4.  `grid_search_p.fit(...)`: Runs the grid search on the scaled training data.\n",
    "5.  `best_params_p = grid_search_p.best_params_`: Retrieves the best hyperparameter combination.\n",
    "6.  `best_score_p = grid_search_p.best_score_`: Retrieves the best cross-validated score (negative RMSE).\n",
    "7.  `print(...)`: Prints the best parameters and the corresponding positive RMSE.\n",
    "8.  `final_knn_model_p = grid_search_p.best_estimator_`: The `best_estimator_` attribute of a fitted `GridSearchCV` object is a model trained on the entire training set (that was passed to `fit`) using the best found parameters. This is ready for prediction.\n",
    "\n",
    "**4. Prediction and Evaluation on Test Set**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "y_pred_p = final_knn_model_p.predict(X_test_scaled_p)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_p = mean_absolute_error(y_test_p, y_pred_p)\n",
    "mse_p = mean_squared_error(y_test_p, y_pred_p)\n",
    "rmse_p = np.sqrt(mse_p)\n",
    "r2_p = r2_score(y_test_p, y_pred_p)\n",
    "\n",
    "print(f\"\\n--- Evaluation on Test Set ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_p:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_p:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_p:.4f}\")\n",
    "print(f\"R-squared (R²): {r2_p:.4f}\")\n",
    "\n",
    "# Visualizing predictions vs actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_p, y_pred_p, alpha=0.7, edgecolors='k', label='Predicted vs Actual')\n",
    "plt.plot([y_test_p.min(), y_test_p.max()], [y_test_p.min(), y_test_p.max()], '--', color='red', lw=2, label='Perfect Prediction Line')\n",
    "plt.xlabel(\"Actual Target Values\")\n",
    "plt.ylabel(\"Predicted Target Values\")\n",
    "plt.title(f\"KNN Regression: Actual vs. Predicted (K={best_params_p['n_neighbors']}, Weight={best_params_p['weights']}, Dist_p={best_params_p['p']})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualizing residuals\n",
    "residuals_p = y_test_p - y_pred_p\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(residuals_p, kde=True)\n",
    "plt.xlabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Residuals\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "*Line-by-line Explanation (Prediction & Evaluation):*\n",
    "1.  `y_pred_p = final_knn_model_p.predict(X_test_scaled_p)`: Uses the tuned final model to make predictions on the (scaled) test features.\n",
    "2.  `mae_p = ...`, `mse_p = ...`, `rmse_p = ...`, `r2_p = ...`: Calculates MAE, MSE, RMSE, and R² using the test set's true values and the predicted values.\n",
    "3.  `print(...)`: Prints the evaluation metrics for the test set.\n",
    "4.  `plt.scatter(y_test_p, y_pred_p, ...)`: Creates a scatter plot comparing actual test values to predicted values.\n",
    "5.  `plt.plot(...)`: Adds a diagonal line representing perfect predictions.\n",
    "6.  `residuals_p = y_test_p - y_pred_p`: Calculates the residuals.\n",
    "7.  `sns.histplot(residuals_p, kde=True)`: Plots a histogram of the residuals with a Kernel Density Estimate. Ideally, residuals should be normally distributed around zero.\n",
    "8.  Standard `plt` and `sns` commands for plot aesthetics.\n",
    "\n",
    "This full project example illustrates the systematic approach: generating/loading data, exploring it, preprocessing, tuning hyperparameters via cross-validation, training the final model, and finally evaluating its performance on unseen data with appropriate metrics and visualizations.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Contrast KNN Regression with Linear Regression and Decision Trees (Regression)\n",
    "\n",
    "Comparing KNN Regression with other common regression algorithms like Linear Regression and Decision Tree Regressors highlights their different characteristics and suitability for various tasks.\n",
    "\n",
    "**KNN Regression vs. Linear Regression:**\n",
    "*   **Model Type:**\n",
    "    *   KNN-R: Non-parametric, instance-based. Learns locally.\n",
    "    *   Linear Regression: Parametric, assumes a linear relationship (`y = β0 + β1x1 + ... + βnxn + ε`). Learns a global model.\n",
    "*   **Assumptions:**\n",
    "    *   KNN-R: No strong assumptions about data distribution or functional form. Assumes similar inputs have similar outputs.\n",
    "    *   Linear Regression: Assumes linearity, independence of errors, homoscedasticity (constant variance of errors), and normally distributed errors (for inference).\n",
    "*   **Training/Prediction Time:**\n",
    "    *   KNN-R: Fast \"training\" (data storage), slow prediction (distance calculations).\n",
    "    *   Linear Regression: Slower training (solves for coefficients, e.g., via Ordinary Least Squares or Gradient Descent), very fast prediction (dot product).\n",
    "*   **Interpretability:**\n",
    "    *   KNN-R: Low interpretability. Predictions are based on local averages, hard to get global insights.\n",
    "    *   Linear Regression: High interpretability. Coefficients directly indicate the strength and direction of each feature's linear impact on the target.\n",
    "*   **Feature Scaling:**\n",
    "    *   KNN-R: Essential, as it's distance-based.\n",
    "    *   Linear Regression: Not strictly required for OLS, but highly recommended for Gradient Descent-based solvers and when using regularization (e.g., Ridge, Lasso) to ensure fair penalization and faster convergence.\n",
    "*   **Handling Non-linear Data:**\n",
    "    *   KNN-R: Naturally handles non-linear data well due to its local nature.\n",
    "    *   Linear Regression: Cannot model non-linear relationships unless features are manually transformed (e.g., polynomial features, log transforms).\n",
    "*   **Extrapolation:**\n",
    "    *   KNN-R: Cannot extrapolate beyond the range of target values in the training data.\n",
    "    *   Linear Regression: Can extrapolate, but these extrapolations are based on the assumed linear trend and can be unreliable far from the observed data range.\n",
    "\n",
    "**KNN Regression vs. Decision Tree Regressor:**\n",
    "*   **Model Representation:**\n",
    "    *   KNN-R: Stores the entire training dataset.\n",
    "    *   Decision Tree Regressor: Builds an explicit tree structure where leaf nodes contain the predicted continuous value (typically the average of target values of training instances in that leaf).\n",
    "*   **Learning Type:**\n",
    "    *   KNN-R: Lazy learner.\n",
    "    *   Decision Tree Regressor: Eager learner.\n",
    "*   **Training/Prediction Time:**\n",
    "    *   KNN-R: Fast training, slow prediction.\n",
    "    *   Decision Tree Regressor: Slower training (finds optimal splits based on variance reduction), fast prediction (traverses the tree).\n",
    "*   **Interpretability:**\n",
    "    *   KNN-R: Low interpretability.\n",
    "    *   Decision Tree Regressor: Relatively high interpretability. The tree structure shows the splitting rules.\n",
    "*   **Feature Scaling:**\n",
    "    *   KNN-R: Essential.\n",
    "    *   Decision Tree Regressor: Insensitive to feature scaling, as splits are based on individual feature thresholds.\n",
    "*   **Prediction Output:**\n",
    "    *   KNN-R: Can produce smooth-ish regression surfaces (especially with larger K and `weights='distance'`). The output is an average of K neighbors.\n",
    "    *   Decision Tree Regressor: Produces piecewise constant predictions. All points falling into the same leaf node get the same predicted value. The regression surface is step-like.\n",
    "*   **Handling Non-linear Data:**\n",
    "    *   KNN-R: Handles non-linearity smoothly.\n",
    "    *   Decision Tree Regressor: Handles non-linearity by making multiple splits, approximating curves with step functions.\n",
    "*   **Sensitivity to Data:**\n",
    "    *   KNN-R: Predictions can be sensitive to the exact location of neighbors, especially with small K.\n",
    "    *   Decision Tree Regressor: Can be unstable; small changes in data can lead to different tree structures (often mitigated by ensemble methods like Random Forests).\n",
    "\n",
    "**When KNN Regression is or isn’t appropriate:**\n",
    "*   **Appropriate:**\n",
    "    *   For datasets with complex, non-linear relationships where parametric assumptions don't hold.\n",
    "    *   When the feature space is not excessively high-dimensional or dimensionality reduction is applied.\n",
    "    *   For smaller datasets where prediction time isn't a critical bottleneck.\n",
    "    *   As a baseline model due to its conceptual simplicity.\n",
    "*   **Not Appropriate (or less suitable):**\n",
    "    *   For very large datasets (due to prediction time and memory).\n",
    "    *   In high-dimensional spaces (curse of dimensionality).\n",
    "    *   When interpretability of how features influence the target is important.\n",
    "    *   When predictions need to be made very quickly.\n",
    "    *   If extrapolation beyond the observed data range is necessary.\n",
    "    *   If features have vastly different scales and are not preprocessed.\n",
    "\n",
    "Choosing the right regression model depends on dataset size, dimensionality, nature of relationships, interpretability needs, and computational constraints.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e943bf7-268c-4780-8678-d25310786c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
